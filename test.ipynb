{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:28:43.272416Z",
     "start_time": "2024-11-10T21:28:35.426830Z"
    }
   },
   "source": [
    "import data.files as files\n",
    "import os\n",
    "from topic.topic_modeling import TopicModel\n",
    "from tqdm import tqdm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klara/Developer/Uni/WiSe2425/text_topic/topic_venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T21:28:43.388103Z",
     "start_time": "2024-11-10T21:28:43.273666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"/Users/klara/Documents/uni/\"\n",
    "pdfs = files.get_files(path=path, file_ending=\"pdf\")\n",
    "dataset_path =\"dataset/\"\n",
    "model_path = 'models/'"
   ],
   "id": "bd15fa715a43d2f8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T21:28:43.393210Z",
     "start_time": "2024-11-10T21:28:43.389029Z"
    }
   },
   "cell_type": "code",
   "source": "os.path.basename(path)",
   "id": "6d5608ede169ac89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T21:28:43.397710Z",
     "start_time": "2024-11-10T21:28:43.394996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_sentences_to_file(sentences, dataset_path):\n",
    "    # save the sentences to a file\n",
    "    #files.save_text_to_file(sentences, dataset_path + \"sentences_old.txt\")\n",
    "    with open(dataset_path + 'sentences1.txt', 'w') as f:\n",
    "        for i in tqdm(range(len(sentences)), desc='Writing sentences to file'):\n",
    "            sentence = sentences[i].encode(\"utf-8\", errors=\"ignore\")\n",
    "            try:\n",
    "                f.write(f\"NEWFILE{sentence}\")\n",
    "            except AttributeError as e:\n",
    "                #f.write(f\"{sentence}\\n\")\n",
    "                print(f\"Error with sentence {i} encountered: {e}\")\n",
    "                pass\n",
    "    f.close()"
   ],
   "id": "724294c03d5bc85b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T21:28:43.400474Z",
     "start_time": "2024-11-10T21:28:43.398461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_sentences_from_file(dataset_path):\n",
    "    # load the sentences from a file\n",
    "    with open(dataset_path + 'sentences_old.txt') as f:\n",
    "        sentences = f.read()#f.readlines()\n",
    "    print(\"File content read successfully\")  # Check if this prints\n",
    "    return sentences"
   ],
   "id": "78861482481e7a9b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:24:58.505291Z",
     "start_time": "2024-11-10T21:28:43.401244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = []\n",
    "# if dataset_path:\n",
    "#     sentences = load_sentences_from_file(dataset_path)\n",
    "# else:\n",
    "for i in tqdm(range(len(pdfs)), desc='Extracting text from pdfs'):\n",
    "    pdf = pdfs[i]\n",
    "    sentence = files.extract_text_from_pdf(pdf)\n",
    "    if type(sentence) != str:\n",
    "        sentence = str(sentence)\n",
    "    sentences.extend([sentence])\n",
    "#sentences = save_sentences_to_file(sentences, dataset_path)"
   ],
   "id": "f94f38605094ed29",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from pdfs:   0%|          | 3/3016 [00:10<2:46:06,  3.31s/it]Ignoring wrong pointing object 485 0 (offset 0)\n",
      "Ignoring wrong pointing object 487 0 (offset 0)\n",
      "Extracting text from pdfs:   4%|▍         | 117/3016 [00:56<13:13,  3.65it/s] EOF marker not found\n",
      "Extracting text from pdfs:  75%|███████▍  | 2248/3016 [08:06<03:29,  3.66it/s]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2250/3016 [08:08<05:30,  2.32it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2252/3016 [08:09<06:27,  1.97it/s]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2253/3016 [08:10<06:37,  1.92it/s]Ignoring wrong pointing object 670 0 (offset 0)\n",
      "Ignoring wrong pointing object 673 0 (offset 0)\n",
      "Ignoring wrong pointing object 1336 0 (offset 0)\n",
      "Ignoring wrong pointing object 1346 0 (offset 0)\n",
      "Ignoring wrong pointing object 1366 0 (offset 0)\n",
      "Ignoring wrong pointing object 1374 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2254/3016 [08:15<22:28,  1.77s/it]Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2255/3016 [08:21<36:12,  2.85s/it]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2256/3016 [08:23<32:59,  2.60s/it]Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 174 0 (offset 0)\n",
      "Ignoring wrong pointing object 188 0 (offset 0)\n",
      "Ignoring wrong pointing object 202 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n",
      "Ignoring wrong pointing object 244 0 (offset 0)\n",
      "Ignoring wrong pointing object 327 0 (offset 0)\n",
      "Ignoring wrong pointing object 347 0 (offset 0)\n",
      "Ignoring wrong pointing object 355 0 (offset 0)\n",
      "Ignoring wrong pointing object 369 0 (offset 0)\n",
      "Ignoring wrong pointing object 384 0 (offset 0)\n",
      "Ignoring wrong pointing object 402 0 (offset 0)\n",
      "Ignoring wrong pointing object 487 0 (offset 0)\n",
      "Ignoring wrong pointing object 492 0 (offset 0)\n",
      "Ignoring wrong pointing object 498 0 (offset 0)\n",
      "Ignoring wrong pointing object 504 0 (offset 0)\n",
      "Ignoring wrong pointing object 509 0 (offset 0)\n",
      "Ignoring wrong pointing object 514 0 (offset 0)\n",
      "Ignoring wrong pointing object 545 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2257/3016 [08:30<47:30,  3.76s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2258/3016 [08:39<1:05:59,  5.22s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2260/3016 [08:41<38:51,  3.08s/it]  Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▍  | 2261/3016 [08:47<50:12,  3.99s/it]Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2262/3016 [08:51<49:13,  3.92s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2263/3016 [08:58<1:03:14,  5.04s/it]Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2264/3016 [09:02<58:42,  4.68s/it]  Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2265/3016 [09:04<48:59,  3.91s/it]Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2267/3016 [09:18<1:11:59,  5.77s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2268/3016 [09:19<55:58,  4.49s/it]  Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2269/3016 [09:24<54:54,  4.41s/it]Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 231 0 (offset 0)\n",
      "Ignoring wrong pointing object 233 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 312 0 (offset 0)\n",
      "Ignoring wrong pointing object 336 0 (offset 0)\n",
      "Ignoring wrong pointing object 353 0 (offset 0)\n",
      "Ignoring wrong pointing object 362 0 (offset 0)\n",
      "Ignoring wrong pointing object 390 0 (offset 0)\n",
      "Ignoring wrong pointing object 421 0 (offset 0)\n",
      "Ignoring wrong pointing object 485 0 (offset 0)\n",
      "Ignoring wrong pointing object 522 0 (offset 0)\n",
      "Ignoring wrong pointing object 566 0 (offset 0)\n",
      "Ignoring wrong pointing object 584 0 (offset 0)\n",
      "Ignoring wrong pointing object 626 0 (offset 0)\n",
      "Ignoring wrong pointing object 647 0 (offset 0)\n",
      "Ignoring wrong pointing object 662 0 (offset 0)\n",
      "Ignoring wrong pointing object 676 0 (offset 0)\n",
      "Ignoring wrong pointing object 722 0 (offset 0)\n",
      "Ignoring wrong pointing object 744 0 (offset 0)\n",
      "Ignoring wrong pointing object 773 0 (offset 0)\n",
      "Ignoring wrong pointing object 818 0 (offset 0)\n",
      "Ignoring wrong pointing object 890 0 (offset 0)\n",
      "Ignoring wrong pointing object 892 0 (offset 0)\n",
      "Ignoring wrong pointing object 923 0 (offset 0)\n",
      "Ignoring wrong pointing object 925 0 (offset 0)\n",
      "Ignoring wrong pointing object 933 0 (offset 0)\n",
      "Ignoring wrong pointing object 947 0 (offset 0)\n",
      "Ignoring wrong pointing object 949 0 (offset 0)\n",
      "Ignoring wrong pointing object 985 0 (offset 0)\n",
      "Ignoring wrong pointing object 987 0 (offset 0)\n",
      "Ignoring wrong pointing object 992 0 (offset 0)\n",
      "Ignoring wrong pointing object 1006 0 (offset 0)\n",
      "Ignoring wrong pointing object 1098 0 (offset 0)\n",
      "Ignoring wrong pointing object 1108 0 (offset 0)\n",
      "Ignoring wrong pointing object 1263 0 (offset 0)\n",
      "Ignoring wrong pointing object 1274 0 (offset 0)\n",
      "Ignoring wrong pointing object 1282 0 (offset 0)\n",
      "Ignoring wrong pointing object 1389 0 (offset 0)\n",
      "Ignoring wrong pointing object 1413 0 (offset 0)\n",
      "Ignoring wrong pointing object 1506 0 (offset 0)\n",
      "Ignoring wrong pointing object 1527 0 (offset 0)\n",
      "Ignoring wrong pointing object 1547 0 (offset 0)\n",
      "Ignoring wrong pointing object 1590 0 (offset 0)\n",
      "Ignoring wrong pointing object 1601 0 (offset 0)\n",
      "Ignoring wrong pointing object 1615 0 (offset 0)\n",
      "Ignoring wrong pointing object 1727 0 (offset 0)\n",
      "Ignoring wrong pointing object 1751 0 (offset 0)\n",
      "Ignoring wrong pointing object 1807 0 (offset 0)\n",
      "Ignoring wrong pointing object 1894 0 (offset 0)\n",
      "Ignoring wrong pointing object 1973 0 (offset 0)\n",
      "Ignoring wrong pointing object 2051 0 (offset 0)\n",
      "Ignoring wrong pointing object 2114 0 (offset 0)\n",
      "Ignoring wrong pointing object 2116 0 (offset 0)\n",
      "Ignoring wrong pointing object 2136 0 (offset 0)\n",
      "Ignoring wrong pointing object 2207 0 (offset 0)\n",
      "Ignoring wrong pointing object 2209 0 (offset 0)\n",
      "Ignoring wrong pointing object 2247 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2270/3016 [10:19<4:05:42, 19.76s/it]Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 168 0 (offset 0)\n",
      "Ignoring wrong pointing object 178 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 212 0 (offset 0)\n",
      "Ignoring wrong pointing object 218 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2271/3016 [10:21<2:56:40, 14.23s/it]Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2272/3016 [10:22<2:07:03, 10.25s/it]Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2274/3016 [10:25<1:13:39,  5.96s/it]Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2275/3016 [10:25<53:21,  4.32s/it]  Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2276/3016 [10:27<43:41,  3.54s/it]Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Extracting text from pdfs:  75%|███████▌  | 2277/3016 [10:29<38:39,  3.14s/it]Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 137 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2278/3016 [10:31<31:41,  2.58s/it]Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2279/3016 [10:32<28:45,  2.34s/it]Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2280/3016 [10:34<25:57,  2.12s/it]Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2281/3016 [10:37<28:36,  2.34s/it]Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2282/3016 [10:40<33:06,  2.71s/it]Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 148 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2283/3016 [10:41<26:18,  2.15s/it]Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2285/3016 [10:46<26:33,  2.18s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2286/3016 [10:48<25:51,  2.12s/it]Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2290/3016 [10:55<18:28,  1.53s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 249 0 (offset 0)\n",
      "Ignoring wrong pointing object 517 0 (offset 0)\n",
      "Ignoring wrong pointing object 520 0 (offset 0)\n",
      "Ignoring wrong pointing object 522 0 (offset 0)\n",
      "Ignoring wrong pointing object 863 0 (offset 0)\n",
      "Ignoring wrong pointing object 865 0 (offset 0)\n",
      "Ignoring wrong pointing object 893 0 (offset 0)\n",
      "Ignoring wrong pointing object 928 0 (offset 0)\n",
      "Ignoring wrong pointing object 1136 0 (offset 0)\n",
      "Ignoring wrong pointing object 6366 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2291/3016 [11:01<33:26,  2.77s/it]Ignoring wrong pointing object 442 0 (offset 0)\n",
      "Ignoring wrong pointing object 860 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2294/3016 [11:05<23:27,  1.95s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Extracting text from pdfs:  76%|███████▌  | 2296/3016 [11:07<16:20,  1.36s/it]Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Ignoring wrong pointing object 740 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2308/3016 [11:18<08:00,  1.47it/s]Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2310/3016 [11:19<07:26,  1.58it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2312/3016 [11:22<11:50,  1.01s/it]Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2313/3016 [11:23<10:33,  1.11it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 143 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2316/3016 [11:30<18:09,  1.56s/it]Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2317/3016 [11:33<20:04,  1.72s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2318/3016 [11:33<15:50,  1.36s/it]Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2319/3016 [11:33<12:45,  1.10s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2320/3016 [11:33<09:34,  1.21it/s]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 258 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2321/3016 [11:39<24:04,  2.08s/it]Ignoring wrong pointing object 314 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2322/3016 [11:42<29:52,  2.58s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2323/3016 [11:43<21:44,  1.88s/it]Ignoring wrong pointing object 871 0 (offset 0)\n",
      "Ignoring wrong pointing object 884 0 (offset 0)\n",
      "Ignoring wrong pointing object 982 0 (offset 0)\n",
      "Ignoring wrong pointing object 1843 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2324/3016 [11:46<25:40,  2.23s/it]Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Ignoring wrong pointing object 652 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2325/3016 [11:49<28:13,  2.45s/it]Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2327/3016 [11:50<17:06,  1.49s/it]Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2328/3016 [11:50<12:22,  1.08s/it]Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 428 0 (offset 0)\n",
      "Ignoring wrong pointing object 2660 0 (offset 0)\n",
      "Ignoring wrong pointing object 3351 0 (offset 0)\n",
      "Ignoring wrong pointing object 3445 0 (offset 0)\n",
      "Ignoring wrong pointing object 4133 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2329/3016 [12:00<44:27,  3.88s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2330/3016 [12:07<52:32,  4.60s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 150 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2331/3016 [12:10<47:08,  4.13s/it]Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2332/3016 [12:11<37:53,  3.32s/it]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Extracting text from pdfs:  77%|███████▋  | 2333/3016 [12:13<31:53,  2.80s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▊  | 2374/3016 [12:19<00:38, 16.84it/s]Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2377/3016 [12:23<04:10,  2.56it/s]Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2381/3016 [12:23<03:15,  3.25it/s]Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2384/3016 [12:28<07:34,  1.39it/s]Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2385/3016 [12:29<08:12,  1.28it/s]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2386/3016 [12:30<08:34,  1.22it/s]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 144 0 (offset 0)\n",
      "Ignoring wrong pointing object 188 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2388/3016 [12:34<12:04,  1.15s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2389/3016 [12:36<13:30,  1.29s/it]Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2390/3016 [12:38<16:48,  1.61s/it]Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2391/3016 [12:40<16:56,  1.63s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2392/3016 [12:41<16:12,  1.56s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Extracting text from pdfs:  79%|███████▉  | 2393/3016 [12:44<21:14,  2.05s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Extracting text from pdfs:  81%|████████  | 2432/3016 [13:22<05:44,  1.69it/s]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  81%|████████  | 2439/3016 [13:23<02:32,  3.78it/s]Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Extracting text from pdfs:  81%|████████  | 2442/3016 [13:23<01:51,  5.16it/s]Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Extracting text from pdfs:  81%|████████  | 2449/3016 [13:23<01:01,  9.18it/s]Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2459/3016 [13:24<00:44, 12.62it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2461/3016 [13:25<01:35,  5.83it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2463/3016 [13:26<02:03,  4.48it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2464/3016 [13:26<01:56,  4.75it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2471/3016 [13:34<06:50,  1.33it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2475/3016 [13:35<03:16,  2.76it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2477/3016 [13:37<05:18,  1.69it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2478/3016 [13:38<07:11,  1.25it/s]Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2479/3016 [13:39<07:28,  1.20it/s]Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2480/3016 [13:40<06:40,  1.34it/s]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2481/3016 [13:41<07:41,  1.16it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2483/3016 [13:41<05:31,  1.61it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2484/3016 [13:41<04:34,  1.94it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2485/3016 [13:42<05:12,  1.70it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2487/3016 [13:43<04:49,  1.83it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Extracting text from pdfs:  82%|████████▏ | 2488/3016 [13:44<04:03,  2.17it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2490/3016 [13:45<04:24,  1.99it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2491/3016 [13:46<06:30,  1.34it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2492/3016 [13:47<07:07,  1.23it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 709 0 (offset 0)\n",
      "Ignoring wrong pointing object 711 0 (offset 0)\n",
      "Ignoring wrong pointing object 728 0 (offset 0)\n",
      "Ignoring wrong pointing object 730 0 (offset 0)\n",
      "Ignoring wrong pointing object 797 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2494/3016 [13:49<07:47,  1.12it/s]Ignoring wrong pointing object 449 0 (offset 0)\n",
      "Ignoring wrong pointing object 692 0 (offset 0)\n",
      "Ignoring wrong pointing object 694 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1195 0 (offset 0)\n",
      "Ignoring wrong pointing object 1254 0 (offset 0)\n",
      "Ignoring wrong pointing object 3187 0 (offset 0)\n",
      "Ignoring wrong pointing object 3197 0 (offset 0)\n",
      "Ignoring wrong pointing object 3622 0 (offset 0)\n",
      "Ignoring wrong pointing object 3707 0 (offset 0)\n",
      "Ignoring wrong pointing object 3726 0 (offset 0)\n",
      "Ignoring wrong pointing object 4293 0 (offset 0)\n",
      "Ignoring wrong pointing object 4335 0 (offset 0)\n",
      "Ignoring wrong pointing object 4704 0 (offset 0)\n",
      "Ignoring wrong pointing object 4719 0 (offset 0)\n",
      "Ignoring wrong pointing object 4803 0 (offset 0)\n",
      "Ignoring wrong pointing object 5268 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2496/3016 [13:58<17:25,  2.01s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2497/3016 [14:03<25:34,  2.96s/it]Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2498/3016 [14:04<21:17,  2.47s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2499/3016 [14:05<15:40,  1.82s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2500/3016 [14:05<11:40,  1.36s/it]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 712 0 (offset 0)\n",
      "Ignoring wrong pointing object 714 0 (offset 0)\n",
      "Ignoring wrong pointing object 731 0 (offset 0)\n",
      "Ignoring wrong pointing object 733 0 (offset 0)\n",
      "Ignoring wrong pointing object 790 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2501/3016 [14:06<10:25,  1.22s/it]Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2502/3016 [14:07<10:15,  1.20s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2507/3016 [14:08<04:36,  1.84it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2509/3016 [14:09<04:17,  1.97it/s]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2513/3016 [14:11<04:50,  1.73it/s]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2515/3016 [14:12<03:30,  2.38it/s]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  83%|████████▎ | 2518/3016 [14:22<14:12,  1.71s/it]Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 390 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▎ | 2519/3016 [14:24<14:07,  1.71s/it]Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 216 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▎ | 2520/3016 [14:26<14:49,  1.79s/it]Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 212 0 (offset 0)\n",
      "Ignoring wrong pointing object 597 0 (offset 0)\n",
      "Ignoring wrong pointing object 678 0 (offset 0)\n",
      "Ignoring wrong pointing object 688 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▎ | 2521/3016 [14:31<20:23,  2.47s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▎ | 2523/3016 [14:34<16:15,  1.98s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▎ | 2524/3016 [15:01<1:10:54,  8.65s/it]Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▎ | 2525/3016 [15:06<1:02:24,  7.63s/it]Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2526/3016 [15:07<47:28,  5.81s/it]  Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 375 0 (offset 0)\n",
      "Ignoring wrong pointing object 606 0 (offset 0)\n",
      "Ignoring wrong pointing object 772 0 (offset 0)\n",
      "Ignoring wrong pointing object 784 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2527/3016 [15:12<44:54,  5.51s/it]Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 441 0 (offset 0)\n",
      "Ignoring wrong pointing object 455 0 (offset 0)\n",
      "Ignoring wrong pointing object 687 0 (offset 0)\n",
      "Ignoring wrong pointing object 709 0 (offset 0)\n",
      "Ignoring wrong pointing object 711 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2528/3016 [15:18<45:29,  5.59s/it]Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 170 0 (offset 0)\n",
      "Ignoring wrong pointing object 671 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2532/3016 [15:23<20:31,  2.55s/it]Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2539/3016 [15:33<13:26,  1.69s/it]Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2542/3016 [15:36<10:55,  1.38s/it]Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2543/3016 [15:40<14:34,  1.85s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 175 0 (offset 0)\n",
      "Extracting text from pdfs:  84%|████████▍ | 2547/3016 [15:48<14:35,  1.87s/it]Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▍ | 2551/3016 [16:01<20:13,  2.61s/it]Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▍ | 2557/3016 [16:18<13:06,  1.71s/it]Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▍ | 2560/3016 [16:24<11:43,  1.54s/it]Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 211 0 (offset 0)\n",
      "Ignoring wrong pointing object 213 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 217 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▍ | 2562/3016 [16:29<14:29,  1.92s/it]Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2565/3016 [16:35<12:16,  1.63s/it]Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2566/3016 [16:36<10:30,  1.40s/it]Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Ignoring wrong pointing object 111 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2567/3016 [16:40<16:34,  2.21s/it]Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Ignoring wrong pointing object 111 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2569/3016 [16:48<20:51,  2.80s/it]Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 194 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2570/3016 [16:52<24:08,  3.25s/it]Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2571/3016 [16:57<28:05,  3.79s/it]Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 237 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2572/3016 [16:58<21:16,  2.88s/it]Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 175 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2573/3016 [17:04<28:29,  3.86s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2574/3016 [17:05<21:16,  2.89s/it]Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 150 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2575/3016 [17:06<18:51,  2.57s/it]Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 243 0 (offset 0)\n",
      "Ignoring wrong pointing object 271 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2576/3016 [17:18<37:56,  5.17s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Extracting text from pdfs:  85%|████████▌ | 2577/3016 [17:29<51:05,  6.98s/it]Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2582/3016 [17:36<14:37,  2.02s/it]Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 192 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2583/3016 [17:45<28:00,  3.88s/it]Ignoring wrong pointing object 246 0 (offset 0)\n",
      "Ignoring wrong pointing object 248 0 (offset 0)\n",
      "Ignoring wrong pointing object 342 0 (offset 0)\n",
      "Ignoring wrong pointing object 452 0 (offset 0)\n",
      "Ignoring wrong pointing object 454 0 (offset 0)\n",
      "Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "Ignoring wrong pointing object 1094 0 (offset 0)\n",
      "Ignoring wrong pointing object 1097 0 (offset 0)\n",
      "Ignoring wrong pointing object 1099 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2584/3016 [17:47<24:48,  3.45s/it]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2585/3016 [17:49<21:13,  2.96s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2587/3016 [17:51<15:57,  2.23s/it]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2589/3016 [17:55<12:42,  1.79s/it]Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2591/3016 [17:56<08:24,  1.19s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2593/3016 [17:57<07:18,  1.04s/it]Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2594/3016 [18:01<11:28,  1.63s/it]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2595/3016 [18:05<17:04,  2.43s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2600/3016 [18:14<12:23,  1.79s/it]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▌ | 2601/3016 [18:17<13:07,  1.90s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  86%|████████▋ | 2604/3016 [18:19<09:03,  1.32s/it]Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 182 0 (offset 0)\n",
      "Ignoring wrong pointing object 231 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2615/3016 [18:22<02:57,  2.26it/s]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2619/3016 [18:24<03:29,  1.90it/s]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 178 0 (offset 0)\n",
      "Ignoring wrong pointing object 659 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2622/3016 [18:28<05:02,  1.30it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2623/3016 [18:28<04:08,  1.58it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2627/3016 [18:28<02:19,  2.78it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2628/3016 [18:29<02:28,  2.61it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2631/3016 [18:29<01:26,  4.47it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2633/3016 [18:31<03:06,  2.06it/s]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Extracting text from pdfs:  87%|████████▋ | 2635/3016 [18:33<03:41,  1.72it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2641/3016 [18:38<05:01,  1.24it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2643/3016 [18:40<04:54,  1.26it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 201 0 (offset 0)\n",
      "Ignoring wrong pointing object 203 0 (offset 0)\n",
      "Ignoring wrong pointing object 213 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 217 0 (offset 0)\n",
      "Ignoring wrong pointing object 229 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 279 0 (offset 0)\n",
      "Ignoring wrong pointing object 281 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 285 0 (offset 0)\n",
      "Ignoring wrong pointing object 287 0 (offset 0)\n",
      "Ignoring wrong pointing object 289 0 (offset 0)\n",
      "Ignoring wrong pointing object 295 0 (offset 0)\n",
      "Ignoring wrong pointing object 301 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2644/3016 [18:42<06:43,  1.08s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 176 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 202 0 (offset 0)\n",
      "Ignoring wrong pointing object 204 0 (offset 0)\n",
      "Ignoring wrong pointing object 206 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2645/3016 [18:43<06:08,  1.01it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2646/3016 [18:43<05:17,  1.16it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 180 0 (offset 0)\n",
      "Ignoring wrong pointing object 182 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Ignoring wrong pointing object 633 0 (offset 0)\n",
      "Ignoring wrong pointing object 661 0 (offset 0)\n",
      "Ignoring wrong pointing object 671 0 (offset 0)\n",
      "Ignoring wrong pointing object 673 0 (offset 0)\n",
      "Ignoring wrong pointing object 679 0 (offset 0)\n",
      "Ignoring wrong pointing object 706 0 (offset 0)\n",
      "Ignoring wrong pointing object 708 0 (offset 0)\n",
      "Ignoring wrong pointing object 717 0 (offset 0)\n",
      "Ignoring wrong pointing object 723 0 (offset 0)\n",
      "Ignoring wrong pointing object 729 0 (offset 0)\n",
      "Ignoring wrong pointing object 731 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2647/3016 [18:44<04:36,  1.34it/s]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2648/3016 [18:44<03:44,  1.64it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 187 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 256 0 (offset 0)\n",
      "Ignoring wrong pointing object 262 0 (offset 0)\n",
      "Ignoring wrong pointing object 275 0 (offset 0)\n",
      "Ignoring wrong pointing object 281 0 (offset 0)\n",
      "Ignoring wrong pointing object 299 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2649/3016 [18:45<04:03,  1.51it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 175 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2651/3016 [18:50<09:14,  1.52s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 292 0 (offset 0)\n",
      "Ignoring wrong pointing object 305 0 (offset 0)\n",
      "Ignoring wrong pointing object 307 0 (offset 0)\n",
      "Ignoring wrong pointing object 309 0 (offset 0)\n",
      "Ignoring wrong pointing object 321 0 (offset 0)\n",
      "Ignoring wrong pointing object 348 0 (offset 0)\n",
      "Ignoring wrong pointing object 383 0 (offset 0)\n",
      "Ignoring wrong pointing object 390 0 (offset 0)\n",
      "Ignoring wrong pointing object 414 0 (offset 0)\n",
      "Ignoring wrong pointing object 420 0 (offset 0)\n",
      "Ignoring wrong pointing object 422 0 (offset 0)\n",
      "Ignoring wrong pointing object 428 0 (offset 0)\n",
      "Ignoring wrong pointing object 434 0 (offset 0)\n",
      "Ignoring wrong pointing object 489 0 (offset 0)\n",
      "Ignoring wrong pointing object 514 0 (offset 0)\n",
      "Ignoring wrong pointing object 530 0 (offset 0)\n",
      "Ignoring wrong pointing object 546 0 (offset 0)\n",
      "Ignoring wrong pointing object 561 0 (offset 0)\n",
      "Ignoring wrong pointing object 563 0 (offset 0)\n",
      "Ignoring wrong pointing object 574 0 (offset 0)\n",
      "Ignoring wrong pointing object 602 0 (offset 0)\n",
      "Ignoring wrong pointing object 608 0 (offset 0)\n",
      "Ignoring wrong pointing object 623 0 (offset 0)\n",
      "Ignoring wrong pointing object 634 0 (offset 0)\n",
      "Ignoring wrong pointing object 646 0 (offset 0)\n",
      "Ignoring wrong pointing object 648 0 (offset 0)\n",
      "Ignoring wrong pointing object 658 0 (offset 0)\n",
      "Ignoring wrong pointing object 699 0 (offset 0)\n",
      "Ignoring wrong pointing object 722 0 (offset 0)\n",
      "Ignoring wrong pointing object 748 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2652/3016 [18:54<14:03,  2.32s/it]Ignoring wrong pointing object 314 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2653/3016 [18:56<13:03,  2.16s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2654/3016 [18:56<09:38,  1.60s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2655/3016 [18:56<07:41,  1.28s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2656/3016 [18:59<10:09,  1.69s/it]Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2657/3016 [19:01<10:19,  1.73s/it]Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2658/3016 [19:03<10:46,  1.81s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2661/3016 [19:04<06:19,  1.07s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 148 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 157 0 (offset 0)\n",
      "Ignoring wrong pointing object 159 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 175 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 187 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Ignoring wrong pointing object 271 0 (offset 0)\n",
      "Ignoring wrong pointing object 273 0 (offset 0)\n",
      "Ignoring wrong pointing object 288 0 (offset 0)\n",
      "Ignoring wrong pointing object 404 0 (offset 0)\n",
      "Ignoring wrong pointing object 415 0 (offset 0)\n",
      "Ignoring wrong pointing object 431 0 (offset 0)\n",
      "Ignoring wrong pointing object 473 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2663/3016 [19:07<06:47,  1.15s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2664/3016 [19:08<06:31,  1.11s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2665/3016 [19:09<06:13,  1.06s/it]Ignoring wrong pointing object 242 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 292 0 (offset 0)\n",
      "Ignoring wrong pointing object 317 0 (offset 0)\n",
      "Ignoring wrong pointing object 342 0 (offset 0)\n",
      "Ignoring wrong pointing object 367 0 (offset 0)\n",
      "Ignoring wrong pointing object 392 0 (offset 0)\n",
      "Ignoring wrong pointing object 394 0 (offset 0)\n",
      "Ignoring wrong pointing object 466 0 (offset 0)\n",
      "Ignoring wrong pointing object 494 0 (offset 0)\n",
      "Ignoring wrong pointing object 546 0 (offset 0)\n",
      "Ignoring wrong pointing object 594 0 (offset 0)\n",
      "Ignoring wrong pointing object 596 0 (offset 0)\n",
      "Ignoring wrong pointing object 622 0 (offset 0)\n",
      "Ignoring wrong pointing object 624 0 (offset 0)\n",
      "Ignoring wrong pointing object 672 0 (offset 0)\n",
      "Ignoring wrong pointing object 674 0 (offset 0)\n",
      "Ignoring wrong pointing object 722 0 (offset 0)\n",
      "Ignoring wrong pointing object 724 0 (offset 0)\n",
      "Ignoring wrong pointing object 772 0 (offset 0)\n",
      "Ignoring wrong pointing object 774 0 (offset 0)\n",
      "Ignoring wrong pointing object 799 0 (offset 0)\n",
      "Ignoring wrong pointing object 801 0 (offset 0)\n",
      "Ignoring wrong pointing object 827 0 (offset 0)\n",
      "Ignoring wrong pointing object 829 0 (offset 0)\n",
      "Ignoring wrong pointing object 854 0 (offset 0)\n",
      "Ignoring wrong pointing object 856 0 (offset 0)\n",
      "Ignoring wrong pointing object 904 0 (offset 0)\n",
      "Ignoring wrong pointing object 906 0 (offset 0)\n",
      "Ignoring wrong pointing object 931 0 (offset 0)\n",
      "Ignoring wrong pointing object 933 0 (offset 0)\n",
      "Ignoring wrong pointing object 958 0 (offset 0)\n",
      "Ignoring wrong pointing object 960 0 (offset 0)\n",
      "Ignoring wrong pointing object 985 0 (offset 0)\n",
      "Ignoring wrong pointing object 987 0 (offset 0)\n",
      "Ignoring wrong pointing object 1037 0 (offset 0)\n",
      "Ignoring wrong pointing object 1039 0 (offset 0)\n",
      "Ignoring wrong pointing object 1040 0 (offset 0)\n",
      "Ignoring wrong pointing object 1042 0 (offset 0)\n",
      "Ignoring wrong pointing object 1072 0 (offset 0)\n",
      "Ignoring wrong pointing object 1073 0 (offset 0)\n",
      "Ignoring wrong pointing object 1075 0 (offset 0)\n",
      "Ignoring wrong pointing object 1197 0 (offset 0)\n",
      "Ignoring wrong pointing object 1198 0 (offset 0)\n",
      "Ignoring wrong pointing object 1199 0 (offset 0)\n",
      "Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "Ignoring wrong pointing object 1201 0 (offset 0)\n",
      "Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "Ignoring wrong pointing object 1203 0 (offset 0)\n",
      "Ignoring wrong pointing object 1204 0 (offset 0)\n",
      "Ignoring wrong pointing object 1205 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2666/3016 [19:11<08:09,  1.40s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Extracting text from pdfs:  88%|████████▊ | 2667/3016 [19:22<23:43,  4.08s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2670/3016 [19:28<15:01,  2.60s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2671/3016 [19:29<12:47,  2.22s/it]Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 315 0 (offset 0)\n",
      "Ignoring wrong pointing object 317 0 (offset 0)\n",
      "Ignoring wrong pointing object 319 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2672/3016 [19:31<11:03,  1.93s/it]Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2673/3016 [19:31<08:58,  1.57s/it]Ignoring wrong pointing object 292 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2674/3016 [19:32<07:58,  1.40s/it]Ignoring wrong pointing object 303 0 (offset 0)\n",
      "Ignoring wrong pointing object 316 0 (offset 0)\n",
      "Ignoring wrong pointing object 318 0 (offset 0)\n",
      "Ignoring wrong pointing object 320 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2675/3016 [19:34<07:27,  1.31s/it]Ignoring wrong pointing object 309 0 (offset 0)\n",
      "Ignoring wrong pointing object 311 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▊ | 2676/3016 [19:35<06:54,  1.22s/it]Ignoring wrong pointing object 306 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Ignoring wrong pointing object 310 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2677/3016 [19:35<06:17,  1.11s/it]Ignoring wrong pointing object 1137 0 (offset 0)\n",
      "Ignoring wrong pointing object 1393 0 (offset 0)\n",
      "Ignoring wrong pointing object 1554 0 (offset 0)\n",
      "Ignoring wrong pointing object 1555 0 (offset 0)\n",
      "Ignoring wrong pointing object 1556 0 (offset 0)\n",
      "Ignoring wrong pointing object 1557 0 (offset 0)\n",
      "Ignoring wrong pointing object 1566 0 (offset 0)\n",
      "Ignoring wrong pointing object 1567 0 (offset 0)\n",
      "Ignoring wrong pointing object 1576 0 (offset 0)\n",
      "Ignoring wrong pointing object 1581 0 (offset 0)\n",
      "Ignoring wrong pointing object 1582 0 (offset 0)\n",
      "Ignoring wrong pointing object 1583 0 (offset 0)\n",
      "Ignoring wrong pointing object 1584 0 (offset 0)\n",
      "Ignoring wrong pointing object 1585 0 (offset 0)\n",
      "Ignoring wrong pointing object 1586 0 (offset 0)\n",
      "Ignoring wrong pointing object 1587 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2679/3016 [19:37<05:51,  1.04s/it]Ignoring wrong pointing object 306 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Ignoring wrong pointing object 310 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2680/3016 [19:39<06:32,  1.17s/it]Ignoring wrong pointing object 301 0 (offset 0)\n",
      "Ignoring wrong pointing object 314 0 (offset 0)\n",
      "Ignoring wrong pointing object 316 0 (offset 0)\n",
      "Ignoring wrong pointing object 318 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2681/3016 [19:40<06:01,  1.08s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2686/3016 [19:41<02:44,  2.00it/s]Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2687/3016 [19:46<08:29,  1.55s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2690/3016 [19:51<08:26,  1.56s/it]Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2691/3016 [19:51<06:35,  1.22s/it]Ignoring wrong pointing object 509 0 (offset 0)\n",
      "Ignoring wrong pointing object 630 0 (offset 0)\n",
      "Ignoring wrong pointing object 695 0 (offset 0)\n",
      "Ignoring wrong pointing object 1523 0 (offset 0)\n",
      "Ignoring wrong pointing object 3122 0 (offset 0)\n",
      "Ignoring wrong pointing object 3188 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2692/3016 [20:01<18:42,  3.47s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2693/3016 [20:03<15:52,  2.95s/it]Ignoring wrong pointing object 170 0 (offset 0)\n",
      "Ignoring wrong pointing object 172 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2694/3016 [20:04<13:16,  2.47s/it]Ignoring wrong pointing object 350 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2695/3016 [20:04<10:27,  1.95s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2696/3016 [20:08<13:12,  2.48s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  89%|████████▉ | 2697/3016 [20:09<10:42,  2.01s/it]Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2700/3016 [20:18<12:28,  2.37s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2702/3016 [20:19<07:53,  1.51s/it]Ignoring wrong pointing object 256 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2703/3016 [20:20<07:20,  1.41s/it]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2705/3016 [20:22<05:54,  1.14s/it]Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2706/3016 [20:24<06:33,  1.27s/it]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 256 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2707/3016 [20:24<05:20,  1.04s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2708/3016 [20:27<08:04,  1.57s/it]Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2709/3016 [20:29<08:14,  1.61s/it]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 180 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2711/3016 [20:31<06:37,  1.30s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2712/3016 [20:31<05:17,  1.05s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2713/3016 [20:32<04:49,  1.05it/s]Ignoring wrong pointing object 304 0 (offset 0)\n",
      "Ignoring wrong pointing object 306 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|████████▉ | 2714/3016 [20:33<05:11,  1.03s/it]Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2715/3016 [20:41<15:47,  3.15s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2716/3016 [20:45<16:41,  3.34s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2717/3016 [20:46<12:59,  2.61s/it]Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2718/3016 [20:50<16:10,  3.26s/it]Ignoring wrong pointing object 256 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2720/3016 [20:51<09:30,  1.93s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2721/3016 [20:53<09:20,  1.90s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2722/3016 [20:54<07:38,  1.56s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2723/3016 [20:54<05:44,  1.17s/it]Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2726/3016 [21:34<29:58,  6.20s/it]Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2727/3016 [22:08<1:08:34, 14.24s/it]Ignoring wrong pointing object 111 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2728/3016 [22:40<1:34:14, 19.63s/it]Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Extracting text from pdfs:  90%|█████████ | 2729/3016 [23:19<2:00:23, 25.17s/it]Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2730/3016 [23:52<2:11:40, 27.62s/it]Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2732/3016 [23:55<1:07:13, 14.20s/it]Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 204 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2733/3016 [23:56<49:05, 10.41s/it]  Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 212 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 236 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2734/3016 [23:59<38:10,  8.12s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 196 0 (offset 0)\n",
      "Ignoring wrong pointing object 414 0 (offset 0)\n",
      "Ignoring wrong pointing object 509 0 (offset 0)\n",
      "Ignoring wrong pointing object 510 0 (offset 0)\n",
      "Ignoring wrong pointing object 519 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2735/3016 [24:02<30:36,  6.54s/it]Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2736/3016 [24:02<21:45,  4.66s/it]Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 176 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 307 0 (offset 0)\n",
      "Ignoring wrong pointing object 309 0 (offset 0)\n",
      "Ignoring wrong pointing object 505 0 (offset 0)\n",
      "Ignoring wrong pointing object 507 0 (offset 0)\n",
      "Ignoring wrong pointing object 584 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2737/3016 [24:05<18:50,  4.05s/it]Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 190 0 (offset 0)\n",
      "Ignoring wrong pointing object 196 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 220 0 (offset 0)\n",
      "Ignoring wrong pointing object 252 0 (offset 0)\n",
      "Ignoring wrong pointing object 262 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2738/3016 [24:07<17:12,  3.71s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 190 0 (offset 0)\n",
      "Ignoring wrong pointing object 246 0 (offset 0)\n",
      "Ignoring wrong pointing object 255 0 (offset 0)\n",
      "Ignoring wrong pointing object 408 0 (offset 0)\n",
      "Ignoring wrong pointing object 419 0 (offset 0)\n",
      "Ignoring wrong pointing object 464 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2739/3016 [24:10<15:51,  3.44s/it]Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 168 0 (offset 0)\n",
      "Ignoring wrong pointing object 264 0 (offset 0)\n",
      "Ignoring wrong pointing object 266 0 (offset 0)\n",
      "Ignoring wrong pointing object 271 0 (offset 0)\n",
      "Ignoring wrong pointing object 273 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2740/3016 [24:12<13:57,  3.03s/it]Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 275 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 458 0 (offset 0)\n",
      "Ignoring wrong pointing object 498 0 (offset 0)\n",
      "Ignoring wrong pointing object 568 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2741/3016 [24:15<13:08,  2.87s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2742/3016 [24:39<42:00,  9.20s/it]Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 201 0 (offset 0)\n",
      "Ignoring wrong pointing object 238 0 (offset 0)\n",
      "Ignoring wrong pointing object 240 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n",
      "Ignoring wrong pointing object 379 0 (offset 0)\n",
      "Ignoring wrong pointing object 381 0 (offset 0)\n",
      "Ignoring wrong pointing object 429 0 (offset 0)\n",
      "Ignoring wrong pointing object 471 0 (offset 0)\n",
      "Ignoring wrong pointing object 533 0 (offset 0)\n",
      "Ignoring wrong pointing object 609 0 (offset 0)\n",
      "Ignoring wrong pointing object 626 0 (offset 0)\n",
      "Ignoring wrong pointing object 636 0 (offset 0)\n",
      "Ignoring wrong pointing object 699 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2744/3016 [24:40<21:43,  4.79s/it]Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 174 0 (offset 0)\n",
      "Ignoring wrong pointing object 186 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 201 0 (offset 0)\n",
      "Ignoring wrong pointing object 497 0 (offset 0)\n",
      "Ignoring wrong pointing object 534 0 (offset 0)\n",
      "Ignoring wrong pointing object 536 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2745/3016 [24:43<19:04,  4.22s/it]Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 248 0 (offset 0)\n",
      "Ignoring wrong pointing object 276 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Ignoring wrong pointing object 310 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Ignoring wrong pointing object 394 0 (offset 0)\n",
      "Ignoring wrong pointing object 396 0 (offset 0)\n",
      "Ignoring wrong pointing object 444 0 (offset 0)\n",
      "Ignoring wrong pointing object 446 0 (offset 0)\n",
      "Ignoring wrong pointing object 482 0 (offset 0)\n",
      "Ignoring wrong pointing object 484 0 (offset 0)\n",
      "Ignoring wrong pointing object 505 0 (offset 0)\n",
      "Ignoring wrong pointing object 507 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2746/3016 [24:45<15:59,  3.55s/it]Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 154 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 203 0 (offset 0)\n",
      "Ignoring wrong pointing object 224 0 (offset 0)\n",
      "Ignoring wrong pointing object 259 0 (offset 0)\n",
      "Ignoring wrong pointing object 289 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2747/3016 [24:46<12:43,  2.84s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2748/3016 [24:51<15:17,  3.42s/it]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Extracting text from pdfs:  91%|█████████ | 2749/3016 [24:51<11:10,  2.51s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2762/3016 [25:02<01:34,  2.68it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2763/3016 [25:13<11:21,  2.70s/it]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 144 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 148 0 (offset 0)\n",
      "Ignoring wrong pointing object 150 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 154 0 (offset 0)\n",
      "Ignoring wrong pointing object 157 0 (offset 0)\n",
      "Ignoring wrong pointing object 159 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 175 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 187 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 201 0 (offset 0)\n",
      "Ignoring wrong pointing object 203 0 (offset 0)\n",
      "Ignoring wrong pointing object 205 0 (offset 0)\n",
      "Ignoring wrong pointing object 207 0 (offset 0)\n",
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 211 0 (offset 0)\n",
      "Ignoring wrong pointing object 213 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 218 0 (offset 0)\n",
      "Ignoring wrong pointing object 220 0 (offset 0)\n",
      "Ignoring wrong pointing object 222 0 (offset 0)\n",
      "Ignoring wrong pointing object 224 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n",
      "Ignoring wrong pointing object 230 0 (offset 0)\n",
      "Ignoring wrong pointing object 232 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 236 0 (offset 0)\n",
      "Ignoring wrong pointing object 238 0 (offset 0)\n",
      "Ignoring wrong pointing object 240 0 (offset 0)\n",
      "Ignoring wrong pointing object 242 0 (offset 0)\n",
      "Ignoring wrong pointing object 244 0 (offset 0)\n",
      "Ignoring wrong pointing object 246 0 (offset 0)\n",
      "Ignoring wrong pointing object 248 0 (offset 0)\n",
      "Ignoring wrong pointing object 250 0 (offset 0)\n",
      "Ignoring wrong pointing object 252 0 (offset 0)\n",
      "Ignoring wrong pointing object 254 0 (offset 0)\n",
      "Ignoring wrong pointing object 256 0 (offset 0)\n",
      "Ignoring wrong pointing object 258 0 (offset 0)\n",
      "Ignoring wrong pointing object 260 0 (offset 0)\n",
      "Ignoring wrong pointing object 262 0 (offset 0)\n",
      "Ignoring wrong pointing object 264 0 (offset 0)\n",
      "Ignoring wrong pointing object 266 0 (offset 0)\n",
      "Ignoring wrong pointing object 268 0 (offset 0)\n",
      "Ignoring wrong pointing object 270 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 276 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 427 0 (offset 0)\n",
      "Ignoring wrong pointing object 430 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2764/3016 [25:13<09:00,  2.15s/it]Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2765/3016 [25:17<10:52,  2.60s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 78 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 144 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2766/3016 [25:20<11:22,  2.73s/it]Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 74 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 202 0 (offset 0)\n",
      "Ignoring wrong pointing object 212 0 (offset 0)\n",
      "Ignoring wrong pointing object 214 0 (offset 0)\n",
      "Ignoring wrong pointing object 216 0 (offset 0)\n",
      "Ignoring wrong pointing object 218 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n",
      "Ignoring wrong pointing object 230 0 (offset 0)\n",
      "Ignoring wrong pointing object 238 0 (offset 0)\n",
      "Ignoring wrong pointing object 240 0 (offset 0)\n",
      "Ignoring wrong pointing object 249 0 (offset 0)\n",
      "Ignoring wrong pointing object 251 0 (offset 0)\n",
      "Ignoring wrong pointing object 253 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 265 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 276 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 292 0 (offset 0)\n",
      "Ignoring wrong pointing object 294 0 (offset 0)\n",
      "Ignoring wrong pointing object 303 0 (offset 0)\n",
      "Ignoring wrong pointing object 305 0 (offset 0)\n",
      "Ignoring wrong pointing object 307 0 (offset 0)\n",
      "Ignoring wrong pointing object 310 0 (offset 0)\n",
      "Ignoring wrong pointing object 313 0 (offset 0)\n",
      "Ignoring wrong pointing object 317 0 (offset 0)\n",
      "Ignoring wrong pointing object 319 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2768/3016 [25:27<11:12,  2.71s/it]Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2770/3016 [25:28<07:29,  1.83s/it]Ignoring wrong pointing object 225 0 (offset 0)\n",
      "Ignoring wrong pointing object 312 0 (offset 0)\n",
      "Ignoring wrong pointing object 330 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2771/3016 [25:29<06:51,  1.68s/it]Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2774/3016 [25:31<04:15,  1.06s/it]Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2775/3016 [25:33<05:47,  1.44s/it]Ignoring wrong pointing object 159 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 329 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2777/3016 [25:36<04:57,  1.24s/it]Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 180 0 (offset 0)\n",
      "Ignoring wrong pointing object 225 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2779/3016 [25:37<03:47,  1.04it/s]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2781/3016 [25:38<02:32,  1.54it/s]Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 152 0 (offset 0)\n",
      "Ignoring wrong pointing object 168 0 (offset 0)\n",
      "Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2782/3016 [25:40<04:30,  1.16s/it]Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n",
      "Ignoring wrong pointing object 233 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2784/3016 [25:42<03:52,  1.00s/it]Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2785/3016 [25:43<03:08,  1.22it/s]Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2786/3016 [25:44<03:21,  1.14it/s]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  92%|█████████▏| 2789/3016 [26:03<14:02,  3.71s/it]Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 337 0 (offset 0)\n",
      "Ignoring wrong pointing object 389 0 (offset 0)\n",
      "Ignoring wrong pointing object 391 0 (offset 0)\n",
      "Ignoring wrong pointing object 616 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2792/3016 [26:06<06:50,  1.83s/it]Ignoring wrong pointing object 207 0 (offset 0)\n",
      "Ignoring wrong pointing object 232 0 (offset 0)\n",
      "Ignoring wrong pointing object 359 0 (offset 0)\n",
      "Ignoring wrong pointing object 396 0 (offset 0)\n",
      "Ignoring wrong pointing object 415 0 (offset 0)\n",
      "Ignoring wrong pointing object 573 0 (offset 0)\n",
      "Ignoring wrong pointing object 672 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2793/3016 [26:08<07:33,  2.03s/it]Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2795/3016 [26:10<04:53,  1.33s/it]Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 233 0 (offset 0)\n",
      "Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2803/3016 [26:14<02:07,  1.67it/s]Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 296 0 (offset 0)\n",
      "Ignoring wrong pointing object 347 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2804/3016 [26:16<03:12,  1.10it/s]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2806/3016 [26:17<02:20,  1.49it/s]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2807/3016 [26:17<02:08,  1.62it/s]Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2808/3016 [26:18<02:25,  1.43it/s]Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 337 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2809/3016 [26:21<04:59,  1.45s/it]Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 383 0 (offset 0)\n",
      "Ignoring wrong pointing object 391 0 (offset 0)\n",
      "Ignoring wrong pointing object 395 0 (offset 0)\n",
      "Ignoring wrong pointing object 397 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2811/3016 [26:23<03:56,  1.15s/it]Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 152 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2812/3016 [26:25<04:55,  1.45s/it]Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Ignoring wrong pointing object 259 0 (offset 0)\n",
      "Ignoring wrong pointing object 316 0 (offset 0)\n",
      "Ignoring wrong pointing object 362 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2813/3016 [26:27<05:27,  1.61s/it]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 208 0 (offset 0)\n",
      "Ignoring wrong pointing object 211 0 (offset 0)\n",
      "Ignoring wrong pointing object 225 0 (offset 0)\n",
      "Ignoring wrong pointing object 264 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2815/3016 [26:31<05:20,  1.59s/it]Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2816/3016 [26:31<04:34,  1.37s/it]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 182 0 (offset 0)\n",
      "Ignoring wrong pointing object 322 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 357 0 (offset 0)\n",
      "Ignoring wrong pointing object 593 0 (offset 0)\n",
      "Ignoring wrong pointing object 595 0 (offset 0)\n",
      "Ignoring wrong pointing object 597 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2817/3016 [26:32<03:53,  1.17s/it]Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 208 0 (offset 0)\n",
      "Ignoring wrong pointing object 360 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2818/3016 [26:35<05:56,  1.80s/it]Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Extracting text from pdfs:  93%|█████████▎| 2819/3016 [26:37<05:54,  1.80s/it]Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 304 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2820/3016 [26:39<05:38,  1.73s/it]Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 373 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2821/3016 [26:41<05:43,  1.76s/it]Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n",
      "Ignoring wrong pointing object 344 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2822/3016 [26:42<05:32,  1.71s/it]Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 240 0 (offset 0)\n",
      "Ignoring wrong pointing object 277 0 (offset 0)\n",
      "Ignoring wrong pointing object 305 0 (offset 0)\n",
      "Ignoring wrong pointing object 325 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 457 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2823/3016 [26:45<06:19,  1.97s/it]Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 157 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 190 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 276 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 288 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2825/3016 [26:48<05:14,  1.65s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2826/3016 [27:16<30:07,  9.51s/it]Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 144 0 (offset 0)\n",
      "Ignoring wrong pointing object 218 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▎| 2827/3016 [27:17<21:51,  6.94s/it]Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 150 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 174 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 202 0 (offset 0)\n",
      "Ignoring wrong pointing object 216 0 (offset 0)\n",
      "Ignoring wrong pointing object 225 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 243 0 (offset 0)\n",
      "Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 356 0 (offset 0)\n",
      "Ignoring wrong pointing object 409 0 (offset 0)\n",
      "Ignoring wrong pointing object 420 0 (offset 0)\n",
      "Ignoring wrong pointing object 423 0 (offset 0)\n",
      "Ignoring wrong pointing object 425 0 (offset 0)\n",
      "Ignoring wrong pointing object 438 0 (offset 0)\n",
      "Ignoring wrong pointing object 444 0 (offset 0)\n",
      "Ignoring wrong pointing object 447 0 (offset 0)\n",
      "Ignoring wrong pointing object 461 0 (offset 0)\n",
      "Ignoring wrong pointing object 464 0 (offset 0)\n",
      "Ignoring wrong pointing object 468 0 (offset 0)\n",
      "Ignoring wrong pointing object 472 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2829/3016 [27:18<11:33,  3.71s/it]Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 297 0 (offset 0)\n",
      "Ignoring wrong pointing object 300 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2830/3016 [27:19<08:46,  2.83s/it]Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2831/3016 [27:19<06:31,  2.11s/it]Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 248 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2832/3016 [27:23<08:05,  2.64s/it]Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2833/3016 [27:23<05:57,  1.96s/it]Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2834/3016 [27:28<08:13,  2.71s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 143 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Ignoring wrong pointing object 353 0 (offset 0)\n",
      "Ignoring wrong pointing object 403 0 (offset 0)\n",
      "Ignoring wrong pointing object 445 0 (offset 0)\n",
      "Ignoring wrong pointing object 450 0 (offset 0)\n",
      "Ignoring wrong pointing object 453 0 (offset 0)\n",
      "Ignoring wrong pointing object 461 0 (offset 0)\n",
      "Ignoring wrong pointing object 540 0 (offset 0)\n",
      "Ignoring wrong pointing object 682 0 (offset 0)\n",
      "Ignoring wrong pointing object 840 0 (offset 0)\n",
      "Ignoring wrong pointing object 842 0 (offset 0)\n",
      "Ignoring wrong pointing object 849 0 (offset 0)\n",
      "Ignoring wrong pointing object 919 0 (offset 0)\n",
      "Ignoring wrong pointing object 921 0 (offset 0)\n",
      "Ignoring wrong pointing object 923 0 (offset 0)\n",
      "Ignoring wrong pointing object 963 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2835/3016 [27:30<07:53,  2.61s/it]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2836/3016 [27:32<07:02,  2.35s/it]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2838/3016 [27:33<04:19,  1.46s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2840/3016 [27:34<02:44,  1.07it/s]Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 285 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2842/3016 [27:38<03:51,  1.33s/it]Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2844/3016 [27:40<03:47,  1.32s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2847/3016 [27:48<06:39,  2.37s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Extracting text from pdfs:  94%|█████████▍| 2849/3016 [27:48<04:06,  1.47s/it]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2851/3016 [27:50<03:14,  1.18s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2852/3016 [27:51<03:07,  1.14s/it]Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2854/3016 [27:51<01:59,  1.35it/s]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2855/3016 [27:52<02:26,  1.10it/s]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2856/3016 [27:54<02:40,  1.00s/it]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2857/3016 [27:55<02:55,  1.10s/it]Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2858/3016 [27:57<03:08,  1.19s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2859/3016 [27:57<02:41,  1.03s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2860/3016 [27:58<02:45,  1.06s/it]Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2861/3016 [28:00<03:12,  1.24s/it]Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2862/3016 [28:01<02:52,  1.12s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2863/3016 [28:02<03:03,  1.20s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▍| 2864/3016 [28:06<04:34,  1.81s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2866/3016 [28:07<03:15,  1.30s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2868/3016 [28:11<03:58,  1.61s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2870/3016 [28:28<10:44,  4.41s/it]Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2871/3016 [28:31<09:51,  4.08s/it]Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2872/3016 [28:31<07:10,  2.99s/it]Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Ignoring wrong pointing object 281 0 (offset 0)\n",
      "Ignoring wrong pointing object 290 0 (offset 0)\n",
      "Ignoring wrong pointing object 398 0 (offset 0)\n",
      "Ignoring wrong pointing object 402 0 (offset 0)\n",
      "Ignoring wrong pointing object 404 0 (offset 0)\n",
      "Ignoring wrong pointing object 412 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2873/3016 [28:37<09:19,  3.91s/it]Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 152 0 (offset 0)\n",
      "Ignoring wrong pointing object 154 0 (offset 0)\n",
      "Ignoring wrong pointing object 168 0 (offset 0)\n",
      "Ignoring wrong pointing object 210 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 249 0 (offset 0)\n",
      "Ignoring wrong pointing object 306 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Ignoring wrong pointing object 336 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2874/3016 [28:40<08:38,  3.65s/it]Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 111 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2876/3016 [28:43<05:52,  2.52s/it]Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 251 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2878/3016 [28:48<05:22,  2.34s/it]Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2879/3016 [28:50<05:06,  2.24s/it]Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Extracting text from pdfs:  95%|█████████▌| 2880/3016 [28:50<03:49,  1.69s/it]Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2881/3016 [28:53<04:25,  1.97s/it]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2882/3016 [28:53<03:19,  1.49s/it]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2884/3016 [28:58<04:00,  1.82s/it]Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2885/3016 [28:59<03:32,  1.63s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2886/3016 [29:00<03:07,  1.44s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 144 0 (offset 0)\n",
      "Ignoring wrong pointing object 188 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2887/3016 [29:03<04:11,  1.95s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2888/3016 [29:05<03:58,  1.86s/it]Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2889/3016 [29:07<04:18,  2.04s/it]Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2890/3016 [29:09<03:57,  1.89s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2891/3016 [29:10<03:36,  1.73s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2892/3016 [29:13<04:29,  2.17s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2893/3016 [29:15<04:10,  2.04s/it]Ignoring wrong pointing object 1023 0 (offset 0)\n",
      "Ignoring wrong pointing object 1033 0 (offset 0)\n",
      "Ignoring wrong pointing object 1043 0 (offset 0)\n",
      "Ignoring wrong pointing object 1052 0 (offset 0)\n",
      "Ignoring wrong pointing object 1055 0 (offset 0)\n",
      "Ignoring wrong pointing object 1059 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2894/3016 [29:16<03:47,  1.87s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2896/3016 [29:17<02:08,  1.07s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2897/3016 [29:17<01:42,  1.16it/s]Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▌| 2900/3016 [29:22<02:23,  1.24s/it]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2904/3016 [29:23<01:11,  1.57it/s]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 143 0 (offset 0)\n",
      "Ignoring wrong pointing object 145 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Ignoring wrong pointing object 353 0 (offset 0)\n",
      "Ignoring wrong pointing object 403 0 (offset 0)\n",
      "Ignoring wrong pointing object 445 0 (offset 0)\n",
      "Ignoring wrong pointing object 450 0 (offset 0)\n",
      "Ignoring wrong pointing object 453 0 (offset 0)\n",
      "Ignoring wrong pointing object 461 0 (offset 0)\n",
      "Ignoring wrong pointing object 540 0 (offset 0)\n",
      "Ignoring wrong pointing object 682 0 (offset 0)\n",
      "Ignoring wrong pointing object 840 0 (offset 0)\n",
      "Ignoring wrong pointing object 842 0 (offset 0)\n",
      "Ignoring wrong pointing object 849 0 (offset 0)\n",
      "Ignoring wrong pointing object 919 0 (offset 0)\n",
      "Ignoring wrong pointing object 921 0 (offset 0)\n",
      "Ignoring wrong pointing object 923 0 (offset 0)\n",
      "Ignoring wrong pointing object 963 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2905/3016 [29:26<02:03,  1.11s/it]Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 210 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2906/3016 [29:27<02:12,  1.21s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2907/3016 [29:28<02:05,  1.15s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2908/3016 [29:53<14:07,  7.85s/it]Ignoring wrong pointing object 59 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 276 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 430 0 (offset 0)\n",
      "Ignoring wrong pointing object 567 0 (offset 0)\n",
      "Ignoring wrong pointing object 640 0 (offset 0)\n",
      "Ignoring wrong pointing object 911 0 (offset 0)\n",
      "Ignoring wrong pointing object 1097 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2909/3016 [30:01<14:26,  8.10s/it]Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 133 0 (offset 0)\n",
      "Ignoring wrong pointing object 224 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 486 0 (offset 0)\n",
      "Ignoring wrong pointing object 717 0 (offset 0)\n",
      "Ignoring wrong pointing object 727 0 (offset 0)\n",
      "Ignoring wrong pointing object 745 0 (offset 0)\n",
      "Ignoring wrong pointing object 964 0 (offset 0)\n",
      "Ignoring wrong pointing object 1016 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1177 0 (offset 0)\n",
      "Ignoring wrong pointing object 1755 0 (offset 0)\n",
      "Extracting text from pdfs:  96%|█████████▋| 2910/3016 [30:16<17:36,  9.97s/it]Ignoring wrong pointing object 111 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2911/3016 [30:26<17:22,  9.93s/it]Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 391 0 (offset 0)\n",
      "Ignoring wrong pointing object 547 0 (offset 0)\n",
      "Ignoring wrong pointing object 819 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2912/3016 [30:31<14:55,  8.61s/it]Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2913/3016 [30:32<10:58,  6.39s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2914/3016 [30:33<08:06,  4.77s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2915/3016 [30:34<05:59,  3.56s/it]Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2916/3016 [30:35<04:36,  2.76s/it]Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 159 0 (offset 0)\n",
      "Ignoring wrong pointing object 395 0 (offset 0)\n",
      "Ignoring wrong pointing object 455 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2917/3016 [30:39<05:16,  3.20s/it]Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n",
      "Ignoring wrong pointing object 232 0 (offset 0)\n",
      "Ignoring wrong pointing object 421 0 (offset 0)\n",
      "Ignoring wrong pointing object 817 0 (offset 0)\n",
      "Ignoring wrong pointing object 1062 0 (offset 0)\n",
      "Ignoring wrong pointing object 1213 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2919/3016 [30:46<04:57,  3.06s/it]Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 148 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 203 0 (offset 0)\n",
      "Ignoring wrong pointing object 211 0 (offset 0)\n",
      "Ignoring wrong pointing object 229 0 (offset 0)\n",
      "Ignoring wrong pointing object 237 0 (offset 0)\n",
      "Ignoring wrong pointing object 260 0 (offset 0)\n",
      "Ignoring wrong pointing object 271 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 300 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 319 0 (offset 0)\n",
      "Ignoring wrong pointing object 321 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Ignoring wrong pointing object 346 0 (offset 0)\n",
      "Ignoring wrong pointing object 354 0 (offset 0)\n",
      "Ignoring wrong pointing object 356 0 (offset 0)\n",
      "Ignoring wrong pointing object 370 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 382 0 (offset 0)\n",
      "Ignoring wrong pointing object 390 0 (offset 0)\n",
      "Ignoring wrong pointing object 407 0 (offset 0)\n",
      "Ignoring wrong pointing object 415 0 (offset 0)\n",
      "Ignoring wrong pointing object 439 0 (offset 0)\n",
      "Ignoring wrong pointing object 447 0 (offset 0)\n",
      "Ignoring wrong pointing object 454 0 (offset 0)\n",
      "Ignoring wrong pointing object 475 0 (offset 0)\n",
      "Ignoring wrong pointing object 484 0 (offset 0)\n",
      "Ignoring wrong pointing object 493 0 (offset 0)\n",
      "Ignoring wrong pointing object 502 0 (offset 0)\n",
      "Ignoring wrong pointing object 540 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2920/3016 [30:46<03:41,  2.31s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 293 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2921/3016 [30:54<06:25,  4.06s/it]Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2923/3016 [31:00<05:29,  3.55s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2924/3016 [31:01<04:20,  2.83s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2925/3016 [31:08<06:13,  4.11s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2927/3016 [31:09<03:10,  2.14s/it]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2929/3016 [31:09<01:47,  1.24s/it]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Extracting text from pdfs:  97%|█████████▋| 2932/3016 [31:09<00:58,  1.45it/s]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2945/3016 [31:19<00:36,  1.93it/s]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2948/3016 [31:21<00:36,  1.87it/s]Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2949/3016 [35:27<1:03:01, 56.44s/it]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2951/3016 [35:28<37:13, 34.36s/it]  Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 216 0 (offset 0)\n",
      "Ignoring wrong pointing object 240 0 (offset 0)\n",
      "Ignoring wrong pointing object 242 0 (offset 0)\n",
      "Ignoring wrong pointing object 255 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2953/3016 [35:37<23:19, 22.21s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2957/3016 [35:53<09:04,  9.24s/it]Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2959/3016 [36:02<06:34,  6.93s/it]Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 99 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2960/3016 [36:09<06:18,  6.76s/it]Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 265 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 304 0 (offset 0)\n",
      "Ignoring wrong pointing object 321 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2961/3016 [36:14<05:48,  6.34s/it]Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2964/3016 [38:25<25:36, 29.56s/it]Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 109 0 (offset 0)\n",
      "Ignoring wrong pointing object 111 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 122 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 144 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 148 0 (offset 0)\n",
      "Ignoring wrong pointing object 150 0 (offset 0)\n",
      "Ignoring wrong pointing object 180 0 (offset 0)\n",
      "Ignoring wrong pointing object 182 0 (offset 0)\n",
      "Ignoring wrong pointing object 221 0 (offset 0)\n",
      "Ignoring wrong pointing object 223 0 (offset 0)\n",
      "Ignoring wrong pointing object 239 0 (offset 0)\n",
      "Ignoring wrong pointing object 241 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 284 0 (offset 0)\n",
      "Ignoring wrong pointing object 300 0 (offset 0)\n",
      "Ignoring wrong pointing object 309 0 (offset 0)\n",
      "Ignoring wrong pointing object 318 0 (offset 0)\n",
      "Ignoring wrong pointing object 591 0 (offset 0)\n",
      "Ignoring wrong pointing object 621 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2966/3016 [38:31<13:09, 15.79s/it]Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 245 0 (offset 0)\n",
      "Ignoring wrong pointing object 277 0 (offset 0)\n",
      "Ignoring wrong pointing object 279 0 (offset 0)\n",
      "Ignoring wrong pointing object 281 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 317 0 (offset 0)\n",
      "Ignoring wrong pointing object 336 0 (offset 0)\n",
      "Ignoring wrong pointing object 338 0 (offset 0)\n",
      "Ignoring wrong pointing object 340 0 (offset 0)\n",
      "Ignoring wrong pointing object 342 0 (offset 0)\n",
      "Ignoring wrong pointing object 344 0 (offset 0)\n",
      "Ignoring wrong pointing object 367 0 (offset 0)\n",
      "Ignoring wrong pointing object 369 0 (offset 0)\n",
      "Ignoring wrong pointing object 371 0 (offset 0)\n",
      "Ignoring wrong pointing object 373 0 (offset 0)\n",
      "Ignoring wrong pointing object 376 0 (offset 0)\n",
      "Ignoring wrong pointing object 416 0 (offset 0)\n",
      "Ignoring wrong pointing object 418 0 (offset 0)\n",
      "Ignoring wrong pointing object 420 0 (offset 0)\n",
      "Ignoring wrong pointing object 422 0 (offset 0)\n",
      "Ignoring wrong pointing object 424 0 (offset 0)\n",
      "Ignoring wrong pointing object 426 0 (offset 0)\n",
      "Ignoring wrong pointing object 428 0 (offset 0)\n",
      "Ignoring wrong pointing object 430 0 (offset 0)\n",
      "Ignoring wrong pointing object 432 0 (offset 0)\n",
      "Ignoring wrong pointing object 434 0 (offset 0)\n",
      "Ignoring wrong pointing object 448 0 (offset 0)\n",
      "Ignoring wrong pointing object 450 0 (offset 0)\n",
      "Ignoring wrong pointing object 452 0 (offset 0)\n",
      "Ignoring wrong pointing object 475 0 (offset 0)\n",
      "Ignoring wrong pointing object 477 0 (offset 0)\n",
      "Ignoring wrong pointing object 479 0 (offset 0)\n",
      "Ignoring wrong pointing object 481 0 (offset 0)\n",
      "Ignoring wrong pointing object 504 0 (offset 0)\n",
      "Ignoring wrong pointing object 506 0 (offset 0)\n",
      "Ignoring wrong pointing object 597 0 (offset 0)\n",
      "Ignoring wrong pointing object 599 0 (offset 0)\n",
      "Ignoring wrong pointing object 689 0 (offset 0)\n",
      "Ignoring wrong pointing object 691 0 (offset 0)\n",
      "Ignoring wrong pointing object 693 0 (offset 0)\n",
      "Ignoring wrong pointing object 714 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2967/3016 [38:35<10:03, 12.31s/it]Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2968/3016 [38:39<07:43,  9.65s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs:  98%|█████████▊| 2969/3016 [38:43<06:19,  8.07s/it]Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▊| 2971/3016 [38:47<03:42,  4.95s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▊| 2975/3016 [39:01<02:26,  3.57s/it]Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 187 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▊| 2976/3016 [43:12<51:58, 77.97s/it]Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▊| 2977/3016 [43:16<36:05, 55.53s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2979/3016 [43:23<18:02, 29.26s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2980/3016 [43:26<12:43, 21.19s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 79 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 96 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 106 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 121 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 129 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 162 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 187 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 201 0 (offset 0)\n",
      "Ignoring wrong pointing object 211 0 (offset 0)\n",
      "Ignoring wrong pointing object 246 0 (offset 0)\n",
      "Ignoring wrong pointing object 248 0 (offset 0)\n",
      "Ignoring wrong pointing object 308 0 (offset 0)\n",
      "Ignoring wrong pointing object 327 0 (offset 0)\n",
      "Ignoring wrong pointing object 355 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2982/3016 [43:35<07:11, 12.69s/it]Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 66 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 104 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 169 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 174 0 (offset 0)\n",
      "Ignoring wrong pointing object 176 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 257 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 265 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2983/3016 [43:39<05:37, 10.21s/it]Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2984/3016 [43:41<04:01,  7.55s/it]Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2985/3016 [43:46<03:31,  6.82s/it]Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2989/3016 [47:34<11:34, 25.73s/it]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2990/3016 [47:34<07:49, 18.08s/it]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2991/3016 [47:34<05:17, 12.68s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Extracting text from pdfs:  99%|█████████▉| 2996/3016 [47:36<01:12,  3.64s/it]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Extracting text from pdfs: 100%|█████████▉| 3003/3016 [47:36<00:17,  1.38s/it]Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Extracting text from pdfs: 100%|█████████▉| 3006/3016 [47:36<00:10,  1.03s/it]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Extracting text from pdfs: 100%|█████████▉| 3008/3016 [47:36<00:06,  1.19it/s]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Extracting text from pdfs: 100%|█████████▉| 3012/3016 [47:41<00:03,  1.03it/s]Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 217 0 (offset 0)\n",
      "Ignoring wrong pointing object 219 0 (offset 0)\n",
      "Ignoring wrong pointing object 279 0 (offset 0)\n",
      "Ignoring wrong pointing object 1051 0 (offset 0)\n",
      "Ignoring wrong pointing object 1815 0 (offset 0)\n",
      "Ignoring wrong pointing object 2227 0 (offset 0)\n",
      "Ignoring wrong pointing object 2428 0 (offset 0)\n",
      "Ignoring wrong pointing object 2570 0 (offset 0)\n",
      "Ignoring wrong pointing object 2656 0 (offset 0)\n",
      "Ignoring wrong pointing object 3409 0 (offset 0)\n",
      "Ignoring wrong pointing object 6133 0 (offset 0)\n",
      "Ignoring wrong pointing object 7496 0 (offset 0)\n",
      "Ignoring wrong pointing object 7802 0 (offset 0)\n",
      "Ignoring wrong pointing object 8663 0 (offset 0)\n",
      "Ignoring wrong pointing object 18660 0 (offset 0)\n",
      "Ignoring wrong pointing object 18777 0 (offset 0)\n",
      "Ignoring wrong pointing object 25534 0 (offset 0)\n",
      "Ignoring wrong pointing object 36149 0 (offset 0)\n",
      "Extracting text from pdfs: 100%|██████████| 3016/3016 [56:15<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:24:58.660814Z",
     "start_time": "2024-11-10T22:24:58.506484Z"
    }
   },
   "cell_type": "code",
   "source": "save_sentences_to_file(sentences, \"dataset/\")",
   "id": "880037b84ac87e0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing sentences to file: 100%|██████████| 3016/3016 [00:00<00:00, 20159.49it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:24:58.672400Z",
     "start_time": "2024-11-10T22:24:58.661555Z"
    }
   },
   "cell_type": "code",
   "source": "sentences[0:10]",
   "id": "f473d09542855c9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[\\'Identification of Key Information with Topic Analysis\\\\non Large Unstructured Text Data\\\\nB A C H E L O R T H E S I S\\\\nDepartment of Electrical Engineering and Computer Science\\\\nUniversity of Kassel\\\\nAuthor Name: Klara Maximiliane Gutekunst\\\\nAddress: Gartenstraße 23\\\\n34125 Kassel\\\\nMatriculation number: 35677772\\\\nE-Mail: klara.gutekunst@student.uni-kassel.de\\\\nDepartment: Chair Intelligent Embedded Systems\\\\nExamining board 1: Prof. Dr. Bernhard Sick\\\\nExamining board 2: Prof. Dr. Gerd Stumme\\\\nSupervisor: Dr. Christian Gruhl\\\\nDate: 21. November 2023\\', \\'\\', \\'Abstract iii\\\\nAbstract\\\\nThe goal of this thesis is to investigate the applicability of computational means to the\\\\nexploration of large unstructured text corpora. Finding relevant documents and intercon-\\\\nnections between documents becomes significantly more difficult due to the sheer amount\\\\nof documents available. Institutes, such as the German tax offices, have access to leak\\\\ndata, for instance, the Panama Papers or theBahamas leak , containing huge amounts of\\\\ndocuments and valuable information yet to be extracted. However, these institutes, com-\\\\npanies and individuals do not have sufficient resources to explore individual documents\\\\nin order to find a specific one or to identify inherent key topics. Hence, computational\\\\nmeans, such as text mining or topic analysis, may help to overcome this obstacle. This\\\\nthesis proposes an approach to finding relevant documents which share common topics\\\\nfrom a large unstructured text corpus. The approach bundles different methods, such as\\\\ntextual embeddings, transformation of images and clustering techniques. As a result of this\\\\nwork, a web interface that enables the comparison of the methods examined via queries\\\\nfor similar documents to a database is provided.\\', \\'\\', \\'Zusammenfassung v\\\\nZusammenfassung\\\\nDas Auffinden relevanter Dokumente und von Zusammenhängen zwischen Dokumenten\\\\nwird durch die enorme Menge an verfügbaren Dokumenten erheblich erschwert. Institutio-\\\\nnen, wie z.B. deutsche Finanzämter, haben Zugang zu Datenleaks, wie etwa den Panama\\\\nPapernoder dem Bahamas-Leak , die große Mengen an Dokumenten und wertvollen In-\\\\nformationen enthalten, die es zu extrahieren gilt. Diese Institute, Unternehmen und\\\\nEinzelpersonen verfügen jedoch nicht über ausreichende Ressourcen, um einzelne Doku-\\\\nmente zu durchsuchen, ein bestimmtes Dokument zu finden oder inhärente Themen zu\\\\nidentifizieren. Daher können computergestützte Verfahren wie Text Mining oderTopic\\\\nanalysis sie dabei unterstützen. In dieser Arbeit wird ein Ansatz vorgestellt, der in\\\\neinem großen unstrukturierten Textkorpus relevante Dokumente mit gemeinsamen The-\\\\nmen findet. Dieser Ansatz bündelt verschiedene Methoden, wie z.B. textuelle Embed-\\\\ndings, Transformation von Bildern und Clustering-Techniken. Als Ergebnis der in dieser\\\\nArbeit untersuchten Methoden wird eine Weboberfläche bereitgestellt, die Abfragen nach\\\\nähnlichen Dokumenten zum Vergleich der verschiedenen Methoden ermöglicht.\\', \\'\\', \\'Contents vii\\\\nContents\\\\n1 Introduction 1\\\\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\\\n1.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\\\n1.3 Own approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\\\n1.4 Structure of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\\\n2 Related work 5\\\\n3 Fundamentals 9\\\\n3.1 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\\\n3.1.1 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\\\n3.1.2 Stop-Word-Removal . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\\\n3.1.3 Stemming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\\\n3.1.4 Lemmatization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\\\n3.2 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\\\n3.2.1 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\\\n3.2.2 Term Frequency - Inverse Document Frequency . . . . . . . . . . . . 12\\\\n3.2.3 Document to Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\\\n3.2.4 Universal Sentence Encoder . . . . . . . . . . . . . . . . . . . . . . . 14\\\\n3.2.5 InferSent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\\\n3.2.6 Sentence-BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\\\n3.3 Similarity measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\\\n3.3.1 Euclidian distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\n3.3.2 Cosine Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\\\n3.4 Topic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\\\n3.4.1 Topic to Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\\\n3.4.2 Word clouds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\\\n3.5 Compression of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\\\n3.5.1 Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\\\n3.5.2 Eigenfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\\\n3.6 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\\\n3.6.1 DBSCAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\\\n3.6.2 OPTICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\\\n3.7 Software frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\\\n3.7.1 Elasticsearch database . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\\\n3.7.2 Flask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\', \\'Contents viii\\\\n3.7.3 Angular . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\\\n4 Own approach 31\\\\n4.1 Offline Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\\\n4.1.1 Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\\\n4.1.2 Eigendocs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\\\n4.1.3 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\\\n4.1.4 Clustering using OPTICS . . . . . . . . . . . . . . . . . . . . . . . . 42\\\\n4.1.5 Topic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\\\n4.1.6 Slurm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\\\n4.2 Web interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\n4.2.1 Backend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\n4.2.2 Frontend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\\\n4.3 Trade-off between memory and query time . . . . . . . . . . . . . . . . . . . 50\\\\n5 Evaluation 51\\\\n5.1 Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\\\n5.2 Eigendocs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\\\n5.3 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\\\n5.4 Clustering using OPTICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\\\n5.5 Comparison of models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\\\n5.6 Comparison with baseline topic analysis approach . . . . . . . . . . . . . . . 67\\\\n6 Conclusion 69\\\\n6.1 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\\\n6.2 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\\\n7 Outlook 73\\\\nBibliography xi\\', \\'List of abbreviations ix\\\\nList of abbreviations\\\\nACID Atomicity, Consistency, Isolation, Durability\\\\nAE Autoencoder\\\\nBERT Bidirectional Encoder Representations from Transformers\\\\nBiLSTM Bi-directional Long Short-Term Memory\\\\nBoW Bag of Words\\\\nCBOW Continuous-Bag-of-Words\\\\nCSS Cascading Style Sheet\\\\nCSV Comma Separated Values\\\\nDoc2Vec Document to Vector\\\\nDAN Deep Averaging Network\\\\nDBSCAN Density-Based Spatial Clustering of Applications with Noise\\\\nDNN Deep Neural Network\\\\nGB Gigabyte\\\\nGloVe Global Vectors\\\\nHDBSCAN Hierarchical DBSCAN\\\\nHNSW Hierarchical Navigable Small World\\\\nIDF Inverse Document Frequency\\\\nIES Intelligent Embedded Systems\\\\nIR Information Retrieval\\\\nJSON JavaScript Object Notation\\\\nKL Karhonen-Loéve\\\\nkNN k-Nearest Neighbour\\\\nLDA Latent Dirichlet Allocation\\\\nLSTM Long Short-Term Memory\\\\nML Machine Learning\\\\nNLP Natural Language Processing\\\\nNN Neural Network\\\\nNoSQL Not only SQL\\\\nOPTICS Ordering Points To Identify Clustering Structure\\\\nPCA Principal Component Analysis\\\\nPKL Pickle\\\\nPV-DBOW Distributed Bag of Words\\\\nPVDM Paragraph Vector Distributed Memory\\\\nRMSE Root Mean Square Error\\\\nRNN Recurrent Neural Network\\\\nRSME Root Mean Square Error\\', \\'List of abbreviations x\\\\nSBERT Sentence-BERT\\\\nSNLI Stanford Natural Language Inference\\\\nSQL Structured Query Language\\\\nSVD Singular Value Decomposition\\\\nSVM Support Vector Machine\\\\nTop2Vec Topic to Vector\\\\nTF-IDF Term Frequency - Inverse Document Frequency\\\\nTF Term Frequency\\\\nUI User Interface\\\\nUMAP Uniform Manifold Approximation and Projection\\\\nURL Uniform Resource Locator\\\\nUSE Universal Sentence Encoder\\\\nVSM Vector Space Model\\\\nWord2Vec Word to Vector\\', \\'1 Introduction 1\\\\n1 Introduction\\\\nThis thesis addresses the task of analysing large unstructured text corpora. Their explo-\\\\nration is challenging due to the heterogeneous nature of the data, i.e. different formats and\\\\nlayouts, and the amount of information. It is not possible to find a group of semantically\\\\nsimilar documents by traversing corpora manually. Therefore, this thesis explores different\\\\napproaches to support the exploration of large unstructured text corpora by computational\\\\nmeans with the goal of grouping documents according to their similarity.\\\\nThe dataset inspected in this thesis is the so-called Bahamas leak . It is a collection of\\\\nroughly 38 Gigabyte (GB) of fiscal documents, which were leaked in 2016 [49]. The doc-\\\\numents are unstructured, i.e. they are of different types, content and layout. They are\\\\nrelevant in the context of tax fraud since they contain information about offshore compa-\\\\nnies and their owners. Tax offices examine this dataset to identify tax evasion. However,\\\\nit has proven to be challenging to identify relevant documents and their interconnections\\\\ndue to the amount of documents contained in the leak.\\\\n1.1 Motivation\\\\nOn a broader scope, this thesis aims to provide computational means to facilitate the\\\\nwork with large unstructured text data for humans. The goal is to actively use Machine\\\\nLearning (ML) techniques to analyze a large text corpus and to reduce the amount of\\\\nmanual human work.\\\\nIn the context of tax fraud, large unstructured text data, such as the Bahamas leak is\\\\nexamined by tax offices. However, tax offices do not have sufficient resources to examine\\\\nall documents to find an object of interest, for instance, an invoice. Hence, ML techniques\\\\nought to facilitate the work of investigators by reducing manual labor. These methods\\\\nshould propose visually, i.e. of the same document type, or semantically, i.e. from the same\\\\ncompany, similar documents to the investigator.\\\\n1.2 Research Questions\\\\nIn order to support the exploration of large unstructured text data, this thesis aims to\\\\nprovide computational means to facilitate the work with large corpora. In this work,\\\\ndifferent methods to derive semantic and visual information from unstructured text data\\', \\'1 Introduction 2\\\\nare applied. These techniques ought to be compared and evaluated. In the following, the\\\\nresearch questions addressed are defined:\\\\nRQ1.Is it possible to use a visual representation to find similar documents in\\\\nthe corpus?\\\\nAssuming that it is valuable to explore documents of similar type, for instance,\\\\ninvoices, simultaneously, the system should be able to find similar documents with\\\\nrespect to their visual appearance. It remains to be seen whether encodings of the\\\\nvisual appearance of a document are sufficient to find similar documents.\\\\nRQ2.Do different embedding methods produce similar results?\\\\nThe task at hand defines a result as a set of response documents similar to a query\\\\ndocument. Hence, one has to compare response sets of different methods. The sim-\\\\nilarity between to response documents can be evaluated with respect to the content\\\\nor the visual appearance of the documents.\\\\nRQ3.How are the results of the system presented to experts?\\\\nThis question aims to find a suitable way to present the results of the system to the\\\\nuser in an intuitive manner.\\\\nRQ4.How can the performance of the system be evaluated?\\\\nSince the dataset is not labeled, the performance of the system cannot be evaluated\\\\nwith respect to a ground truth. Hence, other means of evaluation have to be found.\\\\nThese techniques could include time measurements or qualitative analysis of the\\\\nquery responses.\\\\n1.3 Own approach\\\\nThis thesis proposes an approach to group documents based on their appearance or seman-\\\\ntic similarity, which is defined via different embedding strategies, i.e. methods to derive\\\\nembeddings from texts. Embeddings are numerical representations of words, sentences or\\\\ntexts. They enable the comparison of heterogeneous data via cosine similarity, i.e. the\\\\nangle between embedding vectors, whereas visual information is clustered using different\\\\napproaches including OPTICS (cf. Subsection 3.6.2) beforehand. The resulting groups of\\\\ndocuments are visualized using word clouds.\\\\nThis work’s goals include the implementation of a User Interface (UI) for the techniques\\\\nexamined. However, this UI is not supposed to be an operational application for end users\\\\nfrom the tax office but serves the purpose of displaying the techniques examined. It should\\\\nassist the natural human approach to exploration: A human finds a document of interest,\\\\nfor instance, by keyword search, and thus, wants to find similar documents. The tool\\\\nshould support keyword search, a detailed inspection of a document of interest and the\\\\nexploration of similar documents.\\', \\'1 Introduction 3\\\\n1.4 Structure of the Thesis\\\\nThe thesis is structured as follows.\\\\nChapter 1\\\\nFirstly, the problem of working with large unstructured text corpora is introduced.\\\\nSecondly, thedatasetusedinthisthesisisdescribed. Moreover, thegoalofthisthesis,\\\\nas well as the target audience of the problem investigated is stated. Afterwards, the\\\\nmotivation and research questions are presented. The chapter concludes with an\\\\noutlook on the techniques used and an overview of the thesis.\\\\nChapter 2\\\\nThis chapter covers related work where similar approaches are presented. Moreover,\\\\nthe chapter introduces the literature that serves as a basis for this thesis.\\\\nChapter 3\\\\nThe theoretical foundations of the techniques applied in this thesis are outlined in\\\\nChapter 3. The techniques can be divided into preprocessing (cf. Section 3.1),\\\\nsemantic embeddings (cf. Section 3.2), similarity measurements (cf. Section 3.3),\\\\ntopic analysis (cf. Section 3.4), compression of data (cf. Section 3.5), clustering\\\\nalgorithms (cf. Section 3.6) and software frameworks (cf. Section 3.7).\\\\nChapter 4\\\\nThis chapter describes the implementation of the methods. The implementation is\\\\nbased on the theoretical foundations presented in Chapter 3. On a more granular\\\\nlevel, this chapter covers the offline preprocessing (cf. Section 4.1), the implementa-\\\\ntion of the UI (cf. Section 4.2) and the trade-off between memory and query time in\\\\nSection 4.3.\\\\nChapter 5\\\\nThe evaluation of the methods is presented in this chapter. It gives a reason why\\\\ncertain parameter choices were made with respect to established parameter estima-\\\\ntion approaches. Moreover, it compares the different methods with regard to their\\\\nquery responses and the bundle of methods constructed in the course of this thesis\\\\nto an existing baseline topic analysis approach.\\\\nChapter 6\\\\nThis chapter concludes this thesis. The insights acquired by exploring different tech-\\\\nniques with the goal of the exploration of large unstructured text data are presented\\\\nand the research questions are revised in Section 6.1. In Section 6.2 the scientific\\\\ncontributions are highlighted.\\\\nChapter 7\\\\nThe last chapter gives an outlook on future work. It also includes a discussion of the\\\\nlimitations of this thesis.\\', \\'\\', \\'2 Related work 5\\\\n2 Related work\\\\nThis chapter examines and summarises different literature about topic analysis of text\\\\ncorpora and related fields. It presents a selection of textual embedding methods, visual\\\\ninformation encodings, dimensionality reduction methods, similar data corpora, similarity\\\\nmeasures and clustering methods. Moreover, a few topic analysis libraries are presented.\\\\nThe domain of Information Retrieval (IR) works on large datasets. Hence, multiple sci-\\\\nentific papers working with large corpora, such as [44], have been published. Research on\\\\n(large) fiscal datasets includes ML tasks such as anomaly detection to identify credit card\\\\nfraud [75, 48, 47, 39, 60, 27].\\\\nML techniques usually require numerical data as input. In order to utilize ML techniques,\\\\ntextualdataisoftenrepresentedasreal-valuedvectors. Dependingontheapproach, vectors\\\\neither represent single words, sentences or whole documents. The models used in this\\\\nwork are briefly introduced in the following. More detailed information can be found in\\\\nSection 3.2.\\\\nThe TF-IDF model is a widely used model for text representation. Even though Zhang\\\\net al. discuss TF-IDF’s drawbacks [77] the model is incorporated in this work due to its\\\\nsimplicity.\\\\nMikolov et al. discuss the well-established Word2Vec models CBOW and Skip-gram. The\\\\nauthors found that these Word2Vec models produce high-quality word embeddings on large\\\\ndatasets [44]. These Word2Vec models form the basis of so-called Doc2Vec models which\\\\nembed whole documents. The PVDM model extends the CBOW model to work on a set\\\\nof documents or paragraphs instead of words [78] and is used in this work.\\\\nA more complex model is the SBERT model [58]. This model is an extension of the BERT\\\\nmodel which set state-of-the-art results in many Natural Language Processing (NLP) tasks.\\\\nReimers and Gurevych show that BERT is not suitable for certain similarity measures, such\\\\nas cosine similarity. Moreover, they argue that SBERT overcomes BERT’s shortcomings.\\\\nSince the SBERT model is able to produce document embeddings, it is used in this work.\\\\nThe InferSent model is a sentence embedding model [14]. Conneau et al. state that it\\\\noutperforms models trained in an unsupervised fashion. They train it on a labeled dataset\\\\nand optimize the model’s architecture. Since the model is pretrained and open-source, it\\\\nis used in this work.\\\\nAnother embedding model of interest is the USE model [9]. Cer et al. propose two model\\\\narchitectures which respectively are either superior with regard to accuracy or resource\\', \\'2 Related work 6\\\\nconsumption. They claim that their model surpasses word-level embedding transfer learn-\\\\ning on several NLP tasks. Due to the fact that the pretrained models are open-source, the\\\\none that consumes fewer resources is used in this work.\\\\nThis thesis aims to encode visual information as low-dimensional real-valued vectors. Since\\\\nthe domain of face recognition deals with the task of deriving meaningful information from\\\\nhigh dimensional data, the Eigenfaces approach is adapted to document images in this\\\\nwork. The task of finding similar images of faces is transferred to finding similar document\\\\nimages. Eigenfaces projects face images into a lower-dimensional feature space which best\\\\nencodes the variation among the faces [67]. Since 1991 this technique has been covered in\\\\na lot of papers [67, 76, 71, 65, 16, 64].\\\\nAnowar et al. propose a survey on different dimensionality reduction techniques including\\\\nPCA, LDA and SVD [6]. They conceptually categorize and compare the techniques. The\\\\nauthors conduct experiments on different datasets to compare the techniques’ performance\\\\non classification tasks. They find that the classification accuracy obtained on the reduced\\\\nversion of the datasets is superior to the accuracy achieved on the original datasets. Their\\\\nwork serves as a theoretical foundation for Section 3.5.\\\\nAnother dimensionality reduction technique is an AE [46, 40]. The papers provide a\\\\ntheoretical foundation for Subsection 3.5.1. An AE learns a meaningful low-dimensional\\\\nrepresentation of the input. This representation is used as a compressed version of certain\\\\nembeddings in this work.\\\\nTo determine the similarity between two objects, one has to define a metric. Prevalent\\\\nmetrics in the domain of comparing objects in a Vector Space Model (VSM) include (soft)\\\\ncosine similarity outlined by Sidorov et al., as well as by Charlet and Damnati [62, 11],\\\\nthe Manhattan and the Euclidean norm [37]. Sidorov et al. propose the calculation of the\\\\nsoft similarity. Charlet and Damnati state that the soft cosine similarity is superior to the\\\\ncosine similarity since it takes into account the relations between words. When comparing\\\\ndifferent norms in the context of IR from images Khosla et al. find that the Manhattan\\\\nnorm has a better precision than the Euclidean norm. The similarity metric used in this\\\\nwork is the cosine similarity.\\\\nThe visual information of the document images shall be used to cluster them. Ankerst\\\\net al. introduce the clustering algorithm OPTICS which seems to be suitable for the task\\\\nat hand since it does not return an explicit clustering but a clustering structure [5]. More-\\\\nover, Ankerst et al. state that OPTICS is a method for database mining. Other researchers,\\\\nfor instance, Kanagala and Krishnaiah, compare related clustering algorithms including\\\\nK-Means, DBSCAN and OPTICS. They state that OPTICS overcomes DBSCAN’s dif-\\\\nficulties and K-Means limitations [35]. Patwary et al., Deng et al. and Agrawal et al.\\\\npropose OPTICS extensions for spatially and temporally evolving data or a parallel ver-\\\\nsion [54, 17, 2].\\', \\'2 Related work 7\\\\nThe methods explored in this thesis ought to be bundled into a tool. Some researchers have\\\\nalready developed complete topic analysis libraries whose functionalities can be compared\\\\nto the tool developed in this thesis. They merge a selection of the techniques stated above\\\\ninto a well-reasoned composite. BERTopic is a library that merges SBERT embeddings\\\\nwith UMAP dimension reduction, HDBSCAN clustering and the application of TF-IDF\\\\non the clusters [26].\\\\nOther well-established topic analysis approaches consist of LDA. Wang and Qian propose a\\\\ntechniquethatfirstappliesLDAtoreducethedata’sdimensionalityandthereafterclassifies\\\\nthe result with a Support Vector Machine (SVM) [72]. Similarly, Chen et al. use a kNN\\\\nalgorithm instead of a SVM on the textual subspace generated by LDA [12]. Another\\\\ntechnique proposed is LDA2VEC, which is subject to Churchill and Singh’s work [13].\\\\nChaney and Blei’s paper introduces an open-source library for topic model visualization,\\\\nexemplary showcased on a Wikipedia dataset [10].\\\\nAngelov and Niu and Dai claim that the Top2Vec model not only overcomes LDA’s short-\\\\ncomings [4, 52] but is also developed for topic analysis on a large collection of documents\\\\n[4]. The Top2Vec library serves as a baseline model for the application developed in this\\\\nwork.\\\\nIn contrast to the assumption of this thesis that the prevalent topics are static, in reality,\\\\ntopics may be dynamic or change over time. Not only Alghamdi and Alfalqi, but also\\\\nVayansky and Kumar have published surveys on topic analysis techniques, which take into\\\\naccount factors such as time [3, 69].\\\\nSome of the techniques that were briefly introduced in this chapter partake in the appli-\\\\ncation developed in this thesis. They are described in more detail in Chapter 3.\\', \\'\\', \\'3 Fundamentals 9\\\\n3 Fundamentals\\\\nThe following chapter outlines the theoretical principles of the methods used in this work.\\\\nFirst, the preprocessing of the data is described in Section 3.1. Then, a variety of ways\\\\nto generate numerical representations of textual data is outlined in Section 3.2. After-\\\\nwards, the different similarity measurements are introduced in Section 3.3. A selection of\\\\nconventional topic analysis approaches is outlined in Section 3.4. Subsequently, two data\\\\ncompression techniques are presented in Section 3.5. Then, Section 3.6 presents multiple\\\\nclustering methods. Finally, the libraries used to implement the web application and the\\\\ndatabase are introduced in Section 3.7.\\\\n3.1 Preprocessing\\\\nSimilartootherMLdomains, NLPrequirespreprocessingofthedata. Usually, textualdata\\\\ncontains irrelevant information and noise. Examples of noise include so-called stop words,\\\\nsuch as “the” or “and”. However, irrelevant information can be task-specific. In some cases,\\\\nnumerical data may be regarded to be irrelevant and should be omitted. Preprocessing\\\\nimproves the performance and the results [56]. The next sections describe the non-trivial\\\\npreprocessing steps applied in this work.\\\\n3.1.1 Tokenization\\\\nFigure 3.1: Tokenization visualized using an example text.\\\\nTokenization is the process of splitting a text into smaller pieces, so-called tokens. Tokens\\\\ncan be words and punctuation marks [8]. The definition of a token depends on the applica-\\\\ntion. For instance, certain tokenization implementations may identify tokens as subsequent\\\\nseries of non-whitespace characters omitting all numbers and punctuation marks [63].\\', \\'3 Fundamentals 10\\\\n3.1.2 Stop-Word-Removal\\\\nFigure 3.2: Stop-Word-Removal visualized using an example text.\\\\nOmitting words that are not relevant to the context of the text is called stop-word-removal .\\\\nStop words not only depend on the domain but also the language [63].\\\\n3.1.3 Stemming\\\\nIn order to avoid language inflections, i.e. treating words with similar meanings differently,\\\\nstemming is applied [56]. According to Bird et al., stemming is the process of stripping off\\\\nany affixes, i.e. prefixes and suffixes [63], from a word and returning the stem. Different\\\\ntypes of stemmers are better suited for certain applications than others. Hence, the choice\\\\nof the stemmer depends on the application.\\\\n3.1.4 Lemmatization\\\\nFigure 3.3: Lemmatization visualized using an example text.\\\\nStemming and lemmatization are used to reduce the vocabulary size [56]. Opposed to\\\\nstemming, lemmatization returns only stems that are considered valid words [8]. Some\\\\nimplementations of lemmatizers validate stems with regard to a set of valid words, i.e.\\\\nlemmas, stored in a dictionary. Lemmatizers are usually slower than stemmers [8].\\', \\'3 Fundamentals 11\\\\nTheWordNetLemmatizer from the nltkpackage1requires a vocabulary. According to\\\\nRadu et al., it is frequently used for lemmatization of English texts [56].\\\\n3.2 Embeddings\\\\nUsually, ML techniques require textual inputs to be converted to embeddings [43]. Em-\\\\nbeddings are numerical representations of words, sentences or texts. They can be used to\\\\npresent the textual data as real-valued vectors in a VSM. A simple example of a VSM in\\\\nthe NLP context is shown in Figure 3.4. A VSM is a N-dimensional space [62]. VSMs are\\\\ncommonly used due to their conceptual simplicity and because spatial proximity correlates\\\\nwith semantic proximity [77, 9, 58, 4]. Representations in a VSM can improve the perfor-\\\\nmance in NLP tasks [45]. The following section outlines the fundamentals of a selection of\\\\nembeddings.\\\\nFigure 3.4: A simple VSM. The words are represented as vectors in a two-dimensional\\\\nspace. Since wineis semantically more similar to drinkthan to food, the\\\\nvectors are closer together.\\\\n3.2.1 Neural Networks\\\\nA Neural Network (NN) is a ML model which consists of multiple layers of nodes. A node,\\\\nor so-called neuron, takes an input vector and produces an output vector. The output is\\\\nderived from the calculation of a weighted sum of the inputs and an activation function\\\\n[32]. The architecture of a NN is shown in Figure 3.5. The first and last layers are called\\\\ninput and output layers, respectively. The layers between the input and output layers are\\\\ncalled hidden layers. If a NN has more than one hidden layer, it is called a Deep Neural\\\\nNetwork (DNN) and working with DNNs is considered deep learning. To propagate the\\\\ninput through the network the layers are connected. In a feed-forward NN, the information\\\\nflows from the input layer to the output layer [31].\\\\nNNs are trained using the backpropagation algorithm which reduces the error between the\\\\npredicted and the actual output iteratively. While data is propagated in a forward direction\\\\n1https://www.nltk.org/_modules/nltk/stem/wordnet.html (last accessed: 12/11/2023)\\', \\'3 Fundamentals 12\\\\nFigure 3.5: Architecture of a NN. The input layer is the first layer of the network. It\\\\nreceives the input data x. The output layer is the last layer of the network and\\\\nreturnsy. Between the input and output layers, there are one or more hidden\\\\nlayers.\\\\nthrough the network, the error is propagated in a backward direction. The weights of the\\\\nlayers are adjusted according to the error [32].\\\\n3.2.2 Term Frequency - Inverse Document Frequency\\\\nTerm Frequency - Inverse Document Frequency (TF-IDF) provides a numerical representa-\\\\ntion of a word in a document. Let a corpus of documents be denoted D={d1, d2, ..., d M},\\\\nMbeing the total number of documents in the corpus. Let a sequence of terms wj∈Vbe\\\\ndenoted a document di={w1, w2, ...},Vbeing the vocabulary, i.e. set of distinct words\\\\n[56].\\\\nThe TF-IDF model considers the frequency fwj,dof a word wjin a document dand the\\\\nfrequency of a word in the whole corpus. The frequency fwj,dis defined in Equation 3.1,\\\\nw′\\\\njbeing the number of occurrences of wjind.\\\\nfwj,d=w′\\\\njP\\\\nk∈dw′\\\\nk(3.1)\\\\nTFIDF (wj, d, D ) =TF(wj, d)·IDF (wj, D) (3.2)\\\\nTF(wj, d) =fwj,d (3.3)\\\\nIDF (wj, D) = log2M\\\\nMj(3.4)\\', \\'3 Fundamentals 13\\\\nTF-IDF is calculated using Equation 3.2 from [56]. Each entry of a TF-IDF embedding\\\\nvector represents the TF-IDF value of a word in a document. Hence, the embedding\\\\nvector is of the same length as the vocabulary of the corpus. The Term Frequency (TF)\\\\nis determined utilizing Equation 3.3, whereas the Inverse Document Frequency (IDF) is\\\\ncomputed by Equation 3.4, Mjbeing the number of documents the term wjappears in.\\\\nIDF measures the importance of a term wjin the corpus of documents Dunder the\\\\nassumption that a term’s importance to the data corpus is inversely proportional to its\\\\noccurrence frequency [77]. In other words: Terms which appear in many documents are\\\\nnot as important and thus, weighted less than document-specific terms. The calculation\\\\nof TF and IDF is visualized exemplary in Figure 3.6.\\\\nFigure 3.6: Exemplary calculation of TF and IDF for a document corpus D: TF only\\\\nconsiders the documents of interest while IDF incorporates the importance of\\\\nthe word with respect to D.\\\\nTF-IDF has several drawbacks [56, 77]:\\\\n•TF-IDF does not consider semantic similarities between words.\\\\n•TF-IDF does not take into account the order of words in a document.\\\\n•TF-IDF often produces high dimensional representations which have to be postpro-\\\\ncessed to reduce their dimensionality, e.g., by using Principal Component Analy-\\\\nsis (PCA).\\\\n3.2.3 Document to Vector\\\\nAnother term used for Document to Vector (Doc2Vec) is Paragraph Vector [56, 43].\\\\nDoc2VecaddressesTF-IDF’sdrawbacksbyencodingtextsas N−dimensionalvectorslearnt\\\\nusing the words’ context [56]. N∈Ncan be chosen arbitrarily. It preserves semantic sim-\\\\nilarities between words and encodes linguistic regularities and patterns [45]. The model\\\\nhandlesinputsofdifferentlengths, i.e.inputscanbesentences, paragraphsordocuments.\\\\nDoc2Vec is an adaption of the Word to Vector (Word2Vec) model, which maps words\\\\ninto a VSM [56]. Both approaches assume that words appearing in similar contexts are\\', \\'3 Fundamentals 14\\\\n(a) CBOW architecture cf. [44].\\\\n (b) PVDM architecture cf. [43].\\\\nFigure 3.7: Both approaches predict the centre word w(t)using the context. PVDM is\\\\nan adaption of CBOW to work on a set of documents or paragraphs instead of\\\\nwords.\\\\nsemantically similar. Hence, words which often appear in the same context produce similar\\\\nembeddings.\\\\nThe Doc2Vec embedding is obtained using a shallow NN, i.e. the NN has only one hidden\\\\nlayer. The embeddings are created by the hidden layer. There are two Doc2Vec approaches\\\\nto designing the architecture of the NN:\\\\n•Paragraph Vector Distributed Memory (PVDM):\\\\nPredicts a word given a context [43, 44].\\\\n•Distributed Bag of Words (PV-DBOW):\\\\nPredicts the context given a word [38, 45, 43].\\\\nThe PVDM algorithm considers words within a sliding window and their document the\\\\ncontext of a centre word [43]. The document vector is added to incorporate the docu-\\\\nment’s topic and thus, acts like a memory [43, 4]. PVDM encodes the context words into\\\\nvectors via the Word2Vec Continuous-Bag-of-Words (CBOW) model [55]. Each document\\\\nis mapped to a vector using an additional document-to-vector matrix. The vectors can be\\\\nconcatenated, averaged or summed up [43]. The resulting vector is the prediction of the\\\\ncentral word. The CBOW and the PVDM approach are displayed in Figure 3.7.\\\\n3.2.4 Universal Sentence Encoder\\\\nCer et al. have published their Universal Sentence Encoder (USE) model on TensorFlow\\\\nHub. They propose two architectures, one based on a Transformer and one based on a\\\\nDeep Averaging Network (DAN) [9]. Both models’ input is a lowercase tokenized string.\\\\nTheir output is a 512-dimensional vector.\\\\nThe transformer model is more accurate and more complex than the DAN model [9].\\\\nThe transformer’s (self) attention is used to compute context-aware word embeddings,\\\\nwhich consider both the word order and their semantic identity. Since a sequence of\\', \\'3 Fundamentals 15\\\\nword embeddings of a sentence produces embeddings of different dimensions, the approach\\\\npostprocesses the word embeddings. A sentence vector is obtained by computing the\\\\nelement-wise sum of the word embeddings and normalizing the result by dividing by the\\\\nsquare root of the sentence length.\\\\nThe DAN model receives real-valued embeddings of words and bi-grams as input. A\\\\nbi-gram is a tupel of two subsequent words in a text [8], for instance, (red, wine), (wine,\\\\ntastes), (tastes, good) . The embeddings can be obtained from the text strings using models\\\\nsuch as the Bag of Words (BoW) model [21]. They are averaged and subsequently passed\\\\nto a feedforward DNN [9]. The architecture of the DAN model is depicted in Figure 3.8.\\\\nFigure 3.8: Architecture of the DAN model used for USE based on the textual description\\\\nfrom [14]. The input words and bi-grams (w1, w2, ..., w N)are embedded. The\\\\nembeddingsareaveragedandsubsequentlypassedtoafeedforwardDNN, which\\\\nproduces a 512-dimensional sentence embedding.\\\\nThe models are trained on both unsupervised training data, e.g., Wikipedia, and a su-\\\\npervised training dataset, i.e. Stanford Natural Language Inference (SNLI) [9, 58]. The\\\\nunsupervised training task is to predict the context given an input, i.e. Skip-Gram like\\\\ntasks. The supervised training task is classification [9].\\\\n3.2.5 InferSent\\\\nInferSent is a sentence embedding method trained in a supervised manner on the SNLI\\\\ndataset [14, 58]. The trained model is transferable to other tasks. Conneau et al. com-\\\\npare multiple architectures in their work. The Bi-directional Long Short-Term Mem-\\\\nory (BiLSTM) architecture with max pooling which was found to be the best option for\\\\nthe sentence encoder is depicted in Figure 3.9 [14].\\\\nA Long Short-Term Memory (LSTM) is a Recurrent Neural Network (RNN) that is capable\\\\nof learning long-term dependencies. RNNs have closed loops, i.e. feedback connections\\', \\'3 Fundamentals 16\\\\nbetween the nodes [59]. In other words, a LSTM is able to remember information as a\\\\nso-called state. Certain LSTM mechanisms control whether the current state is deleted,\\\\nwhether new data is saved and to what degree the current state contributes to the current\\\\ninput processed in the node. Hence, LSTM nodes are not only influenced by former outputs\\\\nbut also by their state. Since the LSTM computes different numbers of hidden vectors ht\\\\ndepending on the length of a sentence, a max pooling layer is applied to the hidden vectors\\\\nwhich selects the maximum value for a patch of the hidden vectors.\\\\nFigure 3.9: Architecture of the BiLSTM model with max pooling used for InferSent cf. [14].\\\\nThe input sentence (w1, w2, ..., w T)is read from both directions by a forward\\\\nand a backward LSTM producing− →htand← −htrespectively. After concatenating− →htand← −httoht, max pooling is applied. The output is a fixed-sized embedding.\\\\nAccording to Reimers and Gurevych, InferSent consists of a single BiLSTM layer [58].\\\\nGivenasentence (w1, w2, ..., w T)ofTwords, theBiLSTMarchitecturecomputesthehidden\\\\nrepresentations htfor each word wt. The hidden representation htis the concatenation of\\\\nthe forward and backward hidden vectors− →htand← −ht.− →htand← −htare produced by a forward\\\\nand backward LSTM respectively. Hence, the sentence is read from both directions and\\\\nthus, considers past and future context.\\\\n3.2.6 Sentence-BERT\\\\nSentence-BERT (SBERT) is an enhancement of Bidirectional Encoder Representations\\\\nfrom Transformers (BERT). The applicability of BERT is limited because it does not pro-\\\\nduce independent embeddings for single sentences [58]. Moreover, Reimers and Gurevych\\', \\'3 Fundamentals 17\\\\nfoundthatcommonsimilaritymeasurements, forinstance, theonesdiscussedinSection3.3,\\\\ndo not perform well on sentence embeddings produced by BERT.\\\\nBERTisapre-trainedtransformernetworkwhichpredictsatargetvaluebasedontwoinput\\\\nsentences for sentence classification or sentence-pair regression tasks [58]. The BERT base\\\\nmodel applies multi-head attention over 12 transformer layers, whereas the large model\\\\napplies multi-head attention over 24 transformer layers. The attention mechanism enables\\\\naccess to all hidden states as opposed to only the last hidden state [34]. It derives its output\\\\nvector as a dynamic weighted sum of the hidden states. The final label is derived from a\\\\nregression function, which receives the output of the 12thor24thlayer, respectively.\\\\nFigure 3.10: Architecture of SBERT cf. [58]. BERT is extended by a pooling layer. The\\\\ninput is a sentence and the output is a fixed-sized embedding.\\\\nSBERT provides fixed-sized embeddings for single sentences [58]. It differs from BERT in\\\\nterms of architecture, since it adds a pooling layer after the BERT model. Reimers and\\\\nGurevych compare different pooling strategies, such as using the output of the CLS(i.e.\\\\nfirst) token, mean pooling and max pooling. The architecture of a single SBERT network\\\\nis depicted in Figure 3.10. In order to work with multiple input sentences at the same time,\\\\nsiamese and triplet network architectures, i.e. multiple BERT networks with tied weights,\\\\nare constructed. To perform classification or inference tasks, layers are added on top of\\\\nthe SBERT network. SBERT is trained on the SNLI dataset [58, 28].\\\\nAccording to Reimers and Gurevych, SBERT outperforms InferSent and USE on Seman-\\\\ntic Textual Similarity tasks and on SentEval, which is an evaluation toolkit for sentence\\\\nembeddings [58].\\\\n3.3 Similarity measurement\\\\nEmbeddings not only facilitate human interpretability of relationships between texts, but\\\\nthey also enable the use of metrics, i.e. similarity measures, to quantify the similarity\\\\nbetween texts [63, 37].\\\\nThere are several similarity measures, such as the dot product quantifying the number of\\\\nshared tokens of two texts, the (soft) cosine similarity [62, 11], which is the normalized dot\\\\nproduct and calculates the angle between two vectors, and many more [63, 37, 58]. The\\\\nfollowing section outlines a selection of similarity measures.\\', \\'3 Fundamentals 18\\\\n3.3.1 Euclidian distance\\\\nTheeuclidian distance is a distance measure. In order to measure the distance between\\\\ntwo vectors in a N-dimensional space, the root of the sum of squared distances between the\\\\nrespective values of every dimension is calculated. The Euclidean (L2) norm between two\\\\nvectors a, bis defined in Equation 3.5 [37]. The distance is zero if the vectors are identical,\\\\ni.e.a=b. The more aandbdiffer, the greater is the distance dE(a, b)between them.\\\\ndE(a, b) =vuutNX\\\\ni=1(ai−bi)2 (3.5)\\\\n3.3.2 Cosine Similarity\\\\n(a) Similar vectors a, b.\\\\n (b) Dissimilar vectors a, b.\\\\nFigure 3.11: Cosine Similarity between two vectors considers the angle between them.\\\\nThe similarity between two texts is measured by the cosine of the angle between their\\\\nrespective real-valued vectors. The cosine similarity is defined in Equation 3.6 [62]. For\\\\npositive vectors, for instance, produced by TF-IDF, it is a value between 0and1. If the\\\\nangle is close to zero degrees, the cosine similarity is close to 1and the vectors are similar.\\\\nIf the angle is close to 90degrees, the cosine similarity is close to 0and the vectors are\\\\ndissimilar. Both a similar and a dissimilar pair of vectors are depicted in Figure 3.11.\\\\ncosine (a, b) =a·b\\\\n∥a∥ × ∥ b∥=PN\\\\ni=1aibiqPN\\\\ni=1a2\\\\niqPN\\\\ni=1b2\\\\ni(3.6)\\\\nThe formula from Equation 3.6 assumes that the vectors, which span the VSM are or-\\\\nthogonal and thus, completely independent. However, in practical applications, the index\\\\nterms which span the VSM are often semantically dependent.\\', \\'3 Fundamentals 19\\\\n3.4 Topic analysis\\\\nSince more and more textual data emerges, methods to analyze and extract information\\\\nfrom texts become more important. One of these methods is topic analysis. A topic can\\\\nbe defined as a cluster of words that occur frequently or are semantically similar to each\\\\nother. A document can be represented by one or more topics [3].\\\\n3.4.1 Topic to Vector\\\\nThe approach Topic to Vector (Top2Vec) addresses several problems of state-of-the-art\\\\ntopic analysis approaches, such as Latent Dirichlet Allocation (LDA) [4]. Top2Vec does\\\\nnot require the user to specify the number of topics k, i.e. it does not discretize the topic\\\\nspaceinto ktopics, anditdoesnotrequirestopwordremovalorlemmatization. Itconsiders\\\\nthe semantic meaning of words. Top2Vec only associates one topic with a document.\\\\n(a) Skip-gram architecture cf. [52].\\\\n (b) CBOW architecture cf. [52].\\\\nFigure 3.12: Both learning architectures of Top2Vec. w(t−2), w(t−1), w(t+ 1), w(t+ 2)\\\\nare the context words of the centre word w(t)of topic z(t).\\\\nTop2Vec is based on Word2Vec and Doc2Vec. The documents are embedded using the\\\\nDoc2Vec model PV-DBOW. The two learning architectures CBOW and Skip-gram from\\\\nWord2Vec are adapted to train the model as depicted in Figure 3.12 [52]. The Skip-Gram\\\\nlearning task is to predict the context a word came from [4, 52]. Top2Vec embeds words,\\\\ndocuments and topics in the same feature space. The similarity between embeddings can\\\\nbe measured using the cosine similarity function [52].\\\\nAngelov regards each point in the VSM as a topic, described by its nearest words. The\\\\nauthor states that topics are continuous and can be described by different sets of words [4].\\\\nHence, topic analysis can be defined as the task of finding sets of informative words that\\\\ndescribe the topic of a document. Documents in dense areas of the topic space are con-\\\\nsidered to be about the same topic. The density-based clustering algorithm Hierarchical\\\\nDBSCAN (HDBSCAN) is used to find these dense areas. Since HDBSCAN has difficul-\\\\nties finding dense clusters in high-dimensional data, the dimensionality reduction method\\\\nUniform Manifold Approximation and Projection (UMAP) is applied [4]. The steps of the\\\\ntopic analysis procedure Top2Vec are depicted in Figure 3.13.\\', \\'3 Fundamentals 20\\\\nFigure 3.13: Procedure of topic analysis using Top2Vec.\\\\nA topic vector is denoted as the centroid or average of the document vectors that belong\\\\nto a certain topic. The number of topics is derived from the number of dense areas. It\\\\nis possible to merge topics to hierarchically reduce the number of topics to any number\\\\nsmaller than the number of topics initially found.\\\\n3.4.2 Word clouds\\\\nA word cloud is a technique to visualize the most predominant words in a text [36]. The\\\\nsize of a word correlates to its frequency or importance in the text. However, a word does\\\\nnot have to be meaningful to appear large. A word cloud does not provide information\\\\nabout the meaning or context of words and thus, one has to be careful when interpreting\\\\nthe results.\\\\n3.5 Compression of data\\\\nAccording to Radu et al., a decomposition of data preserves the inner structure in inherent\\\\nclusters. When data analysis techniques are applied to reasonably low-dimensional data,\\\\nthe results usually improve. Moreover, compressed data is less memory-consuming and\\\\noften less difficult to interpret by humans since there are more methods to visualize low-\\\\ndimensional data. In the following, two approaches to reduce the dimensionality of data\\\\nare presented.\\\\n3.5.1 Autoencoder\\\\nThe idea of this approach is to find a meaningful low-dimensional version of the input.\\\\nThe high-dimensional data is encoded into a low-dimensional representation using the\\\\nencoder of an undercomplete Autoencoder (AE) [46]. Hence, the output of the latent\\\\nspace corresponds to the input’s embedding. The low-dimensional representation can be\\\\ndecoded into an approximation of the high-dimensional original using the decoder of the\\\\nAE.\\', \\'3 Fundamentals 21\\\\nFigure 3.14: Structure of an AE cf. [46]. The six-dimensional input is encoded into a three-\\\\ndimensional representation. This encoding is decoded into a six-dimensional\\\\napproximation of the original input.\\\\nAn undercomplete AE is a feed-forward NN, which consists of an encoder and a decoder.\\\\nNNs are discussed in Subsection 3.2.1. It learns efficient (non-correlated) encodings of the\\\\ninput data [46]. It is undercomplete because the dimensionality of the hidden layer, or\\\\nso-called hidden space, is lower than the dimensionality of the input layer [31]. The input\\\\nand output layers have the same dimensionality.\\\\nThe network employs backpropagation to update the parameters of the network during\\\\ntraining. The AE’s goal is to approximate the identity function fθ(X) =X(trivial solution\\\\neliminated) for input Xand function parameters to be learned θ[31].\\\\n3.5.2 Eigenfaces\\\\nAccording to Turk and Pentland, the idea of Eigenfaces is inspired by information theory.\\\\nOpposed to former approaches in the domain of face recognition which relied on the clas-\\\\nsification of images based on a set of predefined facial features, such as distance between\\\\neyes, Eigenfaces does not use predefined features [67]. The goal of this approach is to rep-\\\\nresent images using a smaller set of image features, i.e. compression to a lower-dimensional\\\\nfeature space, such that it is possible to distinguish between the images [67, 71]. These\\\\nfeatures do not necessarily correspond to human facial features [67]. Similar pictures, i.e.\\\\nof the same person, should lie on a manifold in the lower-dimensional feature space [65].\\\\nThe decomposition of input images not only reduces the complexity but also facilitates\\\\nmodeling probability density of a face image [65].\\\\nThe greyscale input images are two-dimensional arrays of numbers: x={xi|i∈S},S\\\\nbeing a square lattice [76, 67]. The images are reshaped to an one-dimensional array\\\\nx= [x1, x2, ..., x n]T∈Rn, where n=∥S∥andRnis the n-dimensional euclidean space\\\\n[76]. Some authors remove the background to omit values outside the face area [67].\\', \\'3 Fundamentals 22\\\\nTurk and Pentland stress that the data should be normalized, i.e. centered, as computed\\\\nin Equation 3.7. Φkis the difference of the k-th training image and the average image\\\\ncalculated using Equation 3.8, Nbeing the number of training images.\\\\nΦk=xk−ψ (3.7)\\\\nψ=1\\\\nNNX\\\\nk=1xk (3.8)\\\\nx=nX\\\\ni=1ˆxiei (3.9)\\\\nThe next step is to find an alternative lower-dimensional representation of the images,\\\\nwhich preserves most of the information of the original image. In mathematical terms, this\\\\ndecomposition can be expressed using the formula in Equation 3.9, ebeing an orthogonal\\\\nbasis [76]. If all basis vectors are used, the original image can be reconstructed using a\\\\nlinear combination of the basis vectors [67, 16]. The number of basis vectors is limited by\\\\nthe minimum of the training set size N[67] and the number of pixels n[16]. In order to\\\\ncompress the input from a nto am-dimensional space, given m≪n, only the first mbasis\\\\nvectors are used. The parameter mand the basis eis chosen such that ˆxiis small for i≥m\\\\n[76]. The compressed version of the image is denoted x≃ˆx= [ˆx1,ˆx2, ...,ˆxm]T. In other\\\\nwords: The compressed image is a vector of the first mweights of the linear combination\\\\nof weight and basis vectors used to transform the compressed image back to the original\\\\nspace. The weights denote the position of the projection of the face images in the feature\\\\nspace or so-called face space spanned by the first mbasis vectors [67].\\\\nIn the context of Eigenfaces, one basis used for decomposition is the Karhonen-Loéve (KL)\\\\nbasis, i.e. PCA [76, 67]. According to Zhang et al., the KL representation is optimal in\\\\nthe sense that it minimizes the Root Mean Square Error (RMSE) between the original\\\\nimage and the compressed image calculated using m < northogonal vectors. The KL\\\\nbasis consists of the eigenvectors of covariance matrix C=E\\\\x02\\\\nxxT\\\\x03\\\\nof the input images x\\\\n[76]. Since these eigenvectors can have facial features, they are called Eigenfaces . There\\\\nare two approaches in the literature to determine the number of Eigenfaces mused to\\\\ncompress the input images:\\\\n(a) The cumulative explained variance of the first i≤neigenvectors (sorted by eigenval-\\\\nuesλi) is calculated [76, 16, 64]. The eigenvalues λican be interpreted as the amount\\\\nof variance explained by the corresponding eigenvector ei, which is equivalent to in-\\\\nformation or entropy. The user can choose how much variance, i.e. information,\\\\nshould be preserved, by choosing msuch that the explained variance is greater than\\\\na chosen threshold. Sudiana et al. use a threshold of 90%. A plot displaying the\\\\ncumulative explained variance and a threshold of 90% is shown in Figure 3.15 (a).\\', \\'3 Fundamentals 23\\\\n(b) The number of Eigenfaces mis chosen using the reconstruction error-complexity\\\\ntrade-off. The reconstruction error, i.e. the RMSE of the original image xand the\\\\ninverse transformed image x′, is calculated in Equation 3.10 for different values of\\\\nm. The “elbow” point marks the point where the reconstruction error decreases only\\\\nslightly for increasing mand thus, is an indicator for the optimal m. A visualization\\\\nof this approach is shown in Figure 3.15 (b).\\\\n0 200 400 600\\\\nNumber of components0.20.40.60.81.0Explained varianceCumulative explained variance on a 700/300 values train/test set\\\\ncumulative explained variance\\\\n90% explained variance\\\\n441 components\\\\n(a) The cumulative explained variance of the\\\\nfirsti≤neigenvectors (sorted by\\\\neigenvalues λi).\\\\n0 100 200 300\\\\nNumber of components3.753.803.853.903.954.004.054.10Weighted RSME1e6\\\\nReconstruction error on a 350/150 values train/test set\\\\nRSME / avg # non\\\\nwhite pixel per image(b) The reconstruction error RMSE calculated\\\\nfor different values of m. The\\\\nreconstruction error increases less rapidly\\\\nafter 10 to 20 components.\\\\nFigure 3.15: Two approaches to determine the number of Eigenfaces mused to compress\\\\nthe input images.\\\\nRSME =sPN\\\\ni=1(xi−x′\\\\ni)2\\\\nN(3.10)\\\\nC≃1\\\\nNNX\\\\nk=1xkxT\\\\nk=1\\\\nNXXT(3.11)\\\\nei=1√λiXvi (3.12)\\\\nIn order to reduce calculation complexity, Cis approximated. Zhang et al. propose the\\\\napproximation displayed in Equation 3.11, with X= [x1,x2, ...,xN],xi∈Rn[76].\\\\nFinding the eigenvectors of XXTis still computationally expensive, since XXTis an\\\\nbynmatrix. According to Zhang et al., the eigenvectors of XXTcan be calculated by\\\\nusing the eigenvectors of XTX. The eigenvectors ei∈RnofXXTcan be derived from\\\\nthe eigenvectors vi∈RNofXTXusing Equation 3.12 as discussed in more detail in\\\\n[76]. According to Anowar et al., the problem is reduced to a NbyNmatrix, which\\\\nis computationally less expensive to solve assuming N≪n. The eigenvectors can be\\\\ncalculated using Singular Value Decomposition (SVD) [76]. SVD is a method, which\\\\ndecomposes a matrix into the so-called left singular vector, the diagonal matrix and the\\\\nright singular vector [6].\\', \\'3 Fundamentals 24\\\\nIn the literature, face images are classified by comparing their position in the face space\\\\nwith those of already known faces [67]. According to [67], this approach performs well\\\\non datasets with little variation in pose, lighting and facial expression. However, Zhang\\\\net al. state, that the performance deteriorates if the variations increase since the changes\\\\nintroduce a bias and thus, the distance function used to make classifications is no longer a\\\\nreliable measure.\\\\n3.6 Clustering\\\\nClustering is used in a variety of domains to group data into meaningful subclasses [54,\\\\n17, 35]. According to Patwary et al. and Radu et al., common domains include anomaly\\\\ndetection, noise filtering, document clustering and image segmentation. The objective is to\\\\nfind clusters, which have a low inter-class similarity and a high intra-class similarity [54].\\\\nThe similarity is measured by a distance function, which is dependent on the data type.\\\\nCommon distance functions are the Euclidean distance, the Manhattan distance and the\\\\nMinkowski distance [35].\\\\nThere are multiple clustering techniques, which can be divided into four categories [2]:\\\\n•Hierarchical clustering : Algorithms, that create spherical or convex-shaped clus-\\\\nters, possibly naturally occurring. A terminal condition has to be defined beforehand.\\\\nExamples include CLINK, SLINK [17] and Ordering Points To Identify Clustering\\\\nStructure (OPTICS) [54].\\\\n•Partitional based clustering : Algorithms, that partition the data into kclusters,\\\\nkis given apriori. Clusters are shaped in a spherical manner, are similar in size and\\\\nnot necessarily naturally occurring. KMeans is a popular example of a partitional-\\\\nbased clustering algorithm.\\\\n•Density based clustering : Density is defined as the number of objects within a\\\\ncertain distance of each other [35]. The resulting clusters can be of arbitrary shape\\\\nand size. The algorithm usually chooses the optimal number of clusters given the\\\\ninput data. However, some algorithms are sensitive to input parameters, such as\\\\nradius, minimum number of points and threshold. Popular examples are Density-\\\\nBased Spatial Clustering of Applications with Noise (DBSCAN) and OPTICS.\\\\n•Grid based clustering : Similar to density-based clustering, but according to\\\\nAgrawal et al. better than density-based clustering. Examples include flexible grid-\\\\nbased clustering [17].\\\\nMultiple approaches listed below use the term ε-neighbourhood , which is defined as the set\\\\nof all objects within a certain distance εof a given object [54]. In other words: Nε(x) =\\\\n{y∈X|dist(x, y)≤ε, y̸=x},εbeing the so-called generating distance.\\', \\'3 Fundamentals 25\\\\n3.6.1 DBSCAN\\\\nThe clusters identified by DBSCAN have a high density and are separated by low-density\\\\nregions [35]. In order to create clusters of minimum size and density, DBSCAN distin-\\\\nguishes between three types of objects [35]:\\\\n•Core objects : An object xwith at least minPts ∈Nobjects in its ε-neighbourhood\\\\nNε(x), i.e.|Nε(x)| ≥minPts is true [54].\\\\n•Border objects : An object with less than minPts objects in its ε-neighbourhood,\\\\nwhich is in the ε-neighbourhood of a core object.\\\\n•Noise objects : An object, which is neither a core object nor a border object.\\\\nKanagala and Krishnaiah define y∈Xasdirectly density reachable from x∈X, ifyis in\\\\ntheε-neighbourhood of core object x[35]. Moreover, a point y∈Xisdensity reachable\\\\nfrom x∈X, if there is a chain of objects x1, ..., x nwith x1=xandxn=y, which are\\\\ndirectly density reachable from each other as displayed in Figure 3.16 [35].\\\\nFigure 3.16: Density reachability cf. [5]. The object y∈Xis density reachable from\\\\nx∈X, since it exists a chain of directly density reachable objects between x\\\\nandy.\\\\nThe objects x∈Xandy∈Xare said to be density connected , if there is an object o,\\\\nfrom which both xandyare density reachable [35]. Density connectivity is visualized in\\\\nFigure 3.17.\\\\nFigure 3.17: Density connectivity cf. [5]. The objects xandyare density connected since\\\\nthere is an object o, from which both xandyare density reachable.\\\\nThe DBSCAN algorithm starts by labeling all objects as core, border or noise points.\\\\nThen, it eliminates noise points and links all core points, which are within each other’s\\\\nneighbourhood [35]. Groups of connected core points form a cluster. In the end, every bor-\\\\nder point is assigned to a cluster. The non-core point cluster assigning is non-deterministic\\\\n[54]. This algorithm creates clusters as a maximal set of density-connected points [35].\\', \\'3 Fundamentals 26\\\\nAccording to Kanagala and Krishnaiah, DBSCAN can identify outliers or noise. How-\\\\never, the algorithm is sensitive to the input parameters minPts andεand has difficulties\\\\ndistinguishing closely located clusters [35]. Moreover, if one wants to obtain hierarchical\\\\nclustering, one has to run the algorithm multiple times with different ε, which is expen-\\\\nsive in terms of memory usage [54]. According to Radu et al., DBSCAN is affected by\\\\nthe curse of dimensionality. Since DBSCAN relies on nearest neighbour queries and these\\\\nbecome less meaningful in high dimensions, i.e. distances become difficult to interpret, the\\\\nquality and accuracy of the results decline with increasing dimensionality [56]. Radu et al.\\\\nfound that their DBSCAN model assigns most objects noise when the dimensionality is\\\\nsufficiently large.\\\\n3.6.2 OPTICS\\\\nOPTICS does not return an explicit clustering, but rather a density-based clustering struc-\\\\nture of the data, which is equivalent to repetitive clustering for a broad range of parameters\\\\n[5]. Ankerst et al. claim that real-world datasets cannot be described by a single global\\\\ndensity, since they often consist of different local densities, as displayed in Figure 3.18.\\\\nFigure 3.18: Clusters with different densities cf. [5]. Since C1andC2have different densi-\\\\nties than AandB, a clustering algorithm using one global density parameter\\\\nwould detect the clusters A,BandC, rather than A,B,C1andC2.\\\\nOpposed to DBSCAN, OPTICS is able to detect clusters of varying densities [17]. OPTICS\\\\nproduces an order of the elements according to the distance to the already added elements\\\\n[17, 54]: The first element added to the order list is arbitrary. The order list is iteratively\\\\nexpanded by adding the element of the ε-neighbourhood to the order list, which has the\\\\nsmallest distance to any of the elements already in the order list. Hence, clusters with\\\\nhigher density, i.e. lower ε, are added first (prioritized) [35, 5]. When there are no more\\\\nelements in the ε-neighbourhood to add, the process is repeated for the other clusters. The\\\\nnon-core point cluster assigning is non-deterministic [54].\\\\nRD(y) =(\\\\nNULL if | Nε(x)|< minPts\\\\nmax(core_dist(x), dist (x, y))otherwise(3.13)\\', \\'3 Fundamentals 27\\\\nOPTICS saves the reachability distance RD(y), as calculated in Equation 3.13 from [54],\\\\nwith core distance core_distbeing the minimal distance εminsuch that |Nεmin(x)| ≥\\\\nminPts (i.e. the distance to the minPtsthpoint in Nε) or NULL else, of each element y\\\\nto its predecessor xin the order list and thus, a representation of the density necessary\\\\nto keep two consecutive objects xandyin the same cluster [54]. If ε < RD (y), then yis\\\\nnot density reachable from any of its predecessors and thus, one can determine whether\\\\ntwo points are in the same cluster using the information saved by OPTICS [54, 5]. If\\\\nthe core distance of an element is not NULL, i.e. it is a core object, and it is not density\\\\nreachable from its predecessors, it is the start of a new cluster [5]. Otherwise, the element\\\\nis a noise point. According to Patwary et al., the algorithm builds a spanning tree, which\\\\nenables obtaining the clusters for a given εby returning the connected components of the\\\\nspanning tree after omitting all edges with ε < RD (y)[54]. The relationship between ε,\\\\ncluster density and nested density-based clusters is displayed in Figure 3.19.\\\\nFigure 3.19: The relationship between ε, cluster density and nested density-based clusters\\\\ncf. [5]. For a constant minPts, clusters with higher density such as C1,C2\\\\nandC3, i.e. a low ε2value, are completely contained in lower density clusters\\\\nsuch as Cgiven ε1> ε 2. This idea forms the basis of OPTICS of expanding\\\\nclusters iteratively and thus, enables the detection of clusters for a broad range\\\\nof neighbourhood radii 0≤εi≤ε.\\\\nThis procedure enables the extraction of clusters for arbitrary 0≤εi≤ε[35, 5]. According\\\\nto Patwary et al.’s work, even though the clustering algorithm is expensive, the extraction\\\\nonly needs linear time. Ankerst et al. claim that the algorithm yields good results if\\\\nthe input parameters minPts andεare “large enough” and thus, the algorithm is rather\\\\ninsensitive to the input parameters.\\\\nThe smaller εis chosen, the more objects will be identified as noise and thus, the algorithm\\\\nwill not identify clusters with low density, since some objects only become core objects for a\\\\nlarger ε[5]. According to Ankerst et al., the optimal value for εcreates one cluster for most\\\\nof the objects with respect to a constant minPts, since information about all density-based\\\\nclusters for εi< εis preserved. Ankerst et al. present a heuristic for choosing εbased on\\\\nthe expected k-nearest neighbour distance [5].\\\\nHigh values for minPts smoothen the reachability curve, even though the overall shape\\\\nstays roughly the same [5]. According to Ankerst et al., the optimal value for minPts is\\\\nbetween 10 and 20.\\', \\'3 Fundamentals 28\\\\n3.7 Software frameworks\\\\nThe embeddings obtained by the methods described in Section 3.2 are stored in a Elas-\\\\nticsearch database. It is described in Subsection 3.7.1. The different methods explored in\\\\nthis work ought to be presented in a web application. This application should be used to\\\\ncompare the methods and to visualize the results. Subsection 3.7.2 and Subsection 3.7.3\\\\nrespectively describe Flask and Angular, which are the frameworks and components used\\\\nto implement the web interface.\\\\n3.7.1 Elasticsearch database\\\\nElasticsearch is a widely used non-relational database, which was designed to store and\\\\nperform full-text search on a large corpus of unstructured data [70]. This open-source\\\\ndistributed document-driven database system is built in Java and is based on the Apache\\\\nLucene (Java) library for high-speed full-text search [70, 74]. According to Zamfir et al.,\\\\nElasticsearch provides Wikipedia’s full-text search and suggestions as well as Github’s\\\\ncode search and Stack Overflow’s geolocation queries and related questions. It enables\\\\nnear real-time search by short refreshing periods which make performed operations on the\\\\ndata quickly available for search.\\\\nElasticsearch is a document store, which stores schemaless key-value pairs called documents\\\\n[22]. The documents are stored in logical units, so-called indices. As stated by Zamfir\\\\net al. and Voit et al., the indices are structured similarly to Apache Lucene’s inverted\\\\nindex format. An index can be spread into multiple nodes. A node is a single running\\\\ninstance of Elasticsearch [74]. An index is divided into one or more shards, which can be\\\\nstored on different servers and enable parallelization. Replicas are copies of shards, which\\\\ncreate redundancy and thus, ensure availability.\\\\nThe documents are saved in a JavaScript Object Notation (JSON) format [70]. A docu-\\\\nment’s fields and field types are defined by the user when initializing the database index.\\\\nBy default, every field of a document is indexed and searchable [74].\\\\nBy specifying the unique _idof a document and the database index, it is possible to\\\\nretrieve a specific document from the database using a GETendpoint of the HTTP API. The\\\\nparameters _source_excludes or_source_includes can be used to define the structure\\\\nof the response [19].\\\\nThe keyword used when performing a full-text search is match. To query for a specific\\\\nvalue, one has to specify the field of interest and the query value.\\\\nElasticsearch preprocesses the query value before starting the search [19]. The default\\\\npreprocessing steps of the so-called default analyzer include tokenization and lowercasing.\\\\nOmitting stop words is disabled by default, but custom stop words can be provided by the\\', \\'3 Fundamentals 29\\\\nuser or the English stop word list can be used. It is possible to create custom tokenizers,\\\\nwhich split the query value into tokens of a certain maximum length.\\\\nAnother useful feature of Elasticsearch is the multi-term synonym expansion where the user\\\\nquery is expanded to include synonyms of the query terms [19]. The maximum number of\\\\nexpansion terms is set to 50 by default but can be configured by the user. By default, the\\\\nmulti-term synonym expansion option is enabled.\\\\nElasticsearch also provides the option to perform fuzzy matching instead of exact search.\\\\nBy enabling the fuzzy matching option, a Elasticsearch query consisting of, for instance,\\\\nBahama returns documents that contain the word Bahamas . By default, this option is not\\\\nenabled but can be enabled and configured individually by the user [19].\\\\nAnother search option of Elasticsearch is the k-Nearest Neighbour (kNN) search on real-\\\\nvalued vectors. The return value of a kNN search is the knearest neighbours to the query\\\\nvector in terms of a certain distance function [42]. In order to perform kNN search on a field\\\\nit has to be of type dense_vector , indexed and a similarity measure has to be defined\\\\nwhen initializing the database [19]. The query value must have the same dimension as the\\\\nvectors stored in the database. A kNN search either returns the exact brute-force nearest\\\\nneighbours or an approximation of the nearest neighbours calculated by the Hierarchical\\\\nNavigable Small World (HNSW) algorithm [42, 19]. HNSW is a graph-based algorithm\\\\n[42].\\\\nBesides Elasticsearch, the elastic stack offers other tools, for instance, Kibana, which pro-\\\\nvides a user interface to manage different models. After saving a model in Kibana, it is\\\\npossible to create a text embedding ingest pipeline, which embeds new documents or rein-\\\\ndexes existing documents [20]. Elasticsearch’s kNN implementation not only allows literal\\\\nmatching on search terms but also semantic search incorporating Kibana’s text embedding\\\\ningest pipeline on search terms [19].\\\\n3.7.2 Flask\\\\nFlask is open source and written in Python by Armin Ronancher in 2004 [7, 50]. According\\\\nto Copperwaite and Leifer and Mufid et al., Flask is one of the most popular Python\\\\nweb frameworks. It provides powerful libraries for core functionality such as routing,\\\\ntemplating, and HTTP request parsing [15]. It can be extended with additional plugins\\\\nwithout affecting the internal structure of the existing system [7].\\\\nFlask uses the Jinja Template Engine for template files including HTML pages, whereas\\\\nstatic files such as Cascading Style Sheet (CSS) files are handled using the Werkzeug WSGI\\\\ntoolkit [7]. According to Aslam et al., Jinja is modeled after the Django template system.\\\\nWerkzeug implements, for instance, requests and response objects [50].\\\\nAll requests received from clients are passed to an instance of the Flask application [25].\\\\nHence, the first step is to create an instance of the Flask class as shown in Listing 3.1.\\', \"3 Fundamentals 30\\\\n1app = Flask(__name__)\\\\nListing 3.1: Initialization of a Flask application instance.\\\\nClients send requests to the web server, which passes them to the Flask application in-\\\\nstance. The queries are then routed to the corresponding functions. Routing is the process\\\\nof mapping Uniform Resource Locator (URL) paths to functions [25]. To define a route,\\\\ntheroutedecorator is used as displayed in Listing 3.2.\\\\n1@api.route( \\'/documents/<id> \\', endpoint= \\'document \\')\\\\n2class Document(Resource):\\\\n3 def get(self, id):\\\\n4 client = Elasticsearch(CLIENT_ADDR)\\\\n5 return query_database.get_doc_meta_data(client, doc_id=id)\\\\nListing 3.2: Exemplary definition of a function to display routing with Flask. The route\\\\ndecorator is used to define the URL path.\\\\nURLs can contain dynamic components, which are enclosed in <>angle brackets. The\\\\nvalues of these components are passed to the function as arguments [25]. By default,\\\\ndynamic components are of type string. However, other types including intand float\\\\nare supported.\\\\nAn endpoint is a class with certain methods, which can be accessed using HTTP requests.\\\\nEvery endpoint can have multiple decorators, including GET,POST,PUTand DELETE[22].\\\\nThe GETmethod is used to retrieve data from the server, whereas the other methods are\\\\nused to either insert, update or delete data.\\\\n3.7.3 Angular\\\\nAngular is a framework for building web applications. It uses Node.js and TypeScript.\\\\nUsually, the source code is structured into different modules, including components and\\\\nservices. Components are used to define the appearance of the application, while services\\\\ncontain the logic of the application and communicate with the backend.\\\\nAngular applications are created using the ng new <name> command line interface [61].\\\\nThis command creates a skeleton, which can be customized to meet the needs of the\\\\napplication.\", \\'4 Own approach 31\\\\n4 Own approach\\\\nIn this thesis, a tool is developed that offers text queries, detailed document inspection and\\\\nqueries for semantically or visually similar documents to the user. This chapter describes\\\\nhow the theoretical basics from Chapter 3 interplay and how they are used to construct\\\\nthis tool. Section 4.1 outlines the steps carried out before the application is operative,\\\\nSection 4.2 covers the resulting application and Section 4.3 discusses the dilemma faced\\\\nwhen balancing memory usage and query time. Specific parameter choices are explained\\\\nin Chapter 5.\\\\n4.1 Offline Processing\\\\nThe tasks carried out before the application is operative are outlined in this section. They\\\\nare considered to be offline preprocessing steps. A process works in an offline fashion if the\\\\nprocess requires access to the whole data at once [29]. This section outlines implementation\\\\ndetails of the way the data is derived, the database storing the data and the baseline topic\\\\nanalysis approach compared to this work’s application.\\\\n4.1.1 Database\\\\nFirst, the content of the Elasticsearch database is described, then, the initialization, inser-\\\\ntion and updating process of filling the database are explained and finally, the process of\\\\nquerying is outlined.\\\\nContent of the database\\\\nIn this work, the database is filled once with data from the Bahamas leak. The data is a\\\\nlarge unstructured corpus of PDF files. Since leak data does not change over time it is not\\\\nnecessary to update the database. After the initialization of the database, it is used for\\\\nqueries. Therefore, the workflow of processing the text corpus is carried out completely\\\\noffline and in advance.\\\\nThe index Bahamas stores different embeddings of the information derived from the text\\\\nlayer and metadata of the documents. As depicted in Figure 4.1, not only textual informa-\\\\ntion is stored in the database, but also information about the appearance of the first page\\\\nof the PDF. The structure of the index is presented in Table 4.1. The visual information\\\\nis stored in the fields pca_image ,pca_optics_cluster andargmax_pca_cluster .\\', \\'4 Own approach 32\\\\nTable 4.1: Fields of the Elasticsearch database index Bahamas .\\\\nField name Field description\\\\n_id Unique identifier of document i. The identifier is gen-\\\\nerated by the sha256 hash algorithm from hashlib using\\\\nthe PDF file as input.\\\\ndoc2vec 55 dimensional Doc2Vec embedding of i.\\\\nsim_docs_tfidf TF-IDFembeddingenhancedbyanall-zeroflagof i. The\\\\nall-zero flag is one if the TF-IDF embedding consists of\\\\nonly zeros, zero else. If the embedding’s dimensionality\\\\nis greater than 2048, the encoder of a trained AE is used\\\\nto compress the embedding.\\\\ngoogle_univ_sent_encoding 512 dimensional USE embedding of i.\\\\nhuggingface_sent_transformer 384 dimensional SBERT embedding of i.\\\\ninferSent_AE InferSent embedding of i. Since the pretrained InferSent\\\\nmodel embedding’s dimension is 4096, the encoder of a\\\\ntrained AE has to reduce the dimension to 2048.\\\\npca_image 13-dimensional PCA version of first page image of i.\\\\npca_optics_cluster Cluster of iidentified by OPTICS on PCA version of\\\\nimage.\\\\nargmax_pca_cluster Number of maximum PCA component as cluster of i.\\\\ntext Text of i.\\\\npath Path to i.\\\\nFigure 4.1: PDFs to Database. First, the data is preprocessed: The first page of a PDF\\\\nfile is converted to an image and the complete text is extracted. The images\\\\nare stored in the database as well as the text and different embeddings of the\\\\ntext. Some values, such as the image or the InferSent embedding, have to be\\\\ncompressed to become a vector of at most 2048 dimensions.\\', \"4 Own approach 33\\\\nInitialization, insertion and updating\\\\nTo facilitate working with and running the code, the initialization of the database is split\\\\ninto multiple steps. As depicted in Figure 4.2, first the database is initialized by defining\\\\nthe index name and the mappings, i.e. the field names, types and sizes. This step is carried\\\\nout using the method create.\\\\nFigure 4.2: Procedure of initialization and filling of the database.\\\\nAfterwards, the documents are created using the method create. The initial creation of\\\\na document only defines the fields id,textandpath.\\\\nThe embeddings are added to the documents in a third step. To increase the efficiency\\\\nof this step, data parallelism, i.e. parallelizing the execution of a method across multiple\\\\ninput values, is applied. In this work, a set of paths to documents is split among multiple\\\\nprocesses. First, the absolute paths of all documents are saved in a list. This list is\\\\npartitioned into num_cpus many lists sub_lists of similar size. Each process works on\\\\na sublist. The Poolobject from the multiprocessing module is used for data parallelism.\\\\nThe steps carried out are displayed in Listing 4.1. The embeddings are subsequentially\\\\ninserted into the database for each sublist.\\\\n1with Pool(processes=num_cpus) as pool:\\\\n2 for model_name in model_names:\\\\n3 proc_wrap = wrapper(model_name=model_name, baseDir=src_path)\\\\n4 pool.map(proc_wrap, sub_lists)\\\\nListing 4.1: Usageof Poolfor dataparallelism. The pathsto the documents are partitioned\\\\ninto sublists which are simultaneously inserted into the database. Since the\\\\nPoolobject does not work with a lambdafunction, a class wrapper is created\\\\nwhich provides the same functionality.\\\\nThedocumentembeddingsareaddedtothedatabaseusingthemethod updateasdisplayed\\\\nin Listing 4.2.\\\\n1client.update(index= \\'bahamas \\', id=id, body={ \\'doc\\':\\\\n2 {MODELS2EMB[model_name]: embedding}})\\\\nListing 4.2: Update of a database entry to insert a specific embedding.\\\\nQueries\\\\nThe default analyzer is used for the full-text search since for instance configuring a maxi-\\\\nmum token length did not seem necessary or likely to improve the results.\", \"4 Own approach 34\\\\n1results = elastic_search_client.search(\\\\n2 index= \\'bahamas \\',\\\\n3 size=count,\\\\n4 from_=(page*count),\\\\n5 query= { \\'match \\': {\\\\n6 \\'text \\': {\\'query \\':text,\\\\n7 \\'fuzziness \\':\\'AUTO \\',}\\\\n8 },\\\\n9 }, source_includes=SRC_INCLUDES)\\\\nListing 4.3: Exemplary query to an Elasticsearch database index. The parameters size\\\\nand from_define the number of results to return and the start index of the\\\\nresults. To enable fuzzy search a value for fuzziness has to be set.\\\\nMoreover, the fuzzy matching option is set to AUTO, which means in terms of keyword or\\\\ntext fields that the allowed Levenshtein Edit Distance, i.e. number of characters changed\\\\nto create an exact match between two terms, to be considered a match, is correlated to\\\\nthe length of the term [19]. By default, terms of length up to two characters must match\\\\nexactly, terms of length three to five characters must have an edit distance of one and terms\\\\noflengthsixormorecharactersmusthaveaneditdistanceoftwo[19]. Anexemplaryquery,\\\\nwhich uses fuzzy search is given in Listing 4.3.\\\\nAccording to Malkov and Yashunin, one of kNN search’s use cases is semantic document\\\\nretrieval, which makes it a good fit for this task. In this work, the approximate nearest\\\\nneighbours search HNSW is used since it is faster and the results are good enough for the\\\\npurpose of this work. The similarity measure used in this work is the cosine similarity. The\\\\nother similarity measures provided by Elasticsearch are the l2_norm or so-called Euclidian\\\\ndistance and the dot_product which is the non-auto-normalized version of the cosine\\\\noption. Since cosine is not defined on vectors with zero magnitude, embeddings that can\\\\nreturn all zero vector representations, such as TF-IDF, are enhanced with an all-zero flag\\\\nbefore inserting them into the database.\\\\nIn this work, the only tool from the elastic stack used is Elasticsearch. Without Kibana,\\\\nthe used models are saved on disk as Pickle (PKL) files. Consequently, instead of using\\\\nthe kNN query structure for semantic search on embeddings provided by Elasticsearch and\\\\nKibana, the normal kNN search on a field that contains an embedding is used.\\\\n4.1.2 Eigendocs\\\\nIn this work, the Eigenfaces approach from Subsection 3.5.2 is used to compress the images\\\\nof the first page of the documents. The idea is that documents not only hold textual\\\\ninformation but also visual information, such as layout, company logo or signature. By\\\\nmapping those images on a subspace, they ought to be grouped by visual similarity. The\", \\'4 Own approach 35\\\\nFigure 4.3: From PDFs to Eigendocs. Firstly, the first page of a document is converted\\\\nto an image. Then, the image is preprocessed: It is placed on a white canvas,\\\\nto ensure all images have the same dimensions. Moreover, it is converted to\\\\ngreyscale and normalized to values between zero and one. Afterwards, the two-\\\\ndimensional image is reshaped into a one-dimensional array. Lastly, the image\\\\nis compressed using Eigendocs.\\\\nprocedure of the Eigenfaces adaption Eigendocs is displayed in Figure 4.3. Different stages\\\\nof this approach are displayed in Figure 4.4.\\\\nFigure 4.4: 10 randomly selected documents from the test set. The number of images in\\\\nthe test set is 561, while the PCA model is fitted to 1680 training images.\\\\nThe original images are displayed in the first row. The second row shows the\\\\nreconstruction from their compressed version in the fourth row. The third row\\\\nshowsthereconstructionerror, i.e.thedifferencebetweenthereconstructedand\\\\nthe original image. The last row presents the greyscale values of the compressed\\\\n13-dimensional image as a line.\\\\nThe documents are first read from a directory. Subsequently, their first page is converted\\\\nto an image and saved. The maximum height and width among all images in a corpus\\\\nof 1000 randomly sampled images are calculated. The selection of 1000 images is used to\\\\nreduce the run time of the script. The maximum height and width are used to create a\\\\nwhite canvas for each image which forms the background. Every image is placed in the\\\\nupper left corner of the canvas. Hence, assuming the selection of documents used to fit\\\\nthe PCA model is representative, scaling is not necessary and thus, the portion of white\\\\npixels on the right and bottom side encodes the sizes of the original image. Therefore, the\\\\nrelative size of images in the corpus is incorporated in the resulting representation of the\\', \\'4 Own approach 36\\\\ninput images. However, some images of the test set are bigger than the maximum values\\\\nof the selected images and as a consequence are scaled.\\\\n1def rgb2gray(img):\\\\n2 return 0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]\\\\n3# more code\\\\n4C = np.ones((max_w,max_h))\\\\n5C[:doc.shape[0],:doc.shape[1]] = rgb2gray(doc)\\\\n6documents.append(C.ravel())\\\\nListing 4.4: Preprocessing of the input images from Dr. Christian Gruhl. Conversion of\\\\nRGB pixel values to greyscale according to [41]. The background is a white\\\\ncanvas. The images are converted to one-dimensional greyscale values.\\\\nAfterwards, the images are converted to greyscale using line 5 of Listing 4.4. Before\\\\nreturning the image, the two-dimensional image vectors are converted to one-dimensional\\\\nones as displayed in line 6 of Listing 4.4. The decomposition is transformed using PCA as\\\\ndisplayed in Listing 4.5. The implementation of PCA from sklearn intrinsically normalizes\\\\nthe data as described in Subsection 3.5.2.\\\\n1pca = decomposition.PCA(n_components=n_components, whiten=True,\\\\n2 svd_solver=\"randomized\")\\\\nListing 4.5: Initialization of the PCA instace used to compress the images. Since the\\\\nEigenfaces approach uses SVD, the adaption Eigendocs has to be implemented\\\\nlikewise applying a svd_solver .\\\\n4.1.3 Embeddings\\\\nFirstly, the implementation of the AE used to compress high-dimensional embeddings is\\\\npresented. Then, the models used to encode the textual data are outlined below with\\\\nregard to implementation details.\\\\nAutoencoder\\\\nIn this work, an AE is used to reduce the dimensionality of the InferSent and the TF-IDF\\\\nembeddings. Since the InferSent model is pretrained, it is not possible to change the\\\\ndimensionality of the embedding without a considerably big effort, i.e. retraining the model\\\\non a sufficiently large data corpus and reconfiguring the model’s parameters. Therefore,\\\\nit is not feasible to change the dimensionality of the InferSent embedding, but rather add\\\\na supplementary layer after the model to produce the final embedding. Similarly, the\\\\nTF-IDF embedding dimension correlates with the vocabulary size and thus, the size of the\\\\ndata corpus. Further reducing the vocabulary size would decrease the TF-IDF model’s\\', \"4 Own approach 37\\\\nquality. Hence, the idea is to use the encoder of an AE to reduce the dimensionality of the\\\\nInferSent and the TF-IDF embedding.\\\\nFigure 4.5: Architecture of the AE.\\\\nThe implementation was provided by the blog post from [33]. It uses the library keras1.\\\\nThe architecture is adapted to fulfil the needs of the specific context. It is presented in\\\\nFigure 4.5.\\\\nTF-IDF\\\\nThe TF-IDF model has to be initialized and trained on the data corpus to build a\\\\ndata-specific vocabulary. An exemplary implementation is given in Listing 4.6. The\\\\nTfidfVectorizer is provided by the scikit-learn package. The inputparameter de-\\\\nfines the input type, i.e. content means that the input is a list of strings or bytes, whereas\\\\nfileassumes the input has a readmethod and filename denotes a list of filenames as\\\\ninput [66]. An embedding is obtained using the command from Listing 4.7.\\\\n1tfidf_model = TfidfVectorizer(input= \\'content \\',\\\\n2 preprocessor=TfidfTextPreprocessor().transform, min_df=3,\\\\n3 max_df=int(len(docs)*0.07))\\\\n4tfidf_model.fit(documents)\\\\nListing 4.6: Initialization of the TF-IDF model. Firstly, an instance of the\\\\nTfidfVectorizer class is created. Secondly, the fitmethod is called to fit\\\\nthe model on the documents.\\\\n1tfidf_model.transform(text).todense()\\\\nListing 4.7: Encoding a text using the TF-IDF model.\\\\nThe preprocessor parameter defines the preprocessing, i.e. string transformation, stage.\\\\nIt is possible to override the default with a custom preprocessing function. The parameters\\\\n1https://keras.io/ (last accessed: 19/11/2023)\", \\'4 Own approach 38\\\\nmin_dfand max_dfdefine the minimum and maximum document frequency of a word in\\\\nthe corpus to be considered relevant. The default values are 1, i.e. a term has to appear\\\\nat least once, and 1.0, i.e. a term appears at most in all documents, respectively [66].\\\\nBy default, the scikit-learn implementation uses the norm=’l2’ configuration, i.e. the\\\\nEuclidean norm. The implementation of TF-IDF in scikit-learn is different from the\\\\noriginal TF-IDF definition. The difference is the calculation of the IDF part, which is given\\\\nin Equation 4.1 from [66]. The one is added to Mijdue to the parameter smooth_id=True\\\\nby default to prevent zero divisions and to avoid logarithmic divergences due to a zero\\\\nargument [55]. After calculating the TF-IDF values, they are normalized by the Euclidean\\\\nnorm given in Equation 4.2.\\\\nidf(wij) = log1 +M\\\\n1 +Mij+ 1 (4.1)\\\\nvnorm =v\\\\n∥v∥2=vq\\\\nv2\\\\n1+v2\\\\n2+...+v2\\\\nM(4.2)\\\\nFigure 4.6: TF-IDF pipeline. Firstly, the text extracted from the documents is prepro-\\\\ncessedusingacustompreprocessor. Then, theTF-IDFvaluesareobtainedfrom\\\\ntheTfidfVectorizer . Afterwards, the all-zero flag is added to the TF-IDF\\\\nweights. If the resulting dimensionality is bigger than 2048, the encoder of an\\\\nAE is used to reduce the dimensionality. The results are stored in the database.\\\\nThe pipeline in Figure 4.6 visualizes the steps carried out in this work to derive the\\\\nTF-IDF embedding of a text and store it in the database. The text of the PDFs is ex-\\\\ntracted and preprocessed using a custom preprocessor. Thereafter, it is embedded using the\\\\nTfidfVectorizer . The TF-IDF weights are the embedding. Before storing the TF-IDF\\\\nweights in the database, they are enhanced with an all-zero flag. The all-zero flag ensures\\\\nthat no all-zero vectors are stored in the database by extending those that have a zero\\\\nmagnitude with a “1” entry and “0” otherwise. All-zero TF-IDF weights indicate that a\\\\ndocument does not have any terms with the vocabulary in common. Since the vocabulary\\\\nis kept relatively small with respect to the number of different words in the data corpus\\\\nto reduce the dimensionality of the embeddings, it is not unlikely that a document does\\\\nnot contain any of the vocabulary terms. The all-zero flag is necessary because the cosine\\', \\'4 Own approach 39\\\\nsimilarity used to query for similar documents in the database cannot handle vectors of\\\\nzero magnitude. This alteration does not change the cosine similarity between non-zero\\\\nmagnitude vectors, since the additional zero adds no supplementary information to the\\\\ncalculation of the cosine similarity. The vectors whose all-zero flag is one have a cosine\\\\nsimilarity of one when being compared to each other.\\\\nFigure 4.7: Preprocessing visualized using an example text. The stop word removal im-\\\\nplicitly tokenizes the text.\\', \\'4 Own approach 40\\\\nThe preprocessing steps of the custom preprocessor are visualized in Figure 4.7. Firstly, the\\\\naccents are stripped from the text. Then, all new line symbols are replaced with a whites-\\\\npace. Afterwards, the text is converted to lowercase. Then the numbers are discretized,\\\\ni.e. all numbers between 0 and 99999 are replaced with the string SMALLNUMBER , numbers\\\\nbigger than 99999 are replaced with the string BIGNUMBER and floats are replaced with the\\\\nstring FLOAT. The next step is to remove all punctuation symbols. To ensure empty tokens\\\\ngenerated by prior preprocessing steps are omitted, all sequences of multiple subsequent\\\\nwhitespaces are discarded. After that, the symbols for numbers are enclosed with pointed\\\\nbrackets, e.g. <SMALLNUMBER> . Then, the text is tokenized, i.e. split at whitespaces, and\\\\nstop words are omitted. The stop word list is provided by the nltkpackage and consists\\\\nof common English stop words. Afterwards, the tokens are lemmatized. The lemmatizer\\\\nused is the WordNetLemmatizer from the nltkpackage. The WordNetLemmatizer uses the\\\\nEnglish lexical database WordNet to return valid stems [53]. In the end, the tokens are\\\\njoined to a string and returned.\\\\nSince the dimensionality of the TF-IDF embeddings is big for a large text corpus, an AE is\\\\nused to reduce the dimensionality of the embedding if its dimensionality exceeds 2048.\\\\nDoc2Vec\\\\nThe library gensimprovides the Doc2Vec model used in this work. The model is initialized\\\\nwith input data of type tagged documents , which are documents with (numerical) tags.\\\\nIn this work, the default parameters are used. The default algorithm is PVDM [24]. The\\\\nparameters vector_size andwindowdefine the dimensionality of the embeddings and the\\\\nsize of the window, i.e. the maximum distance between the current and the predicted word,\\\\nrespectively. The default value for vector_size is 100, whereas the default window size is 8\\\\n[24, 23]. The min_count parameter defines a threshold below which words will be ignored.\\\\nIts default value is 5. The workers parameter denotes the number of threads to be used\\\\nfor training. The default value is 1 [24]. The epochsparameter specifies the number of\\\\niterations over the corpus. The default value is 10. By default, the hierarchical softmax\\\\nalgorithm, i.e. hs=1, is used for training [78]. Many Doc2Vec default values are adopted\\\\nfrom Word2Vec since the gensimDoc2Vec implementation inherits from the Word2Vec\\\\nimplementation.\\\\nInferSent\\\\nThe InferSent model is implemented using PyTorch [58]. The parameters used to initial-\\\\nize the model are presented in Listing 4.8. The parameter version in line 6 indicates\\\\nwhether the model is trained with GloVe or fastText for the value 1 or 2 respectively.\\\\nSince the model is precomputed, it is not possible to change certain parameters, such\\\\nas the word embedding dimension word_emb_dim or the dimension of the output vectors\\\\nenc_lstm_dim .\\', \"4 Own approach 41\\\\n1\\'bsize \\': 64,\\\\n2\\'word_emb_dim \\': 300,\\\\n3\\'enc_lstm_dim \\': 2048,\\\\n4\\'pool_type \\':\\'max\\',\\\\n5\\'dpout_model \\': 0.0,\\\\n6\\'version \\': 1\\\\nListing 4.8: Parameters of the InferSent model.\\\\nThe steps necessary to create a working instance of the InferSent model are presented in\\\\nListing 4.9. After the InferSent model is initialized in line 1, the state_dict of the model\\\\nis loaded in line 2. This dictionary consists of learnable parameters, i.e. weights and bias,\\\\nof the model. The state_dict is obtained from the PKL file of InferSent as stated in [18].\\\\nThe path to the word embeddings is set in line 3. Finally, in line 4, the vocabulary of the\\\\nmodel is built. More precisely, only those embeddings needed are kept while the rest is\\\\ndiscarded.\\\\n1infersent = InferSent(params_model)\\\\n2infersent.load_state_dict(torch.load(model_path))\\\\n3infersent.set_w2v_path(w2v_path)\\\\n4infersent.build_vocab(docs, tokenize=True)\\\\nListing 4.9: Initializing the InferSent model.\\\\nIn this work, a custom set of vector representations of words is used. The custom word\\\\nembeddings are computed by a Word2Vec model trained on 2048 randomly selected doc-\\\\numents from the Bahamas dataset which reduces the run time of the script. The only\\\\nparameter which differs from the default settings of Word2Vec is the vector_size which\\\\nis set to 300. After the Word2Vec model is trained, the word embeddings are saved in a\\\\nfile whose file path is the value of w2v_path in line 3 of Listing 4.8.\\\\nIn this work, an AE is used to reduce the dimensionality of the InferSent embedding.\\\\nUSE\\\\nThe USE model implemented with TensorFlow [58]. In this work, the fourth version of the\\\\nmodel is used. The implementation from Tfhub uses the DAN architecture [68]. The USE\\\\nfile has a size of about 1 GB. It is not necessary to preprocess the data for the model.\\\\nSBERT\\\\nThe SBERT model is implemented with PyTorch [58]. An instance of the model is ob-\\\\ntained as shown in Listing 4.10. The model contains a BERT transformer, which has a\", \"4 Own approach 42\\\\nmax_seq_length of128. It does not convert inputs to lowercase by default [57]. The\\\\noutput of the BERT transformer is passed to a pooling layer, which is initialized with the\\\\npooling_mode parameter. The default is mean_pooling , which calculates the mean of the\\\\noutput vectors of the transformer. The other options include cls_token_pooling , which\\\\nreturns the output of the first token and max_pooling , which returns the maximum value\\\\nof the output vectors. The word embedding dimension is 384 by default [57].\\\\n1SentenceTransformer( \\'paraphrase-MiniLM-L6-v2 \\')\\\\nListing 4.10: Initialization of the SBERT model.\\\\n4.1.4 Clustering using OPTICS\\\\nFigure 4.8: The first page of each document is converted to an image. The image is pre-\\\\nprocessed, i.e. conversion to greyscale and resizing.\\\\nSimilar to the approach from Ankerst et al., OPTICS is used to cluster the images of the\\\\nfirst page of documents in this work. The procedure is displayed in Figure 4.8. There are\\\\ntwo different preprocessing approaches:\\\\n1. The images are first preprocessed to 32x32 normalized greyscale pixels (cf. [5]) as\\\\nvisualized in Figure 4.10 and afterwards compressed to 13-dimensional vectors using\\\\nPCA.\\\\n2. The technique Eigendocs from Subsection 3.5.2 is used to compress the images to\\\\n13-dimensional normalized greyscale images as displayed in Figure 4.4.\\\\n1optics_model = OPTICS(cluster_method= \\'dbscan \\', min_samples=2, max_eps=10,\\\\n2 eps=1.5)\\\\nListing 4.11: Initialization of the OPTICS model. The minimum number of samples\\\\nmin_samples in a cluster corresponds to minPts.\\\\nThe configurations used when initializing an OPTICS model greatly influence the clusters\\\\nreturned. The parameter max_eps is infinity by default but can be specified by the user\\\\nto reduce complexity and runtime. According to literature, max_eps should be big enough\\\\nto include almost all points in a single cluster. The way the reachability plot is used to\\\\nextract clusters is dependent on the cluster_method . One can choose either dbscanorxi\", \\'4 Own approach 43\\\\n0 500 1000 1500 200002468Reachability (epsilon distance)Reachability Plot\\\\nclass 0\\\\nclass 1\\\\nclass 2\\\\nclass 3\\\\nclass 4\\\\nnoise\\\\n(a) The reachability plot of the documents\\\\npreprocessed according to item 1 (cf.\\\\n[5]).\\\\n0 500 1000 150002468Reachability (epsilon distance)Reachability Plot\\\\nclass 0\\\\nclass 1\\\\nclass 2\\\\nclass 3\\\\nclass 4\\\\nnoise(b) The reachability plot of the documents\\\\npreprocessed according to item 2 (i.e.\\\\nEigendocs).\\\\nFigure 4.9: The plot was created using the OPTICS algorithm from the Python library\\\\nscikit-learn. The underlying dataset consists of 2241 documents from the Ba-\\\\nhamas leak. It shows the reachability distance of each document to its prede-\\\\ncessor in the order list.\\\\nas a clustering method. The parameters min_samples and epsinfluence the cluster sizes\\\\nand number of clusters found for a given clustering approach. The value of epsdefines\\\\nthe distance between two points to still be considered neighbours and can be chosen by\\\\nconsulting the reachability plot which is displayed in Figure 4.9. The code to initialize an\\\\nexemplary OPTICS model is displayed in Listing 4.11.\\\\n4.1.5 Topic analysis\\\\nTwo topic analysis approaches are outlined below. The first one serves as the baseline\\\\nmodel of this thesis and the second one is used multiple times throughout the application\\\\nto visualize results obtained by queries.\\\\nTop2Vec\\\\nAngelov’s Top2Vec model is provided in the Python library Top2Vec [4]. In his work,\\\\nUMAP’s hyperparameters are set to 15 nearest neighbours, cosine similarity as the distance\\\\nmetric and 5 as the embedding dimension. The word and document embeddings are\\\\ngenerated by the Doc2Vec version PV-DBOW. The window and vector size are 15 and\\\\n300 respectively and a hierarchical softmax is used.\\\\nIn this work, a class is implemented, which uses the Top2Vec library. When initiating\\\\nan instance of this class, the Top2Vec model is trained on the given document corpus as\\\\ndisplayed in Listing 4.12. The class provides methods to query for the number of topics\\\\nas well as the most similar topics and documents to an input keyword. The most similar\\\\ntopics can be visualized using word clouds. The core functionalities are implemented by\\', \\'4 Own approach 44\\\\nFigure 4.10: Preprocessing of 100 documents to 32x32 normalized greyscale pixels.\\', \"4 Own approach 45\\\\nthe Top2Vec library, but the class is used to modify the return values to be compatible\\\\nwith the UI.\\\\n1Top2Vec(documents=self.documents, speed= \\'fast-learn \\', workers=8)\\\\nListing 4.12: Initialization of the Top2Vec model.\\\\nWord clouds\\\\nThe implementation of word clouds in this thesis is based on the Python library wordcloud\\\\nby Müller [51]. This implementation removes English stop words from the text by default.\\\\nThe input text is split into tokens using a regex. By default, plurals are removed if their\\\\nsingularversionispresentandtheirfrequencyisaddedtotheirsingularversion. Bydefault,\\\\nnumbers are not included as tokens.\\\\nIn order to ensure that the words presented are interpretable, the input text is preprocessed\\\\nas displayed in Listing 4.13. The WordNetLemmatizer from the nltkpackage is used to\\\\nensure the stemmed words exist. A word cloud is initialized as shown in Listing 4.14.\\\\n1lemmatizer = WordNetLemmatizer()\\\\n2tokens = [lemmatizer.lemmatize(token) for token in tokens]\\\\nListing 4.13: Custom preprocessing of word cloud input.\\\\n1wordcloud = WordCloud(width=800, height=500, random_state=21,\\\\n2 contour_width=3, max_font_size=110, background_color= \\'white \\',\\\\n3 max_words=5000).generate( \\',\\'.join(tokens))\\\\nListing 4.14: Initialization of a word cloud.\\\\n4.1.6 Slurm\\\\nSince the data corpus is too big to be processed locally on a Apple M2 Pro MNW83D/A\\\\nwith 16 GB RAM and 12 cores, the Chair Intelligent Embedded Systems (IES) has offered\\\\nto provide computational means to solve this problem. The scripts can be processed by\\\\nmultiple nodes which are managed by Slurm. Slurm is an open-source management tool\\\\nfor Linux clusters [1]. It allocates resources, i.e. compute nodes, and provides the means\\\\nto start, execute and monitor jobs [1, 73].\\\\nThe so-called Slurm daemons control nodes, partitions, jobs and job steps [1]. A partition\\\\nis a group of nodes and a job is the allocation of resources, i.e. compute nodes, to a user for\\\\na limited period of time. A basic visualization of the architecture is given in Figure 4.11.\", \\'4 Own approach 46\\\\nFigure 4.11: Slurm architecture. The management node has a slurmctld daemon, while\\\\nevery compute node has a slurmddaemon. The nodes communicate. The\\\\nuser can use certain commands, for instance srunand squeue, anywhere on\\\\nthe cluster.\\\\nTable 4.2: A selection of sbatchscripts used in this work.\\\\nName of sbatch script Description\\\\nae_config.sh Comparison of different AE architectures in terms of the met-\\\\nrics cosine similarity and Root Mean Square Error (RSME).\\\\nallocate_res.sh Allocates resources to enable a SSH tunnel connection from\\\\na local VSCode instance to the server of the IES. When en-\\\\nabled, the database content can be displayed with the Elas-\\\\nticsearch plugin.\\\\ncreate_database.sh Initializes the database by specifying fields.\\\\ncreate_documents.sh Inserts the document’s metadata information, i.e. path and\\\\ntext.\\\\nelasticContainer.sh Starts the Elasticsearch container using the headless\\\\npodman-compose up command.\\\\ninit_database.sh Initializes database, subsequentially inserts documents meta-\\\\ndata, embeddings and clusters.\\\\ninsert_clusters.sh Inserts PCA weights, OPTICS and argmax clusters.\\\\ninsert_embeddings.sh Subsequentially inserts embeddings of documents.\\\\nown_w2v_model.sh Creates and saves custom Word2Vec model.\\\\nrun_pdf2png.sh Converts and saves the PNG version of the first page of the\\\\nPDFs.\\\\nA job is started by a sbatchscript. This script defines the partition , the job-name ,\\\\nthe number of nodes, the cpus-per-task , the memory memallocated, the timelimit\\\\nand the path to store errorand outputlogs. It is possible to work on multiple CPUs\\\\nsimultaneously to divide the workload of a task. In this work, multiple sbatchscripts\\\\nare used to carry out a variety of tasks. A summary of the tasks and scripts is given in\\\\nTable 4.2.\\', \\'4 Own approach 47\\\\n4.2 Web interface\\\\nA basic web interface is provided to facilitate the comparison of the models explored in\\\\nthis thesis. However, the focus of this work is on the methods and not on the application.\\\\nThe tool consists of a backend and a frontend which are described in Subsection 4.2.1 and\\\\nSubsection 4.2.2.\\\\n4.2.1 Backend\\\\nThe framework used for the backend is Flask. There are multiple endpoints, which are\\\\nused to retrieve data from the server:\\\\n•Documents: Returns a list of documents, which best match the query. The infor-\\\\nmation returned for each document is the respective id,path, and text. The query\\\\ncan be of type match_all , which returns all documents in the database, or a fuzzy\\\\nfull-text query if textis specified, or a kNN query on a certain field of the database\\\\nif both knn_type andknn_source are given. Moreover, the number countand start\\\\nindex pageof the results returned can be specified. By default, the first 10 documents\\\\nare returned.\\\\n•Document: Returnsthemetadata, i.e.textandpath, ofadocumentwiththespecified\\\\nid. The URL to access this endpoint is /documents/<id> .\\\\n•PDF: Returns the PDF file. This endpoint is used to display the PDF in the detail\\\\ncomponentofthefrontend. TheURLtoaccessthisendpointis /documents/<id>/pdf .\\\\n•WordCloud: Returns the bytes of a word cloud image. Depending on additional\\\\nparameters, the word cloud is either generated from one document or a group of\\\\nsimilar documents. If the knn_type is specified, a query for the countmost similar\\\\ndocuments is performed. By default, countis 10. The URL to access this endpoint\\\\nis/documents/<id>/wordcloud .\\\\n•Term Frequency: Returns the term frequency calculated for the specified document.\\\\nThe URL to access this endpoint is /documents/<id>/term_frequency .\\\\n•TopicWordCloud: If termis specified this endpoint returns a word cloud of the\\\\nterms that describe the topics most similar to the query term. The parameter count\\\\nspecifies the number of topics to be returned. Its default value is 3. The topics are\\\\ngenerated by Top2Vec. The URL to access this endpoint is /topics/wordcloud .\\\\n•Topics: Returns the topics generated by Top2Vec. The topics are described by the\\\\nwords closest to the topic vectors. The URL to access this endpoint is /topics.\\\\n4.2.2 Frontend\\\\nThe framework used for the frontend is Angular. There are three main components, which\\\\nare used to display the data:\\', \\'4 Own approach 48\\\\n•Home: The home component is used to display the results of a text query. It consists\\\\nof a search bar, which is used to enter the query term, and a list of results. If no text\\\\nquery is entered the first documents of the database, i.e. the result of a match_all\\\\nquery, are displayed. The search component is shown in Figure 4.12.\\\\n•Detail: The detail component is used to display the details of a document. The\\\\ndocument name and ID are located on the left side of the screen. Beneath the\\\\ndocument name and ID, a button which opens the term frequency image on a new\\\\npage is located. Moreover, the word cloud of the document is displayed. The word\\\\ncloud is generated from the text of the document. On the right side of the screen,\\\\nthere is a PDF viewer which displays the pages of the document. Beneath the PDF\\\\nviewer, the file names and a word cloud of the most similar documents are displayed\\\\nafter a query for them is initiated by the user. The detail component is shown in\\\\nFigure 4.13.\\\\n•Topic: The topic component is used to display the topics of the documents. The\\\\ntopics are lists of words generated by top2vec. The user can query for the most\\\\nsimilar topics to a term. The results are displayed as a word cloud. The upper limit\\\\nof the number of topics can be defined by the user. The topic component is shown\\\\nin Figure 4.14.\\\\nFigure 4.12: Home component of the frontend. The search bar is used to enter the text\\\\nquery. The results of the query are displayed below the search bar.\\', \\'4 Own approach 49\\\\nFigure 4.13: Detail component of the frontend. The chosen document is displayed, as\\\\nwell as its most similar documents in the database. The word clouds of the\\\\ndocument and the most similar documents are displayed.\\', \\'4 Own approach 50\\\\nFigure 4.14: Topic component of the frontend. The topics identified by Top2Vec are listed.\\\\nBelow them, the user can query for the most similar topics to a term. The\\\\nresults are displayed as a word cloud.\\\\n4.3 Trade-off between memory and query time\\\\nAt the beginning of this thesis, it was unclear to what degree the tool, i.e. the database\\\\nfields and query results should be precomputed. A tool which is trained once offline is\\\\nbeneficial due to the amount and the nature of the data. The Bahamas leak is static and\\\\nthus, the database does not need to be updated with new documents.\\\\nIn the course of filling the database with information, one had to face obstacles not only\\\\nregarding excessive memory usage but also long run times of methods. Early on it became\\\\nevident that one either had to reduce accuracy and details in order to achieve less memory\\\\nor one had to settle for minutes to hours of calculations and bigger costs in terms of memory\\\\nconsumption.\\\\nBeforehand, it was not clear which information, i.e. fields in the database, seemed worth the\\\\ntime and memory. For instance, initially, the image of the first PDF page of each document\\\\nwas saved alongside the other fields within the database. After scaling the amount of data\\\\nstored in the database to about 2900 documents, this approach caused severe issues in\\\\nterms of memory usage. Hence, this field is omitted.\\', \\'5 Evaluation 51\\\\n5 Evaluation\\\\nSince the dataset has no ground truth, the procedure used to pick the parameter values\\\\nis not comparable to ground truth-based approaches. Hence, the evaluation is informal\\\\nand the methods applied have arisen from regular consultation with experts from the tax\\\\noffice. Run times of different configurations are measured and compared. Parameters are\\\\nchosen with respect to model-specific procedures, such as reachability plots for OPTICS.\\\\nThe models are compared to each other and the tool constructed from the composition of\\\\nthe models is compared to the baseline topic analysis model Top2Vec.\\\\n5.1 Database\\\\nThere is a variety of parameter values to choose from when working with databases. The\\\\nchoice of the similarity metric is discussed first. Secondly, the reasons for choosing Elas-\\\\nticsearch as a database are presented.\\\\nSimilarity measurements\\\\nAccording to Reimers and Gurevych, the similarity measurements discussed in Section 3.3\\\\nobtained roughly the same results in their experiments [58].\\\\nAs the similarity between vectors is usually calculated using some form of cosine similarity,\\\\nrather than Euclidean distance in literature, cosine similarity is preferable over Euclidean\\\\ndistance. Since the models may produce embeddings which are not normalized, the cosine\\\\nsimilarity is used instead of the dot product.\\\\nElasticsearch\\\\nAccording to Grinberg, Structured Query Language (SQL) databases are a good choice\\\\nfor efficiently storing structured data. This is because their paradigm ACID, i.e. Atom-\\\\nicity, Consistency, Isolation, Durability, provides high reliability. Not only SQL (NoSQL)\\\\ndatabases, on the other hand, are more flexible and can be used to store unstructured\\\\ndata [25]. They do not require a predefined schema and can therefore accept documents\\\\nof arbitrary structure [22]. Usually, NoSQL databases do not offer services such as JOINs.\\\\nNoSQL databases are said to outperform out-of-the-box SQL databases. Since the dataset\\\\nconsists of unstructured documents and the task at hand does not require performing any\\\\nJOINs, a NoSQL database is favourable. Elasticsearch is chosen since it is well known to\\', \\'5 Evaluation 52\\\\nprovide near real-time search and to operate on big data. Subsequently, it is a good fit for\\\\nthe underlying dataset.\\\\nSince Elasticsearch stores vectors of at most 2048 dimensions, the TF-IDF and InferSent\\\\nembeddings are problematic. Besides imposing limits to the dimensionality of the em-\\\\nbeddings, Elasticsearch offers a variety of convenient functionalities, such as the built-in\\\\nkNN search. Therefore, in this work, Elasticsearch is used regardless of the dimensionality\\\\nconstraints imposed by the database. Hence, the techniques are adjusted to the database\\\\nand not vice versa.\\\\nThe time necessary to fill the Elasticsearch database has been evaluated and improved\\\\nthroughout this work. The current time measurements are shown in Figure 5.1. The\\\\ntimes correspond to calculation of 2048 embeddings. It is possible to measure this com-\\\\npuation time individually since the task of filling the database is modulized. Modulizing\\\\nis beneficial since it is possible to update the embeddings without having to recreate the\\\\ndatabase. Moreover, it facilitates debugging and comparing the models used to create the\\\\nembeddings.\\\\n101102103\\\\nTime (seconds)InferSent + AESBERTTFIDF + AEdoc2vecUSE\\\\n4366.5651.13517.0866.895.6Time necessary to compute 2048 embeddings\\\\nFigure 5.1: Time per module of creating the Bahamas database using a random selection\\\\nof 2048 documents. The x-axis is logarithmic. The reference time is measured\\\\nusing cProfiler on a Apple M2 Pro MNW83D/A with 16 GB RAM and 12\\\\ncores.\\\\n5.2 Eigendocs\\\\nIn order to determine the optimal number of components used for Eigendocs the cumulative\\\\nexplained variance and the reconstruction error are plotted as displayed in Figure 3.15\\\\nfrom Subsection 3.5.2. The first plot indicates that 90% of the variance is explained by 441\\\\ncomponents. Usually, that would have been the number of dimensions of the subspace onto\\', \\'5 Evaluation 53\\\\nwhich the documents would have been projected. However, when working with clustering\\\\nalgorithms like OPTICS, the number of dimensions should be reduced even further to\\\\nachieve valid clusters. Therefore the reconstruction error with respect to different numbers\\\\nof components is taken into consideration.\\\\n1sqr_dif = (X_test - X_test_pca_inverse)**2\\\\n2reconstr_err.append(np.sqrt(np.mean(sqr_dif))/\\\\n3 (np.sum(np.abs(1-X_test))/X_test.shape[0]))\\\\nListing 5.1: Adaption of the RSME: Firstly, the squared differences between the original\\\\nand the reconstructed images are calculated. Since the values are normalized,\\\\na 1 corresponds to a white pixel. Then, the absolute values of all non-white\\\\npixels of the test set are summed up. The average number of non-white pixels\\\\nis calculated by dividing the sum by the number of images in the test set. This\\\\napproach considers pixels of value p∈[0,1]as(p·100)% white and thus, they\\\\nare incorporated in the sum.\\\\nUsually, a RSME is minimized to determine the optimal parameter configurations. In this\\\\ncase, the reconstruction error shall be interpreted. To facilitate the interpretability of the\\\\nreconstruction error, its calculation is adapted to incorporate the content of the images.\\\\nAt first sight, the majority of image pixels are white, i.e. do not convey any information.\\\\nTherefore, the reconstruction error is divided by the average number of non-white pixels.\\\\nHence, the reconstruction error of an image is weighted by the amount of information it\\\\nconveys. The calculation is given in Listing 5.1. The result is displayed in Figure 3.15.\\\\nSince the reconstruction error increases less rapidly after 10 to 20 components, the number\\\\nof components is set to 13, which has been an “elbow” point in a similar trial using a not\\\\nrandomly selected dataset of 195 images.\\\\nSome impressions of the Eigendocs algorithm are displayed in Figure 4.4. Assuming that\\\\nthe selection of documents is representative, the preprocessing of the documents using\\\\nEigendocs should have encoded information about the dimensionality of the images. How-\\\\never, this assumption is not valid since bigger images exist. Therefore, the idea of incor-\\\\nporating information about the image’s dimensions is not entirely implemented.\\\\n5.3 Embeddings\\\\nAs discussed in Subsection 4.1.3, there is a range of possible parameter values to choose\\\\nfrom when implementing embedding models. The section below states which findings have\\\\nled to the parameter values applied in this work.\\', \\'5 Evaluation 54\\\\nTF-IDF\\\\nThe main obstacle to overcome is the high dimensionality of the TF-IDF embeddings.\\\\nHence, the goal of the parameter selection is to find a way to reduce the dimensionality of\\\\nthe vocabulary to 2048 which is the maximum dense vector dimensionality of Elasticsearch.\\\\nHowever, the quality of the embeddings should not decline too much.\\\\nThe choice of the preprocessor is investigated with regard to the goal of minimizing the\\\\nvocabulary size. Both the default and a custom preprocessor are tested on a data corpus\\\\nof 2048 randomly selected documents concerning the vocabulary (size). While the default\\\\npreprocessor had a vocabulary size of 5893, the custom preprocessor had a size of 5585.\\\\nThe relative differences between vocabulary sizes seem to be inversely proportional to the\\\\ndataset size since the trend is already visible for two different data corpus sizes in Table 5.1.\\\\nThecustompreprocessorischosenbecauseithadasmallervocabularysize. Thedifferences\\\\nbetween both vocabularies are visualized in Figure 5.2.\\\\nTable 5.1: Comparison of vocabulary sizes resulting from the default and the custom\\\\nTF-IDF preprocessor on different data corpus sizes.\\\\nfirst trial second trial\\\\ndocument corpus size M 195 2048\\\\ncustom preprocessor vocabulary size A 1521 5585\\\\ndefault preprocessor vocabulary size B 1641 5893\\\\n(B-A)/M 120/195 =\\\\n0,6153846154308/2048 =\\\\n0,150390625\\\\nWord cloud in vocabular which is in default but not custom\\\\n(a) The terms only present in the vocabulary\\\\nproduced by the default preprocessor.\\\\nWord cloud in vocabular which is in custom but not default(b) The terms only present in the vocabulary\\\\nobtained from the custom preprocessor.\\\\nFigure 5.2: The word clouds visualize which words are unique to both vocabularies on a\\\\nrandom selection of 2048 documents.\\\\nAs stated in Section 5.1, the TF-IDF embeddings can be problematic with regard to the\\\\ndimensionality limitations imposed by Elasticsearch. The parameters min_dfand max_df\\\\nare set to values which keep the vocabulary size small and thus, the dimensionality of the\\\\nembeddings is reasonably small. Furthermore, this work employs dimensionality reduction\\', \\'5 Evaluation 55\\\\ntechniques to reduce the dimensionality of the embeddings if the embeddings have a higher\\\\ndimensionality than 2048.\\\\nDoc2Vec\\\\nSince no labeled data is available, the evaluation of the Doc2Vec embeddings is limited.\\\\nTherefore, theDoc2Vecembeddingsareevaluatedbycomparingthemtootherembeddings.\\\\nThe Doc2Vec model is not tuned in terms of hyperparameter selection, but the default\\\\nsettings are used since there is no way to evaluate the resulting embeddings.\\\\nInferSent\\\\nThe maxpooling type is used for the InferSent model, since Conneau et al. found by\\\\nconducting experiments using different pooling techniques that it is the best option [14].\\\\nInitially, in this work, the Global Vectors (GloVe) word embeddings were used for the\\\\nInferSent model. However, since the file of precomputed GloVe word embeddings has a\\\\nsize of 5.65 GB and thus, slows down the model, ultimately another word embedding is\\\\nused. The time necessary to compute and insert 195 documents for specific embeddings\\\\nis displayed in Figure 5.3. The custom word embedding used in this work is a Word2Vec\\\\nmodel trained on a selection of 2048 randomly selected documents from the Bahamas\\\\ndataset.\\\\nPennington et al. state that GloVe outperforms Word2Vec on the same corpus, vocabulary\\\\nand window size in terms of quality and speed [55]. Hence, the quality of the results\\\\nobtained in this work may have suffered from using a custom Word2Vec instead of GloVe.\\\\nHowever, sincethecomputationtimeoftheprojectisacrucialfactor, thecustomWord2Vec\\\\nis used.\\\\n0 200 400 600 800\\\\nTime (seconds)Precomputed (GloVe)Custom (Word2Vec)\\\\n899.218159.627InferSent time per used embedding on 195 documents\\\\nFigure 5.3: Reference time necessary to calculate and insert 195 InferSent embeddings for\\\\ndifferent precomputed word embeddings on a Apple M2 Pro MNW83D/A with\\\\n16 GB RAM and 12 cores. Using a custom Word2Vec model is around 5.5\\\\ntimes faster than GloVe.\\', \\'5 Evaluation 56\\\\nUSE\\\\nSincetherearenoparameterstocustomizetheevaluationoftheUSEembeddingsislimited.\\\\nTherefore, the USE embeddings are evaluated by comparing them to other embeddings.\\\\nAE\\\\nIn order to determine which architecture for the hidden or so-called latent space of the AE\\\\nis the best option, different architectures are tested and compared in terms of RSME and\\\\ncosine similarity. The RSME is calculated as given in Listing 5.2. The cosine similarity is\\\\ncalculated as given in Listing 5.3. Due to the fact that cosine similarity values are bound\\\\nby 0 and 1, they are easier to rank than metrics that can yield any real number. However,\\\\ncosine similarity is usually applied to calculate the angle between two vectors and thus,\\\\none has to be cautious when interpreting the results. For instance, the vectors (0,1)Tand\\\\n(0,2)Thave a cosine similarity of 1, even though they are not the same vectors. Since\\\\nan AE is supposed to reconstruct the input rather than return a dependent or related\\\\nvector, this metric should be combined with a traditional metric. The dataset used for the\\\\nevaluation is a selection of 195 documents from the Bahamas dataset.\\\\n1rsme = np.linalg.norm(inverse_embedding - embeddings)\\\\n2 / np.sqrt(embeddings.shape[0])\\\\nListing 5.2: Computation of the RSME between the original and the reconstructed embed-\\\\nding.\\\\n1cos_sim = statistics.mean([np.dot(inverse_emb, embedding)\\\\n2 /(np.linalg.norm(inverse_emb)*np.linalg.norm(embedding))\\\\n3 for inverse_emb, embedding in zip(inverse_embedding, embeddings)])\\\\nListing 5.3: Computation of the cosine similarity between the original and the recon-\\\\nstructed embedding.\\\\nThe scores of different architectures are shown in Figure 5.4. The x-axis displays the\\\\nnumber of neurons in each layer for the respective experiments. The input space is 4096-\\\\ndimensional since that is the dimensionality of InferSent embeddings. The output of the\\\\nencoder is 2048-dimensional which is the maximum dimensionality supported by Elastic-\\\\nsearch for dense vectors. While most of the architectures produce similar results, one\\\\narchitecture stands out. Combining 2500-, 3000- and 3500-dimensional layers in the hid-\\\\nden space produces the worst RSME results. The smallest RSME and the biggest cosine\\\\nsimilarity are achieved by adding a 3500-dimensional layer in the hidden space. However,\\\\nthe results of the best architecture do not differ greatly from the others.\\', \\'5 Evaluation 57\\\\n[2048, 3000, 4096]\\\\n[2048, 3000, 3500, 4096] [2048, 2500, 3000, 4096]\\\\n[2048, 2500, 3000, 3500, 4096][2048, 3500, 4096] [2048, 3000, 4096] [2048, 2500, 4096][2048, 4096]\\\\nNetwork architecture (dimensions)0.00.51.01.52.02.53.03.5Quality of reconstruction1.9661.90327 1.86792.81196\\\\n1.827271.92339 1.875351.95645\\\\n0.932177 0.930991 0.930454 0.908204 0.933453 0.931852 0.932244 0.926698Architecture comparison of autoencoders\\\\nrsme cosine_similarity\\\\nFigure 5.4: The effect of different AE architectures on the reconstruction error. The error\\\\nis measured in terms of RSME (blue bars) and cosine similarity (yellow bars)\\\\nbetween the original and the reconstructed image. The smallest RSME and\\\\nthe biggest cosine similarity belong to the architecture best suited to this task\\\\nand are coloured green.\\', \\'5 Evaluation 58\\\\n5.4 Clustering using OPTICS\\\\nx1012345x2\\\\n02468x3\\\\n4\\\\n2\\\\n0246OPTICS identified clusters\\\\n(a) Preprocessing according to item 1 (cf.\\\\n[5]).\\\\nx10\\\\n1\\\\n2\\\\n3\\\\n4x2\\\\n4\\\\n2\\\\n02468x3\\\\n4\\\\n2\\\\n024OPTICS identified clusters(b) Preprocessing according to item 2 (i.e.\\\\nEigendocs).\\\\nFigure 5.5: The clusters are extracted from the respective reachability plots in Figure 4.9\\\\nby OPTICS. The points in the three-dimensional space correspond to the\\\\nweights of the first three principal components. The blue points are noise\\\\npoints, whereas any other colour denotes a cluster.\\\\nThe algorithm OPTICS is applied to data, which is preprocessed according to item 1\\\\n(cf. [5]) and item 2 (i.e. Eigendocs) from Subsection 4.1.4. The clusters from Figure 5.5\\\\nare extracted from the respective reachability plots in Figure 4.9. The three-dimensional\\\\nplots visualize the first three dimensions of the data and thus, the weights of the first\\\\nthree principal components assigned by the Eigendocs algorithm. By visual inspection and\\\\ncomparison of both plots, it can be seen that the projection by the Eigendocs approach of\\\\nitem 2 scatters the objects further in the three-dimensional space. One could argue that\\\\nthis is due to the fact, that the input data encodes not only the visual appearance in terms\\\\nof page layout but also the size of the document. Possibly, the objects are grouped by\\\\ndocument size.\\\\nTo analyze the results of the clustering, the content of the clusters is examined. Since\\\\nthe documents are not labeled, the content of the clusters is analyzed by visual inspection\\\\nand displayed in Figure 5.6. The yellow images belong to the group identified as noise.\\\\nThe images preprocessed according to item 1 are partitioned into more clusters than the\\\\nEigendocs approach. Most of the certificates are classified as noise for both approaches in\\\\nthe trials carried out.\\\\nThe approach from item 1 (cf. [5]) omits information about the images’ original size. This\\\\ninformation is encoded in the Eigendocs approach. Hence, the preprocessing approach\\\\nchosen to create the OPTICS input for the Elasticsearch database index is Eigendocs.\\\\nAccording to Deng et al., OPTICS was developed to improve DBSCAN flaws. With respect\\\\nto the evolution of these clustering methods, i.e. DBSCAN being OPTICS basis, DBSCAN\\', \\'5 Evaluation 59\\\\n(a) Preprocessing according to item 1 (cf.\\\\n[5]).\\\\n(b) Preprocessing according to item 2 (i.e.\\\\nEigendocs).\\\\nFigure 5.6: In this visualization, at most 10 random elements of a cluster are displayed.\\\\nThe yellow images belong to the group denoted as noise. Most certificates are\\\\nclassified as noise.\\', \\'5 Evaluation 60\\\\nis chosen for the clustering method in Listing 4.11. In order to reduce calculation com-\\\\nplexity the maximum εis 10. The distance between two points to still be considered\\\\nneighbours is defined after a visual inspection of the reachability plot. Considering the\\\\nintrinsic structure of the Eigendocs data, it is set to 1.5to return meaningful clusters.\\', \\'5 Evaluation 61\\\\n5.5 Comparison of models\\\\nThis evaluation does not aim to find the best model but to compare the similarity of the\\\\nmodels’ query results. It is a qualitative evaluation of a selection of documents from the\\\\nBahamas leak. This selection is a 2048 document corpus that is randomly chosen without\\\\nreplacement. A query defines a field, i.e. embedding model, and a query document. The\\\\nquery response consists of the documents that are considered most similar to the query\\\\ndocument in terms of cosine similarity.\\\\nThe differences between the models are illustrated by visualizing the first five response\\\\ndocuments of a sample query. The text of the query document is encoded using the\\\\nrespective model. A kNN query is used to obtain the results from the local database\\\\ncontaining 2048 documents. The query results are displayed in Figure 5.7. The query\\\\ndocument, i.e. the image surrounded by a border, is omitted from the query response. The\\\\ndocuments in the query response are listed according to descending similarity to the query\\\\ndocument. All models except Doc2Vec and USE returned only documents of CREDIT\\\\nSUISSE. Apart from this difference, the query responses of the models are very similar.\\\\nTofurtherinvestigatethedifferencesbetweenthemodels, thequeryresponsesofthemodels\\\\nare compared qualitatively on query documents that are considered unusual in terms of\\\\ntheir appearance. The document in Figure 5.8 is a table consisting of little text compared\\\\nto other samples from the data corpus. The document in Figure 5.9 is mostly handwritten,\\\\nwhich is unusual since most other samples are computer-generated.\\\\nThe models produced good query responses on a query document consisting of little text as\\\\nshown in Figure 5.8. Even though at first glimpse, the response documents seem unrelated\\\\nto the query document, they share multiple words, such as director. Similar response\\\\ndocuments do not have to be of similar visual appearance since the text embeddings only\\\\nconsider information from the text layer.\\\\nThere are query documents such as the one in Figure 5.9 that reveal the dissimilarity\\\\nbetween certain models. Most models’ response documents are similar to Figure 5.9(b).\\\\nThese response documents depict the same type of document, i.e. handwritten receipts for\\\\nan annual fee. The TF-IDF model, however, returns different response documents. The\\\\nresponse documents from Figure 5.9(a) are not handwritten and cover different content,\\\\ni.e. requesting payment and three documents concerning an address change. TF-IDF’s\\\\nresults could thus be considered to be of poor quality.\\\\nSince not only textual information but also visual information is encoded in the database,\\\\nthe next step is to compare the query responses of approaches that consider visual similar-\\\\nity. The query responses are clustered using OPTICS or argmaxof the PCA compression.\\\\nThe first exemplary query document in Figure 5.10 is an image of a usual document.\\\\nBoth clustering approaches yield similar results. More specifically, the responses share two\\\\ndocuments. Moreover, the last document in the response of both approaches differs most\\', \\'5 Evaluation 62\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n5. response documentMost similar images found by doc2vec\\\\n(a) Doc2Vec\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by hugging\\\\n(b) SBERT\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by infer\\\\n(c) InferSent\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by tfidf\\\\n(d) TF-IDF\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by universal\\\\n(e) USE\\\\nFigure 5.7: Exemplary query response for different embeddings on the same query docu-\\\\nment.\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n5. response documentMost similar images found by infer\\\\nFigure 5.8: InferSent query responses on a query document consisting of little text.\\', \\'5 Evaluation 63\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by tfidf\\\\n(a) TF-IDF\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n5. response documentMost similar images found by hugging\\\\n(b) SBERT\\\\nFigure 5.9: Qualitative comparison of query responses. The majority of the query docu-\\\\nment consists of handwritten text. The results of the TF-IDF model are not\\\\nsimilar to the query document. The other models, including SBERT, produce\\\\nresults that are more similar to the query document.\\\\nfrom the group. Most documents have a similar visual appearance, i.e. they have a stamp.\\\\nNone of the result documents originate from the same company as the query document.\\\\nThe second exemplary query document in Figure 5.11 is a certificate. The OPTICS clus-\\\\ntering approach yields a response that is more similar to the query document than the\\\\nargmaxof the PCA compression because all its responses are from the same document\\\\ntype as the query document. Hence, the OPTICS clustering approach is considered to be\\\\nsuperior to the argmaxapproach.\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by pca_optics_cluster\\\\n(a) OPTICS\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by argmax_pca_cluster\\\\n(b)argmaxof PCA compression\\\\nFigure 5.10: Qualitative comparison of query responses. The response documents are clus-\\\\ntered using OPTICS or argmaxof the PCA compression. They are not com-\\\\npared in terms of textual but visual similarity.\\\\nAnother approach to compare the response documents is to visualize the intersection of\\\\nthe query results. The Venn diagram is chosen since it displays intersections of all items of\\', \\'5 Evaluation 64\\\\nquery document\\\\n 1. response document\\\\n 2. response document\\\\n 3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by pca_optics_cluster\\\\n(a) OPTICS\\\\nquery document\\\\n 1. response document\\\\n2. response document\\\\n3. response document\\\\n 4. response document\\\\n 5. response documentMost similar images found by argmax_pca_cluster\\\\n(b)argmaxof PCA compression\\\\nFigure 5.11: Qualitative comparison of query responses. The response documents are clus-\\\\ntered using OPTICS or argmaxof the PCA compression. They are not com-\\\\npared in terms of textual but visual similarity. The query document is a\\\\ncertificate.\\\\nthe power set of the models. To build a Venn diagram, the number of documents that are\\\\nshared between the query results of several models is computed. First, all query responses\\\\nof a model irrespective of the query document are saved in a set. The cardinalities of\\\\nthe intersections of multiple sets are displayed in Figure 5.12. Since five models encode\\\\ntextual information, the Venn diagram consists of five circles. One should be cautious\\\\nwhen interpreting the layout of the Venn diagram since the cardinality of an intersection\\\\nof documents does not correlate with its area in this visualization.\\\\nThe Venn diagrams in Figure 5.12 display the total number of shared response documents\\\\nfor 10 queries and 3, 5 or 10 response documents respectively. The models produce rather\\\\ndissimilar query results. Any combination of more than two models among the green\\\\n(Doc2Vec), orange (SBERT), red (TF-IDF) and blue (USE) models seem to produce rather\\\\ndissimilar results as the number in the respective areas is close to zero across all Venn\\\\ndiagrams.\\\\nSince the Venn diagrams compare the responses of the models irrespective of the query\\\\ndocument, another approach is to compare the query results of the models for each query\\\\ndocument individually. This approach first constructs a matrix of the number of shared\\\\nquery results between all model pairs summed up over all query documents. The matrix\\\\nconsists of five rows and five columns, where each row and column represents a model.\\\\nThe cell values are the number of shared query results between the models of the row\\\\nand column. It is possible to normalize the matrix to obtain the portion of shared query\\\\nresults. If two models produce the same query results, the cell value is either the total\\\\nnumber of query results or 1 if the matrix is normalized. Since the matrix is symmetric,\\\\nonly the upper triangular matrix is computed and the other half is mirrored. The matrix\\\\nis visualized using a heatmap as displayed in Figure 5.13. The code snippet in Listing 5.4\\\\nshows the calculation of the similarity matrix.\\', \\'5 Evaluation 65\\\\n17 19 2 14 3 \\\\n0 0 16 \\\\n1 0 \\\\n0 3 \\\\n2 2 \\\\n1 \\\\n22 \\\\n1 \\\\n1 1 2 \\\\n0 0 0 0 \\\\n0 0 \\\\n0 1 \\\\n0 0 \\\\n2 Venn diagram (10 queries à 3 responses in a 2048 document corpus)\\\\ndoc2vec\\\\nuniversal\\\\ninfer\\\\ntfidf\\\\nhugging\\\\n(a) 3 response documents per query.\\\\n26 34 2 23 3 \\\\n2 2 26 \\\\n3 0 \\\\n0 3 \\\\n2 1 \\\\n1 \\\\n29 \\\\n1 \\\\n1 0 2 \\\\n2 1 0 5 \\\\n1 0 \\\\n0 1 \\\\n1 0 \\\\n6 Venn diagram (10 queries à 5 responses in a 2048 document corpus)\\\\ndoc2vec\\\\nuniversal\\\\ninfer\\\\ntfidf\\\\nhugging (b) 5 response documents per query.\\\\n46 48 2 36 4 \\\\n1 1 34 \\\\n5 3 \\\\n2 7 \\\\n5 1 \\\\n7 \\\\n42 \\\\n4 \\\\n5 0 5 \\\\n0 3 0 8 \\\\n0 1 \\\\n0 0 \\\\n2 8 \\\\n7 Venn diagram (10 queries à 10 responses in a 2048 document corpus)\\\\ndoc2vec\\\\nuniversal\\\\ninfer\\\\ntfidf\\\\nhugging\\\\n(c) 10 response documents per query.\\\\nFigure 5.12: 10 documents are randomly sampled from a 2048 document corpus. For each\\\\nsampled document and model, a kNN query is conducted. The respective\\\\nresponse documents excluding the query document are saved. The cardinality\\\\nof the intersection of all response documents irrespective of query document\\\\nfor different models is visualized in terms of Venn diagrams.\\\\n1sim_matr = np.matrix(np.zeros((len(model_names), len(model_names))))\\\\n2for id in df.index:\\\\n3 for i, model in enumerate(model_names):\\\\n4 for j in range(i, len(model_names)):\\\\n5 sim_matr[i, j] += np.sum([df.loc[id,\\\\n6 model_names[j]].count(item) for item in df.loc[id, model]])\\\\n7 sim_matr[j, i] = sim_matr[i, j]\\\\n8if normalize:\\\\n9 sim_matr /= np.array(len(df.index)* len(df.iloc[0,0]))\\\\nListing 5.4: Calculation of the similarity matrix used to produce the heatmap.\\', \\'5 Evaluation 66\\\\ndoc2vec universal infer tfidf hugging\\\\nmodeldoc2vec\\\\nuniversal\\\\ninfer\\\\ntfidf\\\\nhuggingmodel1 0.323 0.31935 0.2649 0.2578\\\\n0.323 1 0.4138 0.31605 0.3236\\\\n0.31935 0.4138 1 0.32395 0.32055\\\\n0.2649 0.31605 0.32395 1 0.26445\\\\n0.2578 0.3236 0.32055 0.26445 1Portion of equal query results (2000 queries, 10 responses/query, 2048 document corpus)\\\\n0.30.40.50.60.70.80.91.0\\\\nFigure 5.13: Heatmap visualizing portion of shared query results on 2000 queries with 10\\\\nresponses each on a 2048 document corpus.\\\\nThe heatmap in Figure 5.13 shows that the models yield dissimilar query responses. USE\\\\nand InferSent produce the most similar responses, which indicates the maximum value of\\\\nshared query results. However, the maximum is less than 0.5and thus, rather dissimilar.\\\\nAny two-element combination of Doc2Vec, TF-IDF and SBERT produces the most dis-\\\\nsimilar query results, namely only around 25%of shared response documents per query.\\\\nSince the normalization does not consider the distribution of the cardinalities of the inter-\\\\nsections of the query results among the models, another approach is to calculate the mean\\\\nand standard deviation of the cardinalities of the intersections of the query results. There-\\\\nfore, 30 trials are conducted. Each trial consists of 10 randomly sampled query documents\\\\nwith 10 responses each (excluding the query document in the response). The normalized\\\\nsimilarity matrix for each model pair concerning one trial is calculated using Listing 5.4.\\\\nThe cardinalities are stored in a dictionary of lists indexed by the respective model pairs.\\\\nFinally, the mean and standard deviation are computed for each model pair and stored in a\\\\nComma Separated Values (CSV) file. The results are displayed in Table 5.2. The standard\\\\ndeviation does not exceed 0.1for any model pair and is similar among all combinations.\\\\nAs observed before, any two-element combination of Doc2Vec, TF-IDF and SBERT has\\\\nthe lowest ( 18−20%) mean portion of shared query results.\\', \\'5 Evaluation 67\\\\nTable 5.2: Mean and standard deviation of the average portion of shared response docu-\\\\nments of different models on a 2048 document corpus. One trial produced five\\\\nreal values, i.e. one portion per model. A portion is obtained from 10 randomly\\\\nsampled query documents with 10 responses each (excluding the query docu-\\\\nment in the response). The sample selection is based on the same dataset for\\\\neach trial and thus, query documents can be selected for multiple trials. There\\\\nwere 30 trials.\\\\nmodel 1 model 2 meanstd\\\\nDoc2Vec USE 0.260.09\\\\nDoc2Vec InferSent 0.260.07\\\\nDoc2Vec TF-IDF 0.20.08\\\\nDoc2Vec SBERT 0.180.06\\\\nUSE InferSent 0.330.08\\\\nUSE TF-IDF 0.250.08\\\\nUSE SBERT 0.250.06\\\\nInferSent TF-IDF 0.260.08\\\\nInferSent SBERT 0.240.06\\\\nTF-IDF SBERT 0.180.05\\\\n5.6 Comparison with baseline topic analysis approach\\\\nThe baseline topic analysis Top2Vec offers a variety of built-in functionalities to the user.\\\\nIt is possible to retrieve human interpretable inherent topics of a set of documents, as well\\\\nas the topics most similar to certain search terms and word clouds of these results. Hence,\\\\nthis library meets the needs articulated by this work.\\\\nOpposed to Top2Vec, this thesis proposes a composite of different approaches to encoding\\\\nvisual and semantic information and query for them using a database. To be more specific,\\\\nthis thesis not only relies on one semantic embedding model but offers several techniques\\\\nand an approach to incorporate visual information.\\\\nMoreover, the tool implemented in this thesis can display the term frequency of the doc-\\\\nument chosen in the detail component. The Top2Vec library does not offer a comparable\\\\nservice.\\\\nHowever, it is not possible to query for topics of the corpus which best describe a search\\\\nterm. Alternatively, one can perform a fuzzy text search on the documents. The user can\\\\ninspect the PDF of a document upon clicking on its name in the list of documents. The\\\\ndetail view enables the investigation of similar documents in terms of different embedding\\\\napproaches.\\\\nDue to Top2Vec’s architecture, documents and words are mapped into the same VSM.\\\\nHence, the topic vector definition and representation by its closest words are more mean-\\\\ningful than the approach of the thesis. In this thesis, a topic is represented by frequent\\\\nwords in the set of documents that are not necessarily meaningful.\\', \\'\\', \\'6 Conclusion 69\\\\n6 Conclusion\\\\nTo conclude this thesis, the research questions are revised. The insights acquired by ex-\\\\nploring different techniques with the goal of the exploration of large unstructured text data\\\\nare discussed in Section 6.1. Then, Section 6.2 points out the scientific contributions.\\\\n6.1 Discussion\\\\nRQ1seeks to discover whether visual embedding methods are suitable for the task of\\\\nfinding similar documents in large unstructured text corpora.\\\\nIn this work, firstly, the images are preprocessed using the Eigendocs approach as discussed\\\\nin Subsection 4.1.4 and Section 5.4. Then, the numerical data obtained from preprocessing\\\\nthe images is then reduced using PCA. Determining the number of components to be used\\\\nforPCAinameaningfulscientificwaywasproblematic(cf. Subsection3.5.2&Section5.2):\\\\nThe cumulative explained variance did not indicate a small number of components to use.\\\\nThe RSME calculation was found to be unsuitable for the dataset since multiple random\\\\nselections of documents from the dataset did not indicate a clear “elbow” point. The\\\\ncompressed images are clustered using the OPTICS algorithm and the argmaxapproach\\\\n(cf. Section 5.5).\\\\nIn general, the query responses from Section 5.5 which are based on visual information\\\\nconsist of visually similar documents. Hence, in terms of qualitative evaluation, the visual\\\\nrepresentation is considered to be valuable means to find visually similar documents in\\\\nlarge unstructured corpora. Consequently, the answer to RQ1 is positive but acknowledges\\\\nthe lack of scientific justification for the proof of concept’s configuration.\\\\nRQ2aims to find out whether different embedding methods produce similar results.\\\\nWhen comparing different semantic embedding methods in Section 5.5, slight differences\\\\nbetween the models become evident. TF-IDF, Doc2Vec and SBERT are most dissimilar\\\\nto each other. The TF-IDF approach performs rather poorly on unusual query documents\\\\nsuch as handwritten ones.\\\\nThe distinct response documents and their order are different for the same query document\\\\nregardingdifferentsemanticembeddings. Differentsemanticembeddingscanyieldresponse\\\\ndocuments containing the same company name. While the semantic responses’ contents\\\\nare considered similar to the query document and each other, the visual responses are more\\\\ndissimilar from the query document and each other.\\', \\'6 Conclusion 70\\\\nTo answer RQ2, the content and visual appearance of the response documents of different\\\\nembedding techniques is similar, but the actual documents in the response sets differ.\\\\nRQ3poses the question of how the results of the system are presented to experts. The\\\\nidea of the representation is to enable users to explore the data corpus. They should be\\\\nable to query for terms, to find similar documents in the text corpus and to derive the\\\\ninherent topics of the documents. The proof of concept is the implementation of a web\\\\ninterface which offers these services. The web interface is introduced in Section 4.2.\\\\nRQ4addresses the evaluation of the performance of the system. In this work, the sys-\\\\ntem is evaluated with respect to multiple parameters. Section 5.1 discusses the time to\\\\ncompute the different embeddings in Figure 5.1. In Section 5.3, some embedding mod-\\\\nels are evaluated with respect to their time consumption for different configurations (cf.\\\\nFigure 5.3).\\\\nAnother way to evaluate the performance of the system is to compare the results of different\\\\nmodels via the intersection of their response sets (cf. Section 5.5). These intersections can\\\\nbe visualized with Venn diagrams (Figure 5.12) and heatmaps (Figure 5.13). To ensure\\\\nthe results are not random, the statistical properties of the response sets are calculated\\\\nand presented in Table 5.2.\\\\nAdditional findings which were obtained in the course of working on this thesis are\\\\npresented in the following.\\\\nThe InferSent and the TF-IDF model produce embeddings of large dimensionalities. Since\\\\nchanging the dimensionality would require retraining the models or risking the loss of\\\\nquality, the dimensionality is not changed in this work.\\\\nThe database Elasticsearch was chosen because it offers built-in functionalities, such as\\\\nkNN and fuzzy text search. It is well-suited for flexible data since it is possible to insert\\\\nincomplete documents into the database. However, the maximum dimensionality of the\\\\nembeddings is limited to 2048. If the task at hand requires higher dimensional embed-\\\\ndings, such as the ones produced by the InferSent model, another database may be more\\\\nsuitable.\\', \\'6 Conclusion 71\\\\n6.2 Contribution\\\\nIn the context of this thesis, several ML techniques to derive semantic and visual informa-\\\\ntion from unstructured data are explored.\\\\nIn order to find relevant documents in a text corpus, the corpus is made searchable. This\\\\nis achieved by constructing a pipeline that preprocesses the documents and stores them\\\\nin a database in an offline fashion. The local Elasticsearch database stores 2048 randomly\\\\nchosen documents from the whole dataset, whereas the database on the IES server stores\\\\naround 497504incomplete documents. Owing to Elasticsearch’s implementation, (fuzzy)\\\\ntext queries and kNN search queries can be conducted with minimal latency (cf. Subsec-\\\\ntion 3.7.1).\\\\nConcerning RQ1, the visual embedding method Eigendocs is implemented. It is an adap-\\\\ntionoftheprevalentEigenfacesapproachtothetaskoffindingsimilardocuments. Theidea\\\\nof projecting items into a lower dimensional space is kept. The preprocessing is extended\\\\nby placing the document images onto a white canvas as described in Subsection 4.1.2.\\\\nThe semantic embedding methods TF-IDF, Doc2Vec, InferSent, USE and SBERT are\\\\nexplored. The configurations of the models are altered to reduce their runtime. Since\\\\nthe dimensionalities of TF-IDF and InferSent embeddings are too big to be stored in an\\\\nElasticsearch database, the dimensionality of the embeddings is reduced by the encoder of\\\\nan AE (cf. Section 4.1.3).\\\\nIn the matter of RQ3, a web interface is implemented, which provides the possibility to\\\\nconduct text queries and allows to examine a document of interest in more detail (cf.\\\\nSection 4.2): The detail component not only contains a PDF viewer, word clouds of the\\\\nmost frequent words in the document or query response, but also an option to display the\\\\nfile names of the response documents for different embeddings. The tool implemented in\\\\nthis work is not designed for a productive environment since the focus is on the comparison\\\\nof different models rather than usability.\\\\nSince the dataset is not labeled, the evaluation of the results is not trivial. Therefore, with\\\\nregard to RQ2 and RQ4 multiple evaluation methods are implemented (cf. Section 5.5).\\\\nThe first method is a Venn diagram that depicts the intersection of the query responses\\\\nof the power set of different embedding models. The second method is a heatmap that\\\\nillustrates the average portion of shared response documents between different embedding\\\\nmodels. Moreover, the mean and standard deviation of the portion of shared response\\\\ndocuments are calculated to further investigate the distribution of the results obtained\\\\nabove. Furthermore, the time consumption of the computation of embeddings for different\\\\nmodels is evaluated (cf. Section 5.1 & Section 5.3). Lastly, in Subsection 3.4.1, the tool is\\\\ncompared to a baseline topic analysis approach called Top2Vec.\\', \\'\\', \\'7 Outlook 73\\\\n7 Outlook\\\\nWhen investigating both semantic and visual embedding methods, differences between the\\\\nmodels became evident. Overall, the textual embedding methods produced more meaning-\\\\nful responses than the visual embedding methods. However, this is not surprising since the\\\\ntextual embedding methods prioritize documents containing equal or semantically similar\\\\nterms and thus, return documents of similar content or originating from the same company\\\\nas the query document. Visual embedding methods, on the other hand, return visually\\\\nsimilar documents.\\\\nIt is complicated to compare the responses of semantic and visual embedding methods since\\\\nthey operate on fundamentally different data. A more thorough evaluation could include\\\\na survey. A selection of the results of this work is incorporated into the first approach\\\\nto constructing a survey. 13 people with different academic backgrounds have partaken\\\\nin the survey. A sample question and an illustrative result from the survey are displayed\\\\nin Figure 7.1. However, constructing a survey is complicated since semantic similarities\\\\nshould be evaluated on a textual level, i.e. content, which is difficult for non-experts and\\\\nnot natural since humans are prone to assess similarities by visual inspection. Moreover,\\\\nidentification of the target audience is difficult since the target audience of the tool could\\\\nbe expanded to be more general than the tax office.\\\\n(a) A question of the survey.\\\\n (b) Selection of results of survey.\\\\nFigure 7.1: A first survey approach from [30].\\', \\'7 Outlook 74\\\\nSimilar to Pennington et al.’s work, in this thesis, for many models used, any unspeci-\\\\nfied parameters are set to their default values, assuming that they are close to optimal\\\\nacknowledging that this simplification should be revised in a more thorough analysis.\\\\nThe TF-IDF approach performs rather poorly on unusual query documents. There are\\\\nmultiple factors that could have contributed to this result. Firstly, the vocabulary is dras-\\\\ntically reduced to satisfy the database’s constraints concerning dense vector dimensionality.\\\\nThus, TF-IDF may either be unsuitable for the task of finding similar documents when\\\\nthe vocabulary size is restricted or further research is required to find more suitable means\\\\nto compress the embedding before inserting it into the database. Secondly, the evaluation\\\\nof the different preprocessors of TF-IDF is carried out on small datasets consisting of 195\\\\nand 2048 documents. This dataset may not be representative of the whole corpus.\\\\nIn this work, the precomputed GloVe embeddings are replaced by a custom Word2Vec\\\\nmodel. However, Pennington et al. state that GloVe outperforms Word2Vec on the same\\\\ncorpus, vocabulary and window size in terms of quality [55]. Hence, the quality of InferSent\\\\nmight have deteriorated due to the replacement of GloVe by Word2Vec.\\\\nWhenpreprocessingthedocumentimagesintheEigendocsapproach, theimagesareplaced\\\\non a white canvas assuming its dimensions are bigger or equal to all other documents in the\\\\ncorpus. Since this assumption was not true, the images selected to find the dimensionalities\\\\nof the canvas were not representative. Future work should include a more thorough analysis\\\\nof the maximal image sizes in the corpus.\\\\nThe parameter selection for PCA is not representative of the whole dataset, due to the\\\\nfact that the dataset used for calculating the reconstruction error is too small. Moreover,\\\\nthe resulting plot is not optimal for conducting the “elbow method”, since no significant\\\\nchange in the slope is evident.\\\\nDifferent AE architectures are experimentally evaluated on a selection of 195 documents.\\\\nSince the dataset is too small and not drawn randomly from the whole data corpus the re-\\\\nsults are not representative. Thus, future work should include a more elaborate evaluation\\\\nof different AE architectures on a bigger document corpus.\\\\nThe comparison of the different embedding methods in terms of query response similarity\\\\nwas carried out on the data which was stored in the database. For future work, the\\\\ncomparison should be carried out on a separate dataset to evaluate the performance of the\\\\nmodels on unseen data.\\\\nThe evaluation of the similarity between query results of different models so far has not\\\\nconsidered the individual weights for respective query responses because it was difficult\\\\nto find means to interpret and visualize semantic meaningful weight relationships. Hence,\\\\nfuture work could include the weights of the query responses in the evaluation.\\\\nMoreover, the similarity of the query documents is not considered in the evaluation. To\\\\nfurther improve the evaluation, the number of occurrences of query documents in the\\', \\'7 Outlook 75\\\\nresponse documents of other queries could be examined. Another approach to evaluation\\\\ncould be to assess the quality of the images which were returned by multiple models.\\\\nPossibly, one could create a hypothesis about whether better responses correlate with the\\\\nnumber of models that returned them.\\\\nThe elastic stack offers a wide range of tools, for instance, Kibana that can be used to\\\\nmanage models and to create ingest pipelines to embed new documents. If models are\\\\nmanaged by Kibana, the models no longer have to be managed by the user and thus, the\\\\nsystem would most likely be more user-friendly and less prone to errors.\\\\nAnother issue is the fact that the database contains neither all embeddings nor all doc-\\\\numents. The Bahamas leak contains 38 GB of data. Even though multiprocessing using\\\\nPool is used to split the workload across up to 100 processes, the embedding process is not\\\\nfinished after several days. Hence, more advanced coding techniques have to be applied to\\\\nspeed up the embedding process.\\\\nThe domain of financial fraud and tax evasion is very interesting. Thus, future work\\\\ncould include the development of a working system for the tax office based on the system\\\\nimplemented in this thesis. The techniques explored in this work could be used to find\\\\nsimilar documents to a query document and thus, facilitate initial exploration of a large\\\\ndata corpus.\\', \\'Bibliography xi\\\\nBibliography\\\\n[1] Slurm: Quickstartuserguide. URL https://slurm.schedmd.com/quickstart.html .\\\\n[Accessed 16.09.2023].\\\\n[2] K.P. Agrawal, Sanjay Garg, Shashikant Sharma, and Pinkal Patel. Development and\\\\nvalidation of optics based spatio-temporal clustering technique. Information Sciences ,\\\\n369:388–401, 2016.\\\\n[3] Rubayyi Alghamdi and Khalid Alfalqi. A survey of topic modeling in text mining.\\\\nInternational Journal of Advanced Computer Science and Applications , 6:147–153,\\\\n2015.\\\\n[4] Dimo Angelov. Top2vec: Distributed representations of topics. arXiv:2008.09470 ,\\\\n2020.\\\\n[5] Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. Optics:\\\\nOrdering points to identify the clustering structure. SIGMOD Rec. , 28:49–60, 1999.\\\\n[6] Farzana Anowar, Samira Sadaoui, and Bassant Selim. Conceptual and empirical\\\\ncomparison of dimensionality reduction algorithms (pca, kpca, lda, mds, svd, lle,\\\\nisomap, le, ica, t-sne). Computer Science Review , 40(100370):1–13, 2021.\\\\n[7] Fankar Armash Aslam, Hawa Nabeel Mohammed, Jummal Musab Mohd. Munir, and\\\\nMurade Aaraf Gulamgaus. Efficient way of web development using python and flask.\\\\nInternational Journal of Advanced Research in Computer Science , 6:54–57, 2015.\\\\n[8] Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with\\\\nPython. O’Reilly Media, Sebastopol, CA, USA, 1st edition, 2009.\\\\n[9] Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John,\\\\nNoah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung,\\\\nBrian Strope, and Ray Kurzweil. Universal sentence encoder. arXiv:1803.11175 , pages\\\\n1–7, 2018.\\\\n[10] Allison Chaney and David Blei. Visualizing topic models. International AAAI Con-\\\\nference on Web and Social Media , 6:419–422, 2021.\\\\n[11] Delphine Charlet and Géraldine Damnati. Simbow at semeval-2017 task 3: Soft-cosine\\\\nsemantic similarity between questions for community question answering. Orange\\\\nLabs, pages 315–319, 2017.\\', \\'Bibliography xii\\\\n[12] Qiuxing Chen, Lixiu Yao, and Jie Yang. Short text classification based on lda\\\\ntopic model. In International Conference on Audio, Language and Image Process-\\\\ning (ICALIP) , pages 749–753, 2016.\\\\n[13] Rob Churchill and Lisa Singh. The evolution of topic modeling. ACM Comput. Surv. ,\\\\n54:1–35, 2022.\\\\n[14] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes.\\\\nSupervised learning of universal sentence representations from natural language infer-\\\\nence data. arXiv:1705.02364 , 2018.\\\\n[15] Matt Copperwaite and Charles Leifer. Learning Flask Framework . Packt Publishing,\\\\n2015.\\\\n[16] Laura Dayton, Dante Rousseve, Neil Sehgal, and Sindura Sriram. Final project report:\\\\nMethods of facial recognition. CSCI, 2020.\\\\n[17] Z. Deng, Y. Hu, M. Zhu, and et al. A scalable and fast optics for clustering trajectory\\\\nbig data. Cluster Computing , 18:549––562, 2014.\\\\n[18] download-infersent. Infersent. URL https://github.com/facebookresearch/\\\\nInferSent . [Accessed 14.11.2023].\\\\n[19] Elasticsearch-guide. Elasticsearch guide. URL https://www.elastic.co/guide/en/\\\\nelasticsearch/reference/current/index.html . [Accessed 15.09.2023].\\\\n[20] Elasticsearch-kNN-embedding. How to deploy a text embedding model and use it\\\\nfor semantic search. URL https://www.elastic.co/guide/en/machine-learning/\\\\n8.10/ml-nlp-text-emb-vector-search-example.html . [Accessed 15.09.2023].\\\\n[21] Fabio. Deep averaging network.ipynb. URL https://github.com/f0bs/\\\\nMachine_Learning/blob/master/Deep%20Averaging%20Network.ipynb4 . [Accessed\\\\n04.10.2023].\\\\n[22] Daniel Gaspar and Jack Stouffer. Mastering Flask Web Development: Build Enter-\\\\nprise-Grade, Scalable Python Web Applications , volume 2. Packt Publishing, 2018.\\\\n[23] gensim-doc2vec-init. gensim.models.doc2vec. URL https://tedboy.github.io/\\\\nnlps/generated/generated/gensim.models.Doc2Vec.__init__.html . [Accessed\\\\n01.10.2023].\\\\n[24] gensim-word2vec-init. Word2vec embeddings. URL https://radimrehurek.com/\\\\ngensim/models/word2vec.html#gensim.models.word2vec.Word2Vec . [Accessed\\\\n01.10.2023].\\\\n[25] Miguel Grinberg. Flask Web Development: Developing Web Applications with Python .\\\\nO’Reilly Media, Inc., 2018.\\', \\'Bibliography xiii\\\\n[26] Maarten Grootendorst. Bertopic: Neural topic modeling with a class-based tf-idf\\\\nprocedure. arXiv:2203.05794 , 2022.\\\\n[27] Machine Learning Group. Credit card fraud detection, 2017. URL https://www.\\\\nkaggle.com/datasets/mlg-ulb/creditcardfraud . [Accessed 18.11.2023].\\\\n[28] Stanford NLP Group. The stanford natural language inference (snli) corpus. URL\\\\nhttps://nlp.stanford.edu/projects/snli/ . [Accessed 18.12.2023].\\\\n[29] Christian M. Gruhl. Novelty Detection for Multivariate Data Streams with Probalistic\\\\nModels. Kassel university press, 2022.\\\\n[30] Klara M. Gutekunst. Empirische bewertung von suchanfragen. URL https://forms.\\\\ngle/EU8UUxnWc7hWBngj8 . [Accessed 20.11.2023].\\\\n[31] Klara M. Gutekunst. Identifying fiscal fraud with anomaly detection techniques. Tech-\\\\nnical report, University of Kassel, 2023. URL https://github.com/KlaraGtknst/\\\\nidentifying-fiscal-fraud .\\\\n[32] Tom Hanika. Artificial intelligence. Technical report, 2023. Lecture script.\\\\n[33] impl-src-ae. Image compression using autoencoders in keras. URL https://blog.\\\\npaperspace.com/autoencoder-image-compression-keras/ . [Accessed 06.11.2023].\\\\n[34] Dan Jurafsky and James H. Martin. Speech and Language Processing , volume 3. 2023.\\\\n[35] Hari Krishna Kanagala and V.V. Jaya Rama Krishnaiah. A comparative study of k-\\\\nmeans, dbscan and optics. In International Conference on Computer Communication\\\\nand Informatics (ICCCI) , pages 1–6, 2016.\\\\n[36] Pooja Kherwa and Poonam Bansal. Topic modeling: A comprehensive review. EAI\\\\nEndorsed Transactions on Scalable Information Systems , 7:1–16, 2019.\\\\n[37] Gunjan Khosla, Navin Rajpal, and Jasvinder Singh. Evaluation of euclidean and man-\\\\nhanttan metrics in content based image retrieval system. In International Conference\\\\non Computing for Sustainable Global Development (INDIACom) , pages 12–18, 2015.\\\\n[38] Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. From word\\\\nembeddings to document distances. PMLR, 37:957–966, 2015.\\\\n[39] Tzu-Hsuan Lin and Jehn-Ruey Jiang. Credit card fraud detection with autoencoder\\\\nand probabilistic random forest. Mathematics , 9:2683–2699, 2021.\\\\n[40] Tzu-Hsuan Lin and Jehn-Ruey Jiang. Credit card fraud detection with autoencoder\\\\nand probabilistic random forest. Mathematics , 9(21), 2021. URL https://www.mdpi.\\\\ncom/2227-7390/9/21/2683 .\\\\n[41] Fredrik Lundh, Jeffrey A. Clark, and Contributors. The image class. URL\\\\nhttps://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.\\\\nImage.convert . [Accessed 14.12.2023].\\', \\'Bibliography xiv\\\\n[42] Yu. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor\\\\nsearch using hierarchical navigable small world graphs. arXiv:1603.09320 , pages 1–13,\\\\n2018.\\\\n[43] Tomas Mikolov and Quoc Le. Distributed representations of sentences and documents.\\\\nJMLR, 32:1–9, 2014.\\\\n[44] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of\\\\nword representations in vector space. arXiv:1301.3781 , pages 1–12, 2013.\\\\n[45] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.\\\\nDistributed representations of words and phrases and their compositionality.\\\\narXiv:1310.4546 , 2013.\\\\n[46] Sumit Misra, Soumyadeep Thakur, Manosij Ghosh, and Sanjoy Kumar Saha. An\\\\nautoencoder based model for detecting fraudulent credit card transaction. Procedia\\\\nComputer Science , 167:254–262, 2020. International Conference on Computational\\\\nIntelligence and Data Science.\\\\n[47] Sumit Misra, Soumyadeep Thakur, Manosij Ghosh, and Sanjoy Kumar Saha. An\\\\nautoencoder based model for detecting fraudulent credit card transaction. Procedia\\\\nComputer Science , pages 254–262, 2020.\\\\n[48] GiuliaMoschini, RégisHoussou, JérômeBovay, andStephanRobert-Nicoud. Anomaly\\\\nand fraud detection in credit card transactions using the arima model. Engineering\\\\nProceedings , 5:56–67, 2021.\\\\n[49] Mauritius Much, Frederik Obermaier, Bastian Obermayer, and Vanessa Wormer.\\\\nSo funktioniert das system bahamas. URL https://www.sueddeutsche.de/\\\\nwirtschaft/bahamas-leaks-so-funktioniert-das-system-bahamas-1.3172913 .\\\\n[Accessed 08.08.2023].\\\\n[50] Mohammad Robihul Mufid, Arif Basofi, M. Udin Harun Al Rasyid, Indhi Farhandika\\\\nRochimansyah, and Abdul rokhim. Design an mvc model using python for flask\\\\nframework development. In International Electronics Symposium (IES) , pages 214–\\\\n219, 2019.\\\\n[51] Andreas Müller. word cloud. URL https://github.com/amueller/word_cloud/\\\\ntree/main . [Accessed 05.10.2023].\\\\n[52] Li-Qiang Niu and Xin-Yu Dai. Topic2vec: Learning distributed representations of\\\\ntopics.International Conference on Asian Language Processing (IALP) , pages 193–\\\\n196, 2015.\\\\n[53] nltk-lemma-wordnet. nltk.corpus.reader.wordnet module. URL https://www.nltk.\\\\norg/api/nltk.corpus.reader.wordnet.html . [Accessed 27.10.2023].\\', \\'Bibliography xv\\\\n[54] Mostofa Ali Patwary, Diana Palsetia, Ankit Agrawal, Wei-keng Liao, Fredrik Manne,\\\\nand Alok Choudhary. Scalable parallel optics data clustering using graph algorith-\\\\nmic techniques. In High Performance Computing, Networking, Storage and Analysis .\\\\nACIM, 2013.\\\\n[55] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global\\\\nvectors for word representation. EMNLP, page 1532–1543, 2014.\\\\n[56] Robert-George Radu, Iulia-Maria Rădulescu, Ciprian-Octavian Truică, Elena-Simona\\\\nApostol, and Mariana Mocanu. Clustering documents using the document to vector\\\\nmodel for dimensionality reduction. AQTR, pages 1–6, 2020.\\\\n[57] Nils Reimers and Iryna Gurevych. sentence-transformers/paraphrase-minilm-l6-v2.\\\\nURL https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-\\\\nv2#sentence-transformersparaphrase-minilm-l6-v2 . [Accessed 04.10.2023].\\\\n[58] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese\\\\nbert-networks. arXiv:1908.10084 , 2019.\\\\n[59] Larry R.MEDSKER and L. C. JAIN. Recurrent neural networks. Design and Appli-\\\\ncations, volume 5. 2001.\\\\n[60] Yusuf Sahin and Ekrem Duman. Detecting credit card fraud by decision trees and\\\\nsupport vector machines. World Congress on Engineering , 2188:442–447, 2012.\\\\n[61] Shyam Seshadri. Angular Up and Running: Learning Angular, Step by Step . O’Reilly\\\\nMedia, Inc., 2018.\\\\n[62] Grigori Sidorov, Alexander Gelbukh, Helena Gómez-Adorno, and David Pinto. Soft\\\\nsimilarity and soft cosine measure: Similarity of features in vector space model. Com-\\\\nputación y Sistemas , 18:491–504, 2014.\\\\n[63] Gerd Stumme and Robert Jäschke. Internet-suchmaschinen. Technical report, 2011.\\\\nLecture script.\\\\n[64] Dodi Sudiana, Mia Rizkinia, and Fahri Alamsyah. Performance evaluation of machine\\\\nlearning classifiers for face recognition. In International Conference on Quality in\\\\nResearch (QIR): International Symposium on Electrical and Computer Engineering ,\\\\npages 71–75, 2021.\\\\n[65] Yichuan Tang and Xuan Choo. Intrinsic divergence for facial recognition. Centre for\\\\nTheoretical Neuroscience , pages 1–17, 2008. Paper.\\\\n[66] tfidf-scikit-learn. Tf–idf term weighting. URL https://scikit-learn.org/\\\\nstable/modules/feature_extraction.html#text-feature-extraction . [Accessed\\\\n29.09.2023].\\\\n[67] M. A. Turk and A. P. Pentland. Face recognition using eigenfaces. Computer Society\\\\nConference on Computer Vision and Pattern Recognition , pages 586–591, 1991.\\', \\'Bibliography xvi\\\\n[68] UniversalSentEnc-dev. universal-sentence-encoder. URL https://tfhub.dev/\\\\ngoogle/universal-sentence-encoder/4 . [Accessed 04.10.2023].\\\\n[69] Ike Vayansky and Sathish A.P. Kumar. A review of topic modeling methods. Infor-\\\\nmation Systems , 94(101582):1–15, 2020.\\\\n[70] A. Voit, A. Stankus, S. Magomedov, and I. Ivanova. Big data processing for full-text\\\\nsearch and visualization with elasticsearch. IJACSA, 8:1–8, 2017.\\\\n[71] ChangWangandSridharMahadevan. Multiscalemanifoldlearning. AAAI Conference\\\\non Artificial Intelligence , 27:912–918, 2013.\\\\n[72] Ziqiang Wang and Xu Qian. Text categorization based on lda and svm. In Inter-\\\\nnational Conference on Computer Science and Software Engineering , pages 674–677,\\\\n2008.\\\\n[73] Andy B. Yoo, Morris A. Jette, and Mark Grondona. Slurm: Simple linux utility\\\\nfor resource management. In Job Scheduling Strategies for Parallel Processing , pages\\\\n44–60, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.\\\\n[74] V. Zamfir, M. Carabas, C. Carabas, and N. Tapus. Systems monitoring and big data\\\\nanalysis using the elasticsearch system. International Conference on Control Systems\\\\nand Computer Science , pages 188–193, 2019.\\\\n[75] Vladimir Zaslavsky and Anna Strizhak. Credit card fraud detection using self-\\\\norganizing maps. Information and Security , 18:48–63, 2006.\\\\n[76] Jun Zhang, Yong Yan, and M. Lades. Face recognition: eigenface, elastic matching,\\\\nand neural nets. 85(9):1423–1435, 1997. doi: 10.1109/5.628712.\\\\n[77] Wen Zhang, Taketoshi Yoshida, and Xijin Tang. Tfidf, lsi and multi-word in infor-\\\\nmation retrieval and text categorization. IEEE International Conference on Systems,\\\\nMan and Cybernetics , pages 108–113, 2008.\\\\n[78] Radim Řehůřek. Doc2vec paragraph embeddings, 2022. URL https://\\\\nradimrehurek.com/gensim/models/doc2vec.html . [Accessed 01.10.2023].\\', \\'Eidesstattliche Erklärung xvii\\\\nEidesstattliche Erklärung\\\\nHiermit erkläre ich, Klara Maximiliane Gutekunst, dass ich die vorliegende Arbeit mit dem\\\\nTitel “Identification of Key Information with Topic Analysis on Large Unstructured Text\\\\nData” selbstständig und nur mit den nach der Prüfungsordnung der Universität Kassel\\\\nzulässigen Hilfsmitteln angefertigt habe. Die verwendete Literatur ist im Literaturverze-\\\\nichnis angegeben. Wörtlich oder sinngemäß übernommene Inhalte habe ich als solche\\\\nkenntlich gemacht.\\\\nKassel, 21. November 2023\\\\nKlara Maximiliane Gutekunst\\']',\n",
       " \"[' ', '', '', '']\",\n",
       " '[\\'Prof. Dr. Gerd Stumme  \\\\nDr. Robert Jäschke  \\\\nM. Sc. Christoph Scholz  Internet -Suchmaschinen  \\\\nSommersemester 2011 \\\\n\\', \\'2 Wir wollen wissen …  \\\\n•Wie funktionieren Google und MSN Search?  \\\\n•Wie sammeln sie Informationen?  \\\\n•Welche Tricks benutzen sie?  \\\\n•Mögliche Nutzung außerhalb des Webs?  \\\\n•Wie kann man diese Ansätze verbessern?  \\\\n•Verstehen von natürlicher Sprache?  \\\\n•Benutzerinteraktion?  \\\\n•Was kann man tun, um diese Ansätze zu beschleunigen?  \\\\n•Schnellere Computer? Caching? Kompression?  \\\\n•Wie entscheiden wir, ob die Ansätze funktionieren oder nicht?  \\\\n•Im allgemeinen für alle Anfragen, oder für spezielle Anfragen?  \\\\n•Für spezielle Dokumentensammlungen oder das Web?  \\\\n•Maße?  \\\\n•Was kann man noch mit diesen Ansätzen machen?  \\\\n•Andere Medien?  \\\\n•Andere Aufgaben?  \\', \\'3 Übersicht  \\\\n•Einführung  \\\\n•Boolesches und Vektorraum -Retrieval -Modelle  \\\\n•Elementares Tokenizing, Indexing, und die Implementierung von  \\\\nvektorraumbasiertem  Retrieval   \\\\n•Performanz -Bewertung von Retrieval -Systemen  \\\\n•Anfrage -Operationen (Relevance Feedback, Anfrageerweiterung)  \\\\n•Anfragesprachen und –paradigmen  \\\\n•Strukturelle Anfragen  \\\\n•Texteigenschaften  \\\\n•Web-Suche: Einführung, Crawling, Interfaces, Link -Analyse  \\\\n•Empfehlungssysteme  \\\\n•Text-Clustering & -Klassifikation  \\\\n•Informations -Extraktion  \\\\n•Aktuelle Suchmaschinen, Trends, Suche im Web 2.0  \\', \\'Vorlesung  \\\\n•  Beginn: 13. April 2011  \\\\n•  Mittwoch 10:15 -11:45 Uhr, Raum 0413  \\\\n \\\\nÜbungen  \\\\n• Beginn: 27. April 2011  \\\\n• Mittwoch 16:00 -17:30 Uhr, Raum -1607  \\\\n• wird als Präsenzübung abgehalten (s. nächste Folie)  \\\\n \\\\nPraktikum  \\\\n• optional, vorlesungsbegleitend  \\\\n• Einführung am 20. April 2011, 16:00, Raum -1607  \\\\n \\\\nUnterlagen  \\\\n• siehe Literatur  \\\\n \\\\nPrüfung  \\\\n• Die Prüfung wird je nach Teilnehmerzahl mündlich oder schriftlich abgehalten.  \\\\n \\\\n Organisatorisches  \\', \\'Organisatorisches  \\\\nPräsenzübung  bedeutet  \\\\n• selbständiges Bearbeiten  des Übungsblattes in Kleingruppen à 3 -4 Personen  \\\\n unter Betreuung des Assistenten  \\\\n• kein prinzipielles Wiederholen des Vorlesungsstoffs  \\\\n• kein Vorrechnen der Musterlösung etc. (Diese wird später zur Verfügung gestellt.)  \\\\n• Nötig dafür:  \\\\n• selbständige Vorlesungsnachbereitung  vor der Übung  \\\\n• Mitbringen des Skriptes  \\\\n• eigene Aktivität entfalten  \\', \\'Organisatorisches  \\\\nWarum dieses Übungskonzept ? \\\\n•aktives Erarbeiten des Vorlesungsstoffes bringt mehr  \\\\n•Zusammenhänge im Stoff erkennen  \\\\n•strukturiertes Denken und selbständiges Arbeiten lernen  \\\\n•Teamarbeit lernen  \\\\n•Erklären lernen (als Tutor und als Teilnehmer)  \\\\n•Klausurtraining ; -) \\\\n•Ihr Studium der ...  haben Sie abgeschlossen. Zu Ihren persönlichen Stärken zählen \\\\nSie Eigeninitiave , Kommunikations - und Kooperationsbereitschaft, Teamarbeit.                                      \\\\n(Typischer Anzeigentext)  \\', \\'Praktikum – Implementieren einer Suchmaschine  \\\\n•Umsetzung der in der Vorlesung Internet -Suchmaschinen vermittelten Konzepte in die \\\\nPraxis.  \\\\n•Implementieren einer eigenen Suchmaschine in sechs Teilaufgaben.  \\\\n•Umfang 3 CPs.  \\\\n•Einführung am 20. April 2011, 16:00, Raum -1607  \\\\n•Abgabe der ersten Praxisaufgabe bis 3.5.10 , 14:00 per Moodle , danach jeden zweiten \\\\nDienstag, 14:00.  \\\\n•Ausgabe der nächsten Aufgabe im Laufe des selben Tages  \\\\n•Präsentation des Ergebnisses innerhalb einer Woche nach Abgabe, Termin individuell \\\\nnach Absprache  \\\\n•Das Praktikum entspricht den Praxisaufgaben bis 2010 und kann daher bei erfolgreicher \\\\nTeilnahme an den Praxisaufgaben nicht erneut eingebracht werden.  \\', \\'Praktikum – Benotung  \\\\n•Jede Aufgabe wird einzeln bewertet, die Endnote wird gemittelt.  \\\\n•Zum Bestehen darf maximal eine Aufgabe mit „ n.b.“ bewertet sein.  \\\\n•Mindestanforderungen:  \\\\n• Der Code muss kompilierbar sein.  \\\\n• Die vorgegebenen JUnit -Tests müssen laufen.  \\\\n• Der Code muss kommentiert sein.  \\\\n• Während der Präsentation muss der Code erklärt werden können.   \\\\n• Die vorgegebenen Interfaces dürfen nicht verändert werden.  \\\\n \\', \\'Organisatorisches  \\\\nSprechstunden nach Absprache:  \\\\n \\\\nGerd Stumme:   stumme@cs.uni -kassel.de         0561/804 -6251  \\\\nRobert Jäschke :   jaeschke@cs.uni -kassel.de    0561/804 -6253  \\\\nChristoph Scholz:   scholz@cs.uni -kassel.de    0561/804 -6266  \\\\n   \\\\nFG Wissensverarbeitung, FB Mathematik/Informatik  \\\\nRaum 0440, Wilhelmshöher Allee 73  \\\\n \\\\nInformationen im Internet:    \\\\n•  http://www.kde.cs.uni -kassel.de  \\\\n•  https://moodle.uni -kassel.de/moodle/course/view.php?id=4367  \\\\n \\\\nHier ist u.a. folgendes zu finden:  \\\\n• aktuelle Ankündigungen  \\\\n• Folienkopien  \\\\n• Übungsblätter  \\\\n• Literaturempfehlungen  \\\\n• Termine    \\', \\'10 Literatur  \\\\nWesentliche Quellen  \\\\n \\\\n•Ricardo Baeza -Yates & Berthier Ribeiro -Neto. Modern Information Retrieval, New York, NY: ACM Press; 1999; 513 pp. \\\\n(ISBN: 0 -201-39829 -X.)  \\\\n \\\\n•Ian H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and \\\\nImages, Morgan Kaufmann Publishing, San Francisco, ISBN 1 -55860 -570-3. \\\\n \\\\n•Reginald Ferber. Information Retrieval. Suchmodelle und Data -Mining -Verfahren für Textsammlungen und das Web. \\\\ndpunkt -Verl.: Heidelberg 2003.  \\\\n \\\\n•Rijsbergen. C.J van, Information retrieval, http://www.dcs.gla.ac.uk/Keith/Preface.html  \\\\n \\\\n•Konzepte des Information Retrieval, http://irgroup.cs.uni -magdeburg.de/dt/vorlesungen/WS03 -04_KIR.htm  \\\\n \\\\n•Intelligent Information Retrieval and Web Search, http://www.cs.utexas.edu/users/mooney/ir -course/  \\', \\'11 Literatur  \\\\nWeiteres Material  \\\\n \\\\n•R.R. Korfhage. Information storage and retrieval. Wiley: New York, 1997  \\\\n•G. Salton / M.J. McGill. Information Retrieval - Grundlegendes für Informationswissenschaftler. McGraw -Hill: Hamburg \\\\netc., 1987  \\\\n• Machine Learning, Tom Mitchell, McGraw Hill, 1997.  \\\\n \\', \\'Internet -Suchmaschinen, Kassel, SS 2011 1 Einführung  \\\\nEinführung  \\\\nInternet -Suchmaschinen  \\', \\'2 Was ist Information Retrieval (IR)?  \\\\nInformation -Retrieval (IR)   (Informationswiedergewinnung, \\\\ngelegentlich Informationsbeschaffung)  \\\\n \\\\n \\\\n   ist eine Forschungsrichtung, die sich mit \\\\ncomputergestützter, inhaltsorientierter und unscharfer \\\\nSuche in unstrukturierten Datenmengen beschäftigt.  \\\\n \\\\n \\\\nhttp://de.wikipedia.org/wiki/Information -Retrieval  \\\\nhttp://www.ib.hu -berlin.de/~is/web -lehrsammlung/Begriffe/Retrieval.htm  \\\\n \\\\n Einführung  \\', \\'Internet -Suchmaschinen, Kassel, SS 2010  3 Einführung  \\\\n\\', \\'Internet -Suchmaschinen, Kassel, SS 2010  4 Einführung  \\\\n\\', \\'5 Was ist Information Retrieval (IR)?  \\\\n•Indexierung  und Retrieval ( Finden , Wiederfinden ) von Texten  \\\\n•Suchen  nach Seiten  im World Wide Web ist die aktuelle  “killer \\\\napp”.  \\\\n•Beschäftigt  sich in erster  Linie mit dem Finden  der relevanten  \\\\nDokumente  gemäß  einer  gegebenen  Frage  (Query).  \\\\n•Beschäftigt  sich außerdem  mit dem effizienten  Finden  von \\\\nDokumenten  in großen  Dokumentensammlungen . \\\\n \\\\n \\\\n Einführung  \\', \\'6 Information Retrieval – Data Retrieval  Einführung  \\\\n  Data Retrieval  Information Retrieval  \\\\nMatching  Exact match  Partial match, best match  \\\\nInference  Deduction  Induction  \\\\nModel  Deterministic  Probabilistic  \\\\nClassification  Monothetic  Polythetic  \\\\nQuery language  Artificial  Natural  \\\\nQuery specification  Complete  Incomplete  \\\\nItems wanted  Matching  Relevant  \\\\nError response  Sensitive  Insensitive  \\\\nC.J. van Rijsbergen, 1979 S.1  \\', \\'7 Eine typische IR -Aufgabe:  \\\\nGegeben:  \\\\n\\\\uf06eTextkorpus mit natürlichsprachlichen Textdokumenten.  \\\\n\\\\uf06eEine Benutzeranfrage in Form eines Textstrings.  \\\\n \\\\nFinde:  \\\\n\\\\uf06eEine geordnete Menge an Dokumenten, die relevant zur \\\\nAnfrage sind.  \\\\n Einführung  \\', \\'8 IR System  \\\\nIR \\\\nSystem  Query \\\\nString  Document  \\\\ncorpus  \\\\nRanked  \\\\nDocuments  1. Doc1  \\\\n2. Doc2  \\\\n3. Doc3  \\\\n    . \\\\n    . \\\\n Einführung  \\', \\'9 Drei Phasen des IR   \\\\n•Fragestellung (Information Need)  \\\\n•Bestimmung einer Antwort (Response)  \\\\n•Bewertung der Antwort (Evaluation)  \\\\n \\\\n \\\\n\\\\uf0e0 Interaktiver Prozess  \\\\n Einführung  \\', \\'10 Frage stellen   \\\\n•Fragesteller = “user”  \\\\n•Befindet sich in einem bestimmten Umfeld / Bewusstsein - ein \\\\nkognitiver Zustand  \\\\n•Ist sich seiner Wissenslücken bewusst  \\\\n•Kann diese Lücken evtl. nicht genau bestimmen  \\\\n•Paradox des FOA ( Finding Out About ): \\\\n•Wenn der Nutzer in der Lage ist, die richtige Frage zu stellen, \\\\nbesteht häufig keine Notwendigkeit mehr für diese Frage.  \\\\n•“The need to describe that which you do not know in order \\\\nto find it.” Roland Hjerppe  \\\\n•Anfrage  \\\\n•Ausdruck dieses schlecht definierten Zustandes  Einführung  \\', \\'11 Frage beantworten   \\\\n•Wenn der Antwortende ein Mensch ist:  \\\\n•Ist er in der Lage, die schlecht gestellte Frage in eine bessere \\\\numzuformulieren?  \\\\n•Kennt der Antwortende die Antwort?  \\\\n•Kann er diese Antwort in Worten ausdrücken?  \\\\n•Wird der Anfrager diese Antwort verstehen?  \\\\n•Haben beide das notwendige Hintergrundwissen?  \\\\n \\\\n•Wenn der Antwortende ein Computersystem ist...  \\\\n \\\\n Einführung  \\', \\'12 Bewertung der Antwort   \\\\n•Wie gut wird die Frage beantwortet?  \\\\n•Wurde die Antwort vollständig beantwortet oder nur \\\\nteilweise?  \\\\n•Wurden Hintergrundinformationen zur Verfügung gestellt?  \\\\n•Wurden Hinweise für weitergehende Untersuchungen \\\\ngegeben?  \\\\n \\\\n•Wie relevant  ist die Antwort für den Frager?  Einführung  \\', \\'13 Relevanz  \\\\nRelevanz ist eine subjektive Beurteilung und kann folgendes \\\\neinschließen:  \\\\n•Richtiges Thema  \\\\n•Aus der richtigen Zeit (zeitgemäß)  \\\\n•Aus vertrauenswürdiger Quelle (verlässlich)  \\\\n•Antwort berücksichtigt die Ziele des Nutzers und die \\\\nbeabsichtigte Nutzung der Information ( information need ) Einführung  \\', \\'14 Relevanz (Forts.)  \\\\nIn welcher Art kann ein Dokument relevant sein für eine  \\\\nAnfrage?  \\\\n•Präzise Antwort auf eine präzise Frage.  \\\\n•Wer ist in Meiers Grab begraben? Meier . \\\\n•Frage wird teilweise beantwortet.  \\\\n•Wo ist Söhrewald? In der Nähe von Kassel . \\\\n•Weitere Informationsquellen vorschlagen.  \\\\n•Was ist Lymphedema? Schau in diesem medizinischen \\\\nLexikon nach.  \\\\n•Hintergrundinformationen geben.  \\\\n•Den Fragesteller an relevante, ihm bekannte Informationen \\\\nerinnern.  Einführung  \\', \\'15 Relevanz bei der Stichwort -Suche [Keyword Search]  \\\\n•Die einfachste Form der Relevanz ist das wortwörtliche \\\\nVorkommen des Anfragestrings im Text.  \\\\n \\\\n•Eine weniger restriktive Idee ist, dass die einzelnen  Wörter* \\\\naus der Anfrage häufig im Textdokument vorkommen müssen \\\\n(bag of words ). \\\\n Einführung  \\\\n* Siehe http://www.spiegel.de/kultur/zwiebelfisch/0,1518,307445,00.html   zum Unterschied \\\\nzwischen Worten und Wörtern!  \\\\n \\', \\'16 Probleme mit Stichwörtern  \\\\nMan findet relevante Dokumente nicht bei synonymen \\\\nTermen.  \\\\n\\\\uf06e“restaurant” vs. “café”  \\\\n\\\\uf06e“Auto” vs. “PKW”  \\\\nMan erhält irrelevante Dokumente durch mehrdeutige \\\\nTerme (Homonyme).  \\\\n\\\\uf06e“Bank” (Finanzinstitut vs. Sitzgelegenheit)  \\\\n\\\\uf06e“Apple” (company vs. fruit)  \\\\n\\\\uf06e“bit” (unit of data vs. act of eating)  \\\\n Einführung  \\', \\'17 Intelligentes IR  \\\\n•Bedeutung des Wortes wird mit in Erwägung gezogen.  \\\\n \\\\n•Reihenfolge der Wörter in der Anfrage wird beachtet.  \\\\n \\\\n•Anpassung an den Anwender durch direktes oder indirektes \\\\nFeedback.  \\\\n \\\\n•Zuverlässigkeit der Quelle wird beachtet.  \\\\n Einführung  \\', \\'18 IR-System -Architektur  \\\\nText \\\\nDatabase  Database  \\\\nManager  Indexing  \\\\nIndex  Query  \\\\nOperations  \\\\nSearching  \\\\nRanking  Ranked  \\\\nDocs User \\\\nFeedback  Text Operations  User Interface  \\\\nRetrieved  \\\\nDocs User \\\\nNeed  Text \\\\nQuery  Logical View  \\\\nInverted  \\\\n       file Einführung  \\', \\'19 IR-Systemkomponenten  \\\\nText Operations  berechnet die Wörter des Indexes (tokens).  \\\\n\\\\uf06eStopword removal  \\\\n\\\\uf06eStemming  \\\\n \\\\nIndexing  konstruiert einen invertierten Index  aus Wörtern \\\\nmit Zeigern zu den Dokumenten.  \\\\n \\\\nSearching  findet mit Hilfe des invertierten Index Dokumente, \\\\ndie Tokens aus der Anfrage enthalten.  \\\\n \\\\nRanking  gewichtet alle gefundenen Dokumente gemäß einer \\\\nRelevanzmetrik.  \\\\n Einführung  \\', \\'20 IR-Systemkomponenten (Forts.)  \\\\nUser Interface  ist für die Interaktion mit dem Anwender \\\\nverantwortlich:  \\\\n\\\\uf06eAnfrage entgegennehmen und Dokumente präsentieren.  \\\\n\\\\uf06eRelevance feedback.  \\\\n\\\\uf06eVisualisierung der Ergebnisse.  \\\\n \\\\nQuery Operations  verändert die Anfrage zur Verbesserung der \\\\nErgebnisse:  \\\\n\\\\uf06eAnfrageerweiterung (Query expansion) mittels Thesaurus.  \\\\n\\\\uf06eAnfrageanpassung mittels Relevance Feedback.  \\\\n Einführung  \\', \\'21 Anwendung: Web -Suche  \\\\nWeb-Suche ist die Anwendung des IR auf HTML -Dokumente \\\\ndes World Wide Web.  \\\\n \\\\nUnterschiede:  \\\\n\\\\uf06eMan muss die Dokumente für den Korpus im Web \\\\neinsammeln (Crawling)  \\\\n\\\\uf06eAusnutzung der strukturierten Layout -Information in HTML  \\\\n\\\\uf06eUnkontrollierbare Veränderung der Dokumente  \\\\n\\\\uf06eAusnutzung der Linkstruktur  Einführung  \\', \\'22 Web Search System  \\\\nQuery \\\\nString  \\\\nIR \\\\nSystem  \\\\nRanked  \\\\nDocuments  1. Page1  \\\\n2. Page2  \\\\n3. Page3  \\\\n    . \\\\n    . \\\\n Document  \\\\ncorpus  \\\\nWeb \\\\n Spider  Einführung  \\', \\'23 Weitere IR -nahe Aufgaben  \\\\n•Automated document categorization (Kategorisieren)  \\\\n•Automated document clustering (Gruppieren)  \\\\n•Automated Text Summarization (Zusammenfassen)  \\\\n•Question answering (Frage/Antwort)  \\\\n•Information filtering (spam filtering) (Filtern)  \\\\n•Information extraction (Extrahieren)  \\\\n•Information integration (Integrieren)  \\\\n•Recommending information or products (Empfehlen)  \\\\n•Ranking in Web 2.0  \\\\n Einführung  \\', \\'24 Geschichte des WWW und des IR  \\\\nbis 1960:  \\\\n \\\\n•“Informationsexplosion” nach dem Ende des zweiten Weltkrieges \\\\nführt zur Notwendigkeit, diese besser zu organisieren.  \\\\n•1945 Vannevar Bush verfolgt mit der Memex -Maschine ähnliche \\\\nIdeen wie sie im heutigen Web zu finden sind (Assoziation von \\\\nInformationen mit Links).  \\\\n Einführung  \\\\n\\', \\'25 Geschichte des WWW und des IR  \\\\n1960-70er:  \\\\n \\\\n•Initiale Untersuchung von Text -Retrieval -Systemen für “kleine” \\\\nKorpora bestehend aus Zusammenfassungen wissenschaftlicher \\\\nPublikationen sowie Gesetzes - und Geschäftsdokumenten.  \\\\n \\\\n•Die Entwicklung einfacher Boolean - and Vector -Space -Modelle.  \\\\n \\\\n•Prof. Salton und seine Studenten an der Cornell Universität \\\\nwaren die führenden Forscher auf diesem Gebiet.  \\\\n \\\\n•1965 Ted Nelson prägt den Begriff „Hypertext“.  \\\\n Einführung  \\', \\'26 Geschichte des WWW und des IR (Forts.)  \\\\n1980er:  \\\\n \\\\n•Systeme mit großen Dokumentensammlungen entstehen, viele \\\\nlaufen in Unternehmen:  \\\\n \\\\n•Lexis-Nexis  \\\\n•Dialog  \\\\n•MEDLINE  \\\\n Einführung  \\', \\'27 1990er:  \\\\n\\\\uf06eSuche nach “FTP -baren” Dokumenten im Internet  \\\\n\\\\uf0a7Archie  \\\\n\\\\uf0a7WAIS \\\\n\\\\uf06eSuche im World Wide Web  \\\\n\\\\uf0a7Lycos  \\\\n\\\\uf0a7Yahoo  \\\\n\\\\uf0a7Altavista  Einführung  \\\\nGeschichte des WWW und des IR (Forts.)  \\', \\'28 Geschichte des WWW und des IR (Forts.)  \\\\nauch 1990er:  \\\\n\\\\uf06eOrganisierte Wettkämpfe  \\\\n\\\\uf0a7NIST TREC (Text REtrieval Conference)  \\\\n \\\\n\\\\uf06eRecommender -Systeme  \\\\n\\\\uf0a7Amazon  \\\\n \\\\n\\\\uf06eAutomatisiertes Text -Kategorisieren und -Clustern  \\\\n Einführung  \\', \\'29 Geschichte des WWW und des IR (fort.)  \\\\n2000er  \\\\n\\\\uf06eAnalyse der Links für die Web -Suche  \\\\n\\\\uf0a7Google  \\\\n\\\\uf06eAutomatisierte Informationsextraktion  \\\\n\\\\uf0a7Whizbang  \\\\n\\\\uf0a7Burning Glass  \\\\n\\\\uf06eFrage/Antwort (Question Answering)  \\\\n\\\\uf0a7TREC Q/A track  \\\\n Einführung  \\', \\'30 Geschichte des WWW und des IR (fort.)  \\\\nauch 2000er:  \\\\n\\\\uf06eMultimedia -IR \\\\n\\\\uf0a7Image  \\\\n\\\\uf0a7Video  \\\\n\\\\uf0a7Audio und Musik  \\\\n\\\\uf06eMehrsprachiges IR (Cross -Language IR)  \\\\n\\\\uf0a7DARPA Tides (Translingual Information Detection, Extraction \\\\nand Summarization)  \\\\n\\\\uf06eZusammenfassen von Dokumenten  Einführung  \\', \\'31 Verwandte Forschungsgebiete  \\\\n•Datenbanken (Database Management)  \\\\n•Bibliothekswesen (Library and Information Science)  \\\\n•Künstliche Intelligenz (Artificial Intelligence)  \\\\n•Sprachverarbeitung (Natural Language Processing)  \\\\n•Maschinelles Lernen (Machine Learning), Data Mining  Einführung  \\', \\'32 Datenbanken (Database Management)  \\\\n•Fokussiert auf strukturierte Daten, die in relationalen \\\\nTabellen gespeichert sind und nicht auf freien Text.  \\\\n \\\\n•Beschäftigt sich mit der effizienten Abarbeitung von \\\\nwohldefinierten Anfragen in einer formalen Sprache \\\\n(SQL).  \\\\n \\\\n•Klare Semantik für Daten und Anfragen.  \\\\n \\\\n•Aktuell beschäftigt man sich auch mit semi -strukturierten \\\\nDaten wie XML (bringt DB näher zu IR)  Einführung  \\\\n\\\\uf0e0 Datenbanken -Vorlesung im Sommersemester  \\', \\'33 Bibliothekswesen (Library and Information Science)  \\\\n•Fokussiert auf die Mensch -Maschine -Schnittstelle des IR \\\\n(human -computer interaction, user interface, \\\\nvisualization).  \\\\n \\\\n•Beschäftigt sich mit der effektiven Kategorisierung \\\\nmenschlichen Wissens.  \\\\n \\\\n•Beschäftigt sich mit der Analyse des Verhältnisses \\\\nzwischen Personen und Publikationen.  \\\\n \\\\n•Aktuelle Arbeiten im Bereich der Digitalen Bibliotheken \\\\nbringen das BW näher an IR. \\\\n \\\\n Einführung  \\\\n\\\\uf0e0 Siehe www.bibsonomy.org  \\', \\'34 Künstliche Intelligenz (Artificial Intelligence)  \\\\n•Fokussiert auf Methoden zur Akquisition, Repräsentation \\\\nund zum Ableiten von (neuem) Wissen.  \\\\n \\\\n•Formalismen zur Repräsentation von Wissen und Anfragen \\\\nsind: \\\\n\\\\uf06ePrädikatenlogik  \\\\n\\\\uf06eBeschreibungslogiken  \\\\n\\\\uf06eBayesian Networks  \\\\n \\\\n•Aktuelle Arbeiten im Bereich Semantic Web und \\\\nOntologien schaffen einen engeren Bezug zu IR.  \\\\n Einführung  \\\\n\\\\uf0e0 Vorlesung Künstliche Intelligenz im WS  \\', \\'35 Sprachverarbeitung (Natural Language Processing)  \\\\n•Fokussiert auf die syntaktische, semantische und \\\\npragmatische Analyse von natürlichsprachlichem Text  \\\\n \\\\n•Die sytaktische und semantische Analyse könnte eine \\\\nbedeutungsbezogene anstatt einer stichwortbasierten Suche \\\\nermöglichen.  \\\\n Einführung  \\', \\'36 Sprachverarbeitung in Richtung IR:  \\\\n•Methoden zur Wortsinnerkennung von mehrdeutigen Wörtern \\\\nim Kontext ( word sense disambiguation ). \\\\n \\\\n•Methoden zur Identifikation von spezifischen Informationen in \\\\nTexten ( information extraction ). \\\\n \\\\n•Methoden zur Beantwortung von natürlichsprachlichen \\\\nAnfragen auf Dokumentkorpora.  Einführung  \\', \\'37 Maschinelles Lernen (Machine Learning, ML)  \\\\nKDD, Data Mining  \\\\n•Fokussiert auf die Entwicklung von Systemen, die in der \\\\nLage sind, ihre Leistung anhand ihrer Erfahrung zu \\\\nsteigern.  \\\\n \\\\n•Automatische Klassifikation von Beispielen basierend \\\\nauf Lernmethoden, die auf vorklassifizierten \\\\nTrainingsbeispielen basieren ( supervised learning ). \\\\n \\\\n•Automatisierte Methoden zum Gruppieren von nicht -\\\\nklassifizierten Beispielen in sinnvolle Gruppen \\\\n(unsupervised learning ). Einführung  \\\\n\\\\uf0e0 Knowledge -Discovery -Vorlesung im WS  \\', \\'38 ML in Richtung IR:  \\\\nText-Kategorisierung  \\\\n\\\\uf06eAutomatisches Klassifizieren in Hierarchien (Yahoo).  \\\\n\\\\uf06eAdaptive Filter/Recommender.  \\\\n\\\\uf06eAutomatische Spamfilter.  \\\\n \\\\nText-Clustern  \\\\n\\\\uf06eClustern von IR Anfrageergebnissen.  \\\\n\\\\uf06eAutomatisches Ableiten von Hierarchien (Yahoo).  \\\\n \\\\nLernen für Informationsextraktion  \\\\n \\\\nText Mining  \\\\n Einführung  \\', \\'Internet -Suchmaschinen, Kassel, SS 2010  39 Overview  \\\\n• Introduction  \\\\n• What is IR, task, systems (in detail), history  \\\\n• Web search  \\\\n• IR related tasks  \\\\n• IR related research areas  \\\\n• Boolean and Vector -Space Retrieval Models  \\\\n• Retrieval Models (Boolean, Statistical, Vector Space Model)  \\\\n• Weighting, Similarity Measure  \\\\n• Basic Tokenizing, Indexing, and Implementation of Vector -Space Retrieval  \\\\n• Tokenizing, Stopwords, Stemming  \\\\n• Implementation of Sparse Vectors, Inverted Files, IDF computing  \\\\n• Retrieval with an Inverted Index  \\\\n• Analysis of time complexity  \\\\n• Performance Evaluation of Information Retrieval Systems  \\\\n• Gold standard - Precision, Recall, F -Measure, Rank measures  \\\\n• Subjective relevance measures  \\\\n• Trec, Cystic Fibrosis Collection  \\\\n• Query Operations (Relevance Feedback / Query Expansion)  \\\\n• Query Reformulation – Rochio Model, Pseudo Feedback, Thesaurus, Wordnet, statistical Thesaurus  \\\\n• Local vs. global Analysis of the query  \\\\n• Query Languages  \\\\n• Boolean, Natural Language, Phrasal,  Proximity and Structural Queries  \\\\n• Pattern Matching, Levenstein Distance, Regular Expressions  \\\\n• Text Properties and Languages  \\\\n• Zipf’s Law  \\\\n• Meta Data  \\\\n• Web Search: Introduction  \\\\n• WWW history, Challenges for IR, Statistics about the web, web search principle  \\\\n• Web Search: Spidering  \\\\n• Spiders, spider programming in java, link extraction, multi threaded spider, topic directed spider  \\\\n• Web Search: Interfaces  \\\\n• Interface, Clustering  \\\\n• Apache TomCat, Servlet, Session Tracking, Simple Search Servlet  \\\\n• Web Search: Link Analysis  \\\\n• Meta Search Engines, Bibliometrics, Hits, PageRank, Google Ranking,  \\\\n• Recommender Systems  \\\\n• Book recommender, collaborative filtering, content based recommender, combination  \\\\n• experiments movie domain  \\\\n• Active learning  \\\\n• Text Clustering & Classification  \\\\n• Introduction of Clustering and Classification, specific text properties for clustering and classification  \\\\n• Information Extraction  \\\\n• MUC, Simple pattern, Template based Extraction, Filler Extraction,  \\\\n• Learning for IE  \\\\n• Web Extraction (shop bot)  \\\\n• Aktuelle Suchmaschinen, Trends, Suche im Web 2.0  \\', \\'1 Boolesche und Vektorraum -  \\\\nModelle  \\\\nViele Folien in  diesem Abschnitt sind eine deutsche Übersetzung der Folien von  \\\\nRaymond J. Mooney (http://www.cs.utexas.edu/users/mooney/ir -course/).  \\', \\'2 Retrieval -Modelle  \\\\n•Ein Retrieval -Modell spezifiziert die Details \\\\nder \\\\n–Repräsentation von Dokumenten,  \\\\n–Repräsentation von Anfragen,  \\\\n–Retrievalfunktion  \\\\nund legt so die Interpretation von “Relevanz” \\\\nfest. \\\\n•Relevanz kann binär sein oder eine \\\\nReihenfolge angeben ( ranked retrieval ). \\\\n \\', \\'3 Klassen von Retrieval -Modellen  \\\\n•Boolesche Modelle (mengenbasiert)  \\\\n–Erweitertes Boolesches Modell  \\\\n•Vektorraummodelle (vector space) \\\\n(statistisch -algebraischer Ansatz)   \\\\n–Latente Semantische Indexierung  \\\\n•Wahrscheinlichkeitsbasierte Modelle  \\', \\'4 Weitere Modell -Dimensionen  \\\\n•Logische Sicht auf die Dokumente  \\\\n–Indexterme  \\\\n–Volltext  \\\\n–Volltext + Struktur (z.B. Hypertext)  \\\\n•Anwenderaufgabe  \\\\n–Retrieval  \\\\n–Browsing  \\', \\'5 Retrieval -Aufgaben  \\\\n•Ad-hoc-Retrieval : Fester Dokumentenkorpus, \\\\nverschiedene Anfragen.  \\\\n•Filtern : Feste Anfrage, fortlaufender  \\\\nDokumentenstrom.  \\\\n–Anwenderprofil: Ein Modell mit relativ statischen \\\\nPräferenzen.  \\\\n–Binäre Entscheidung: relevant/nicht relevant.  \\\\n•Routing : Das gleiche Modell wie beim Filtern, \\\\njedoch erfolgt eine fortlaufende Bereitstellung von  \\\\nRanglisten und bzgl. mehrerer Filter  \\\\n–meist in Firmen für mehrere Personen in Form von \\\\nProfilen.  \\', \\'6 Allgemeine Vorverarbeitungs -Schritte  \\\\n•Löschen ungewollter Zeichen/Markups  (z.B. HTML Tags, \\\\nInterpunktion, Nummern, etc.).  \\\\n•Auf Tokens anhand von Leerzeichen herunterbrechen \\\\n(Schlüsselwörter)  \\\\n•Wortstämme berechnen, um Wörter auf ihre Grundform \\\\nabzubilden  \\\\n–computational \\\\uf0e0 comput  \\\\n•Stopwörter entfernen (z.B. a, the, it, etc., oder der, die, das).  \\\\n•Typische Phrasen erkennen (möglicherweise durch \\\\nVerwendung eines domänenspezifischen Wörterbuches).  \\\\n•Invertierten Index erstellen  \\\\n(Schlüsselwort \\\\uf0e0 Dokumentenliste, die dies enthält).  \\', \\'7 Boolesches Modell  \\\\n•Ein Dokument wird als eine Menge  von \\\\nSchlüsselwörtern (index terms) repräsentiert.  \\\\n•Anfragen sind boolesche Ausdrücke von \\\\nSchlüsselwörtern, verbunden durch AND, OR und \\\\nNOT.  \\\\n–[[[Rio & Brazil] | [Hilo & Hawaii]] & hotel & !Hilton]  \\\\n•Ausgabe:  Dokument ist relevant oder nicht.  \\\\n  Kein partielles Suchen, kein Ranking.  \\', \\'8 •Ist ein beliebtes Retrieval -Modell, da:  \\\\n–für einfache Anfragen einfach zu verstehen.  \\\\n–einfacher Formalismus.  \\\\n•Boolesche Modelle können erweitert werden, um \\\\nRanking einzuschließen.  \\\\n•Effiziente Implementierungen für normale \\\\nAnfragen möglich.  Das Boolesche Retrieval -Modell  \\', \\'9 Probleme Boolescher Modelle  \\\\n•Sehr starr: AND verlangt Anwesenheit aller  Terme; OR \\\\ndie von mindestens einem ; es gibt keine Zwischenlösungen.  \\\\n•Schwierig, komplexe Anwenderanfragen auszudrücken.  \\\\n•Schwierig, die Anzahl der abgerufenen Dokumente zu \\\\nsteuern.  \\\\n–Alle passenden Dokumente werden zurückgegeben.  \\\\n•Schwierig, Ergebnis einzuordnen.  \\\\n–Alle passenden Dokumente passen logisch auf die Anfrage.          \\\\n\\\\uf0e0 keine weitere Bewertung.  \\\\n•Schwierig, Relevanz -Feedback zu integrieren.  \\\\n–Falls ein Dokument vom Anwender als relevant oder irrelevant \\\\nidentifiziert wird, wie sollte die Anfrage geändert werden?  \\', \\'10 Statistische Modelle  \\\\n•Ein Dokument  wird typischerweise  als   \\\\n bag of words (Sack mit Wörtern )  \\\\n(ungeordnete  Liste  von Wörtern  und deren  Häufigkeiten ) \\\\nrepräsentiert . \\\\n•“Sack” = Multimenge , d.h. eine Menge , die das mehrfache  \\\\nVorkommen  des gleichen  Elementes  erlaubt . \\\\n•Anfrage  = Anwender  spezifiziert  eine Menge  gewünschter  \\\\nTerme  (mit optionalen  Gewichten ): \\\\n–Gewichtete  Anfrageterme :  \\\\n    Q =  < Datenbank  0.5; Text 0.8; Information 0.2 >  \\\\n–Ungwichtete  Anfrageterme :  \\\\n    Q  =  < Datenbank ; Text; Information >  \\\\n–Boolesche  Bedingungen  können  in der Anfrage  nicht  spezifiziert  \\\\nwerden . \\', \\'11 Statistisches Retrieval   \\\\n•Retrieval basierend auf Ähnlichkeit  zwischen Anfrage \\\\nund Dokumenten.  \\\\n•Output: Dokumente, nach ihrer Ähnlichkeit zur \\\\nAnfrage geordnet  \\\\n•Ähnlichkeit basierend auf Auftretens -Häufigkeiten  \\\\nvon Schlüsselwörtern in der Anfrage und im \\\\nDokument.  \\\\n•Automatisches Relevanz -Feedback kann unterstützt werden \\\\ndurch:  \\\\n–Relevante Dokumente werden zur Anfrage “addiert”.  \\\\n–Irrelevante Dokumente werden von der Anfrage “subtrahiert”.  \\', \\'12 Probleme des Vektorraum -Modells  \\\\n•Wie können wichtige Worte in einem Dokument bestimmt \\\\nwerden?  \\\\n–Wordbedeutung?  \\\\n–Wort -N-Gramme (und Phrasen, Idiome,…)  \\\\uf0e0 Terme  \\\\n•Wie kann der Grad der Wichtigkeit eines Terms innerhalb \\\\neines Dokuments und innerhalb des kompletten Korpus \\\\nbestimmt werden?  \\\\n•Wie kann der Grad der Ähnlichkeit zwischen einem \\\\nDokument und der Anfrage bestimmt werden?  \\\\n•Im Falle des Webs: Was ist die Dokumentensammlung  \\\\nund was sind die Auswirkungen von Links, \\\\nFormatierungsinformationen, etc.?  \\', \\'13 Das Vektorraum -Modell  \\\\n•Gehe davon aus, das  t eindeutige Terme nach der \\\\nVorverarbeitung bleiben; nenne sie Indexterme oder das \\\\nVokabular.  \\\\n•Diese “orthogonalen” Terme spannen einen Vektorraum \\\\nmit Dimension  t  auf. \\\\n•Jedem Term i in einem Dokument oder einer Anfrage j \\\\nwird ein reellwertiges Gewicht wij. zugeordnet (im \\\\neinfachsten Fall die Anzahl des Auftretens von i in j). \\\\n•Sowohl Dokumente als auch Anfragen werden als  \\\\n t-dimensionale Vektoren ausgedrückt:  \\\\n          dj = (w1j, w2j, …, wtj) \\\\n \\', \\'14 Grafische Darstellung  \\\\nBeispiel : \\\\nD1 = 2T1 + 3T2 + 5T3 \\\\nD2 = 3T1 + 7T2 +   T3 \\\\nQ = 0T1 + 0T2 +  2T3 T3 \\\\nT1 \\\\nT2 D1 = 2T1+ 3T2 + 5T3 \\\\nD2 = 3T1 + 7T2 +  T3 Q = 0T1 + 0T2 + 2T3 \\\\n7 3 2 5 \\\\n•Ist D1 oder D2 zu Q ähnlicher?  \\\\n•Wie messe ich den Grad der \\\\nÄhnlichkeit? Abstand? Winkel? \\\\nProjektion?  \\', \\'15 Dokumentensammlung  \\\\n•Eine Sammlung  von n Dokumenten  kann  im \\\\nVektorraummodell  durch  eine Term -Dokument -Matrix \\\\ndargestellt  werden . \\\\n•Ein Eintrag  in der Matrix entspricht  dem “Gewicht ” eines  \\\\nTerms in dem Dokument ; Null heisst , dass der Term im \\\\nDokument  keine  Bedeutung  hat oder dass er im Dokument  \\\\neinfach  nicht  vorkommt . \\\\n        T1   T2    ….      Tt \\\\nD1    w11  w21   …      wt1 \\\\nD2    w12  w22   …      wt2 \\\\n :       :      :               :  \\\\n :       :      :               :  \\\\nDn    w1n  w2n   …      wtn \\\\n \\', \\'16 Termgewichte: Termhäufigkeit  \\\\n•Häufigere Terme in einem Dokument sind \\\\nwichtiger, d.h. indikativer für das Thema.  \\\\n        fij = Häufigkeit von Term i in Dokument j  \\\\n \\\\n•Man kann Termhäufigkeit  (tf) über den gesamten \\\\nKorpus normalisieren mit:  \\\\n        tfij   = fij   / maxk,l {fkl} \\\\n   \\\\n\\', \\'17 Termgewichte:  \\\\nInvertierte Dokumenthäufigkeit  \\\\n•Terme, die in vielen verschiedenen Dokumenten auftreten, \\\\nsind weniger  indikativ für das Gesamtthema.  \\\\n     df i = Dokumenthäufigkeit des Terms  i   \\\\n           = Anzahl der Dokumente, die Term  i enthalten  \\\\n   (document frequency, df) \\\\n     idfi = Invertierte Dokumenthäufigkeit von Term  i,   \\\\n           = ln ( N/ df i)   \\\\n             (N: gesamte Anzahl von Dokumenten)  \\\\n   (inverted document frequency, idf) \\\\n•Eine Angabe zur Unterscheidungsfähigkeit  eines Terms.  \\\\n•Der Logorithmus wird benutzt, um die Auswirkung \\\\nbezüglich tf  zu  dämpfen.  \\\\n \\', \\'18 TF-IDF-Gewichtung  \\\\n•Ein typischer zusammenhängender Indikator für \\\\ndie Wichtigkeit eines Terms ist tf-idf-Gewichtung : \\\\nwij =  tfij ¢ idfi  =  tfij ¢ ln(N/ dfi)  \\\\n•Einem Term, der häufig im Dokument, aber selten \\\\nim Rest der Sammlung auftritt, wird hohes \\\\nGewicht gegeben.  \\\\n•Viele andere Wege zur Bestimmung von \\\\nTermgewichten wurden vorgeschlagen.  \\\\n•Experimentell konnte gezeigt werden, dass tf-idf  \\\\ngut funktioniert.  \\', \\'19 Berechnung TF -IDF -- Ein Beispiel  \\\\n•Gegeben sei ein Dokument, das Terme mit den \\\\nfolgenden Häufigkeiten enthält:  \\\\n    A(3), B(2), C(1)  \\\\n•Die Sammlung enthält 10.000 Dokumente und die \\\\nDokumenthäufigkeiten dieser Terme seien:  \\\\n    A(50), B(1300), C(250)  \\\\nDann ist:  \\\\nA:  tf = 3/3;  idf = ln(10000/50) = 5.3;     tf-idf = 5.3  \\\\nB:  tf = 2/3;  idf = ln(10000/1300) = 2.0; tf-idf = 1.3  \\\\nC:  tf = 1/3;  idf = ln(10000/250) = 3.7;   tf-idf = 1.2  \\', \\'20 HTML -Struktur & Merkmalgewichtung  \\\\n•Gewichte ggf. Tokens unter bestimmten HTML - \\\\nTags stärker:  \\\\n–<TITLE> Token (Google scheint Titelüberein -stimmungen zu \\\\nmögen)  \\\\n–<H1>,<H2>… Token  \\\\n–<META> Schlüsselwort -Token  \\\\n•Zerlege eine Seite in verschiedene Abschnitte (z.B. \\\\nNavigationsleiste und Seiteninhalt) und gewichte \\\\nauf den unterschiedlichen Abschnitten basierende \\\\nToken unterschiedlich.  \\', \\'21 Anfragevektor  \\\\n•Der Anfragevektor wird typischerweise als \\\\nDokument behandelt und ebenfalls mit tf -\\\\nidf gewichtet.  \\\\n•Eine Alternative für den Anwender ist, die \\\\nGewichte für die Anfrage direkt anzugeben.  \\', \\'22 Ähnlichkeitsmaß  \\\\n•Ein Ähnlichkeitsmaß  ist eine Funktion, die den \\\\nGrad der Ähnlichkeit  zwischen zwei Vektoren \\\\nberechnet.  \\\\n \\\\n•Benutzung eines Ähnlichkeitsmaßes zwischen der \\\\nAnfrage und jedem Dokument:  \\\\n–Es ist möglich, die gewonnenen Dokumente in der \\\\nReihefolge der vermuteten Bedeutung zu klassifizieren.  \\\\n–Es ist möglich, eine bestimmte Schwelle  anzugeben, so \\\\ndass die Größe der erhaltenen Menge an Dokumenten \\\\ngesteuert werden kann.  \\', \\'23 Ähnlichkeitsmaß: Skalarprodukt  \\\\n•Ähnlichkeit zwischen Vektoren für das Dokument dj und Anfrage q \\\\nkönnen berechnet werden als das Skalarprodukt (inner product) der \\\\nbeiden Vektoren:  \\\\n \\\\n               sim(dj,q) = dj•q =      wij · wiq \\\\n \\\\n    wobei wij das Gewicht von Term i in Dokument j und wiq das Gewicht von \\\\nTerm i in der Anfrage ist  \\\\n•Für binäre Vektoren ist das Skalarprodukt die Anzahl der \\\\nübereinstimmenden Anfrageterme in einem Dokument (Größe der \\\\nSchnittmenge).  \\\\n•Für gewichtete Termvektoren ist es die Summe der Produkte der \\\\nGewichte der passenden Terme.  \\\\n\\\\uf0e5\\\\n\\\\uf03dt\\\\ni1\\', \\'24 Eigenschaften des Skalarproduktes  \\\\n \\\\n•Das Skalarprodukt hat keine obere Schranke.  \\\\n \\\\n•Es favorisiert lange Dokumente mit einer großen \\\\nAnzahl von unterschiedlichen Termen.  \\\\n \\\\n•Es misst, wieviel Terme übereinstimmen, aber \\\\nnicht, wie viele Terme nicht  passen.  \\\\n \\', \\'25 Skalarprodukt – Beispiele  \\\\nBinär:  \\\\n–D  =  1,    1,    1,   0,    1,    1,     0  \\\\n–Q  =  1,    0 ,   1,   0,    0,    1,     1  \\\\n \\\\nsim(D, Q) = 3  Vektorgröße = Größe d. V okabulars = 7  \\\\n \\\\n0 bedeutet, dass ein entsprechender Term \\\\nnicht im Dokument oder der Anfrage \\\\nenthalten ist.  \\\\n Gewichtet:  \\\\n           D1 = 2T1 + 3T2 + 5T3           D2 = 3T1 + 7T2 +  1T3       \\\\n                  Q = 0T1 + 0T2 +  2T3 \\\\n \\\\n sim(D1 , Q) = 2*0 + 3*0 + 5*2  = 10  \\\\n       sim(D2 , Q) = 3*0 + 7*0 + 1*2  =  2  \\', \\'26 Das Kosinus -Ähnlichkeitsmaß  \\\\n•Kosinus -Ähnlichkeit misst den Kosinus \\\\ndes Winkels zwischen zwei Vektoren.  \\\\n•Dies ist das Skalarprodukt  normalisiert \\\\ndurch die Vektorlänge.  \\\\nD1 = 2T1 + 3T2 + 5T3     CosSim( D1 , Q) = 10 / \\\\uf0d6(4+9+25)(0+0+4) = 0.81 \\\\nD2 = 3T1 + 7T2 + 1T3     CosSim( D2 , Q) =  2 / \\\\uf0d6(9+49+1)(0+0+4) = 0.13 \\\\n Q = 0T1 + 0T2 + 2T3 \\\\uf0712 t3 \\\\nt1 \\\\nt2 D1 \\\\nD2 Q \\\\uf0711 \\\\nD1 ist 6 mal besser als D2 wenn die Kosinusähnlichkeit verwendet wird,  aber nur \\\\n5 mal besser bei Verwendung des Skalarprodukts.  \\\\n\\\\uf0e5\\\\uf0e5\\\\uf0e5\\\\n\\\\uf03d \\\\uf03d\\\\uf03d\\\\uf0b7\\\\n\\\\uf0d7\\\\uf0d7\\\\n\\\\uf03d\\\\n\\\\uf0d7t\\\\nit\\\\nit\\\\ni\\\\nw www\\\\nqdqd\\\\niq ijiq ij\\\\njj\\\\n1 12 21) (\\\\n\\\\uf072\\\\uf072\\\\uf072\\\\uf072CosSim( dj, q) = \\', \\'27 Naive Implementierung  \\\\n•Berechne für jedes Dokument der Sammlung D einen tf-idf -\\\\ngewichteten Vektor dj mit Länge |V| (V=Vokabular).  \\\\n•Konvertiere die Anfrage in einen tf-idf-gewichteten Vektor q. \\\\n•Für jedes dj in D \\\\n       berechne Wert (Score) sj = cosSim( dj, q) \\\\n•Sortiere Dokumente nach abnehmendem Score.  \\\\n•Präsentiere dem Anwender die besten Dokumente.  \\\\n \\\\nZeitkomplexität:  O(| V|·|D|)   Schlecht für größere V und D ! \\\\n|V| = 10,000; | D| = 100,000; | V|·|D| = 1,000,000,000  \\', \\'28 Kommentare zum Vektorraum -Modell  \\\\n•Einfacher, mathematisch basierter Ansatz.  \\\\n•Berücksichtigt sowohl lokale ( tf) als auch globale \\\\n(idf) Wortauftretenshäufigkeiten.  \\\\n•Liefert Ergebnisse, die nicht unbedingt alle \\\\nSuchbegriffe enthalten.  \\\\n•Liefert eine geordnete Liste.  \\\\n•Funktioniert – trotz offensichtlicher Schwächen – \\\\nin der Praxis ziemlich gut.  \\\\n•Ermöglicht eine effektive Implementierung für \\\\ngroße Dokumentensammlungen.  \\', \\'29 Probleme mit Vektorraum -Modellen  \\\\n•Fehlende semantische Informationen (z.B. Wortbedeutung, \\\\nNegation).  \\\\n•Fehlende syntaktische Informationen (z.B. Phrasenstruktur, \\\\nWortreihenfolge, Bereichsinformationen).  \\\\n•Setzt Termunabhängigkeit voraus (ignoriert z.B. Synonyme).  \\\\n•Weniger Kontrolle als im booleschen Modell (z.B. die \\\\nForderung, dass ein Term im Dokument erscheinen muss ). \\\\n–Bei gegebener Anfrage mit zwei Termen “A B” könnte es sein, dass \\\\nein Dokument, das A häufig enthält, aber B gar nicht, einem \\\\nDokument, das sowohl A und B enthält, aber weniger häufig, \\\\nvorgezogen wird.  \\\\n \\', \\'1  \\\\nElementares Tokenizing,  \\\\nIndexing, und die  \\\\nImplementierung von  \\\\nvektorraumbasiertem  \\\\nRetrieval   \\\\n \\\\nViele Folien in  diesem Abschnitt sind eine deutsche Übersetzung der Folien von  \\\\nRaymond J. Mooney (http://www.cs.utexas.edu/users/mooney/ir -course/).  \\', \\'2 KSM – Kasseler Suchmaschine  \\\\n•KSM wird ein einfaches, in Java \\\\ngeschriebenes Vektorraum -Retrieval - \\\\nSystem werden.  \\\\n•Entsteht während der praktischen Übung bei \\\\njedem Teilnehmer.  \\\\n•Wird mit HTML - und ASCII -Dateien \\\\numgehen können und einen einfachen \\\\nSpider enthalten.  \\', \\'3 Einfaches Tokenizing  \\\\n•Zerlege Text in eine Sequenz einzelner Token (Terme) . \\\\n•Manchmal sind Interpunktion ( e-mail), Zahlen (1999), und \\\\nGroß -/Kleinschreibung ( Republican  vs. republican ) ein \\\\naussagekräftiger Teil  eines Tokens.  \\\\n•Häufig sind sie es jedoch nicht.  \\\\n•Die einfachste Annäherung ist, alle Zahlen und \\\\nInterpunktionen zu ignorieren und nur ununterbrochene \\\\nStrings alphabetischer Zeichen ohne Berücksichtigung der \\\\nGroß - und Kleinschreibung als Token zu verwenden.  \\', \\'4 Tokenizing HTML  \\\\n•Sollte Text in HTML -Befehlen, der \\\\ntypischerweise nicht vom Anwender gesehen \\\\nwerden kann, als Token im Modell enthalten sein?  \\\\n–Wörter, die in URLs erscheinen.  \\\\n–Wörter, die in “Metatext” von Bildern erscheinen.  \\\\n \\\\n•Die einfachste – und in KSM verwendete \\\\nAnnäherung –  ist, alle HTML -Tag-Informationen \\\\n(zwischen “<“ und “>”) beim Berechnen der \\\\nTokens auszuschließen.  \\', \\'5 Dokumente in KSM  \\\\n•Dokumente aus verschiedenen Quellen  \\\\n–ASCII -Datei  \\\\n–HTML -Datei bzw. URL  \\\\n–String  \\\\n•Auch Anfragen sind Dokumente!  \\\\n–String  \\', \\'6 Stopwörter  \\\\n•Wörter mit hoher Häufigkeit werden normalerweise \\\\nignoriert  (z.B. Funktionswörter: “a”, “the”, “in”, “to”; \\\\nPronomen: “I”, “he”, “she”, “it”).  \\\\n•Stopwörter sind sprachabhängig. KSM verwendet für \\\\nEnglisch eine Standardmenge von etwa 500 Wörtern.  \\\\n•Aus Effizienzgründen sollte man Stopwörter als Strings in \\\\neiner Hash -Tabelle abspeichern, um auf diese in konstanter \\\\nZeit zugreifen zu können.  \\\\n \\\\n Stopwortlisten für verschiedene Sprachen findet man z.B. unter: \\\\nhttp://www.unine.ch/info/clef/  \\\\n \\', \\'7 Stemming  \\\\n•Reduziert Token auf die “Stamm” -Form eines \\\\nWortes, um morphologische Variationen zu \\\\nerkennen.  \\\\n–“computer”, “computational”, “computation” werden \\\\nalle auf den gleichen Wortstamm reduziert.  \\\\n•Eine korrekte morphologische Analyse ist sprach -\\\\nspezifisch und kann komplex sein (meist \\\\nwörterbuchbasiert).  \\\\n•Stemming löscht relativ “blind” iterativ bekannte \\\\nAffixe (Präfixe und Suffixe).  \\', \\'8 Porter Stemmer  \\\\n•Einfaches Verfahren für das Entfernen von Suffixen \\\\nim Englischen.  \\\\n•Ist ohne ein Lexikon verwendbar.  \\\\n•Kann ungewöhnliche Stämme bilden, die weder \\\\nenglische Wörter noch Wortstämme im \\\\ngrammatikalischen Sinne sind:  \\\\n–“computer”, “computational”, “computation” werden alle \\\\nreduziert auf den gleichen Token “comput”.  \\\\n•Kann eigenständige Wörter zu demselben Stamm \\\\nverschmelzen (auf das gleiche Token reduzieren).  \\\\n•Erkennt nicht alle morphologischen \\\\nAbstammungen.  \\', \\'9 Porter -Stemmer -Algorithmus  \\\\n•Der Algorithmus besteht aus einer Kaskade von \\\\nSubstitutionen für gegebene Bedingungen. Bsp.:  \\\\n \\\\n•GENERALIZATIONS  \\\\n•GENERALIZATION  \\\\n•GENERALIZE  \\\\n•GENERAL  \\\\n•GENER  \\\\n \\\\n•Online -Version: http://maya.cs.depaul.edu/~classes/ds575/porter.html  \\\\n \\\\n Porter, M.F., 1980, An algorithm for suffix stripping, \\\\nProgram, 14(3) :130 -137 \\\\nDie folgenden Folien stammen von Prof. Bonnie J. Dorr.  \\', \\'10 Porter -Stemmer -Algorithmus  \\\\n•Definitionen  \\\\n–C = String mit einem oder mehreren Konsonanten. Ein Konsonant \\\\nist alles außer: A E I O U oder (Y mit vorausgehendem C)  \\\\n–V = String mit einem oder mehreren Vokalen  \\\\n–M = ungefähres Maß für die Anzahl der Silben  \\\\n–Words = (C)*(V*C*)M(V)*  \\\\n•    M=0    TR,  EE,  TREE,  Y,  BY  \\\\n•    M=1    TROUBLE,  OATS,  TREES,  IVY  \\\\n•    M=2    TROUBLES,  PRIVATE,  OATEN,  ORRERY  \\\\n•Bedingungen  \\\\n–*S  - Stamm endet mit S  \\\\n–*v* - Stamm enthält ein V  \\\\n–*d  - Stamm endet mit doppeltem C, z.B., -TT, -SS  \\\\n–*o  - Stammende ist CVC, wobei das zweite C kein W, X oder Y \\\\nist, z.B., -WIL, HOP  \\', \\'11 Porter -Stemmer -Algorithmus  \\\\nStep 1 : Plural Nouns and Third Person Singular Verbs  \\\\n SSES \\\\uf0e0 SS        caresses  \\\\uf0e0  caress \\\\n IES \\\\uf0e0 I                 ponies    \\\\uf0e0  poni \\\\n                              ties      \\\\uf0e0  ti \\\\n SS \\\\uf0e0 SS               caress    \\\\uf0e0  caress \\\\n S \\\\uf0e0     cats      \\\\uf0e0  cat  \\\\n*<S>  =  ends with < S>  \\\\n*v* = contains a V (Vokal)  \\\\n*d  = ends with double C \\\\n(Konsonant)  \\\\n*o  = ends with CVC  \\\\n        second C is not W, X or Y  \\\\nm = Anzahl Silben - 1 \\\\n \\\\nStep 2a : Verbal Past Tense and Progressive Forms  \\\\n (m > 0) EED \\\\uf0e0 EE         feed \\\\uf0e0 feed, agreed \\\\uf0e0 agree \\\\ni (*v*) ED  \\\\uf0e0               plastered \\\\uf0e0  plaster, bled \\\\uf0e0 bled \\\\nii  (*v*) ING \\\\uf0e0     motoring  \\\\uf0e0 motor, sing \\\\uf0e0 sing \\\\nStep 2b : If 2a.i or 2a.ii is successful, Cleanup  \\\\n    AT \\\\uf0e0 ATE       conflat(ed) \\\\uf0e0 conflate  \\\\n    BL \\\\uf0e0 BLE          troubl(ed) \\\\uf0e0 trouble \\\\n    IZ \\\\uf0e0 IZE        siz(ed) \\\\uf0e0 size \\\\n(*d and not (*L or *S or *Z))  hopp(ing) \\\\uf0e0  hop, tann(ed) \\\\uf0e0 tan \\\\n     \\\\uf0e0 single letter  hiss(ing) \\\\uf0e0 hiss, fizz(ed) \\\\uf0e0 fizz  \\\\n(m=1 and *o) \\\\uf0e0 E             fail(ing) \\\\uf0e0 fail, fil(ing) \\\\uf0e0 file \\\\n \\', \\'12 Porter -Stemmer -Algorithmus  \\\\nStep 3: Y \\\\uf0e0 I \\\\n (*v*) Y \\\\uf0e0 I                happy  \\\\uf0e0 happi \\\\n                       sky \\\\uf0e0 sky \\\\n Step 4 : Derivational Morphology I: Multiple Suffixes  \\\\n    (m>0) ATIONAL ->  ATE           relational     ->  relate  \\\\n    (m>0) TIONAL  ->  TION          conditional    ->  condition  \\\\n                                    rational       ->  rational  \\\\n    (m>0) ENCI    ->  ENCE          valenci        ->  valence  \\\\n    (m>0) ANCI    ->  ANCE          hesitanci      ->  hesitance  \\\\n    (m>0) IZER    ->  IZE           digitizer      ->  digitize  \\\\n    (m>0) ABLI    ->  ABLE          conformabli    ->  conformable  \\\\n    (m>0) ALLI    ->  AL            radicalli      ->  radical  \\\\n    (m>0) ENTLI   ->  ENT           differentli    ->  different  \\\\n    (m>0) ELI     ->  E             vileli        - >  vile \\\\n    (m>0) OUSLI   ->  OUS           analogousli    ->  analogous  \\\\n    (m>0) IZATION ->  IZE           vietnamization ->  vietnamize  \\\\n    (m>0) ATION   ->  ATE           predication    ->  predicate  \\\\n    (m>0) ATOR    ->  ATE           operator       ->  operate  \\\\n    (m>0) ALISM   ->  AL            feudalism      ->  feudal  \\\\n    (m>0) IVENESS ->  IVE           decisiveness   ->  decisive  \\\\n    (m>0) FULNESS ->  FUL           hopefulness    ->  hopeful  \\\\n    (m>0) OUSNESS ->  OUS           callousness    ->  callous  \\\\n    (m>0) ALITI   ->  AL            formaliti      ->  formal  \\\\n    (m>0) IVITI   ->  IVE           sensitiviti    ->  sensitive  \\\\n    (m>0) BILITI  ->  BLE           sensibiliti    ->  sensible  \\\\n  \\\\n*<S>  =  ends with < S>  \\\\n*v* = contains a V (Vokal)  \\\\n*d  = ends with double C \\\\n(Konsonant)  \\\\n*o  = ends with CVC  \\\\n        second C is not W, X or Y  \\\\nm = Anzahl Silben - 1 \\\\n \\', \\'13 Porter -Stemmer -Algorithmus   \\\\n*<S>  =  ends with < S>  \\\\n*v* = contains a V (Vokal)  \\\\n*d  = ends with double C \\\\n(Konsonant)  \\\\n*o  = ends with CVC  \\\\n        second C is not W, X or Y  \\\\nm = Anzahl Silben - 1 \\\\n \\\\nStep 5 : Derivational Morphology II: More Multiple Suffixes  \\\\n    (m>0) ICATE ->  IC              triplicate     ->  triplic  \\\\n    (m>0) ATIVE ->                  formative      ->  form \\\\n    (m>0) ALIZE ->  AL              formalize      ->  formal  \\\\n    (m>0) ICITI ->  IC              electriciti    ->  electric  \\\\n    (m>0) ICAL  ->  IC              electrical     ->  electric  \\\\n    (m>0) FUL   ->                  hopeful        ->  hope \\\\n    (m>0) NESS  ->                  goodness       ->  good \\', \\'14 Porter -Stemmer -Algorithmus   \\\\n*<S>  =  ends with < S>  \\\\n*v* = contains a V (Vokal)  \\\\n*d  = ends with double C \\\\n(Konsonant)  \\\\n*o  = ends with CVC  \\\\n        second C is not W, X or Y  \\\\nm = Anzahl Silben - 1 \\\\n Step 6 : Derivational Morphology III: Single Suffixes  \\\\n    (m>1) AL    ->                  revival        ->  reviv  \\\\n    (m>1) ANCE  ->                  allowance      ->  allow  \\\\n    (m>1) ENCE  ->                  inference      ->  infer  \\\\n    (m>1) ER    ->                  airliner       ->  airlin  \\\\n    (m>1) IC    ->                  gyroscopic     ->  gyroscop  \\\\n    (m>1) ABLE  ->                  adjustable     ->  adjust  \\\\n    (m>1) IBLE  ->                  defensible     ->  defens  \\\\n    (m>1) ANT   ->                  irritant       ->  irrit  \\\\n    (m>1) EMENT ->                  replacement    ->  replac  \\\\n    (m>1) MENT  ->                  adjustment     ->  adjust  \\\\n    (m>1) ENT   ->                  dependent      ->  depend  \\\\n    (m>1 and (*S or *T)) ION ->     adoption       ->  adopt  \\\\n    (m>1) OU    ->                  homologou      ->  homolog  \\\\n    (m>1) ISM   ->                  communism      ->  commun  \\\\n    (m>1) ATE   ->                  activate       ->  activ  \\\\n    (m>1) ITI   ->                  angulariti     ->  angular  \\\\n    (m>1) OUS   ->                  homologous     ->  homolog  \\\\n    (m>1) IVE   ->                  effective      ->  effect  \\\\n    (m>1) IZE   ->                  bowdlerize     ->  bowdler  \\', \\'15 Porter -Stemmer -Algorithmus  \\\\n \\\\nStep 7a : Cleanup \\\\n    (m>1) E \\\\uf0e0       probate  \\\\uf0e0  probat \\\\n                                      rate   \\\\uf0e0  rate \\\\n    (m=1 and not *o) E \\\\uf0e0      cease  \\\\uf0e0 ceas \\\\n \\\\nStep 7b: More Cleanup  \\\\n \\\\n    (m > 1 and *d and *L)    controll \\\\uf0e0 control \\\\n   \\\\uf0e0 single letter     roll \\\\uf0e0 roll \\\\n \\\\n           \\\\n                                     \\\\n  \\\\n*<S>  =  ends with < S>  \\\\n*v* = contains a V (Vokal)  \\\\n*d  = ends with double C \\\\n(Konsonant)  \\\\n*o  = ends with CVC  \\\\n        second C is not W, X or Y  \\\\nm = Anzahl Silben - 1 \\\\n \\', \\'16 Fehler des Porter Stemmer  \\\\n•“over -stemming”, zuviel wurde entfernt:  \\\\n–organization, organ \\\\uf0ae organ  \\\\n–police, policy \\\\uf0ae polic  \\\\n–arm, army \\\\uf0ae arm \\\\n•“under -stemming”, zu wenig entfernt:  \\\\n–cylinder (cylind), cylindrical (cylindr)  \\\\n–Create (creat), creation  \\\\n–Europe (europ), European  \\\\n \\', \\'17 Dünn besetzte Vektoren  \\\\n•Das Vokabular – und damit auch die \\\\nDimensionalität des Vektorraums – kann sehr groß \\\\nwerden, ~104 Terme.  \\\\n•Jedoch enthalten die meisten Dokumente und \\\\nAnfragen nur sehr wenige Wörter, somit sind die \\\\nVektoren dünn besetzt („sparse“), d.h. die meisten \\\\nEinträge sind  0.  \\\\n•Man benötigt also effiziente Methoden zur \\\\nSpeicherung von und zum Rechnen mit dünn \\\\nbesetzten Vektoren.  \\', \\'18 Dünn besetzte Vektoren als Listen  \\\\n•Idee: Speichere nur Tokens, deren Gewicht \\\\nungleich 0 ist, zusammen mit ihrem Gewicht als \\\\nVektoren in einer verketteten Liste.  \\\\n–Platzbedarf ist proportional zur Anzahl der Tokens ( n) \\\\nim Dokument.  \\\\n–Erfordert eine lineare Suche in der Liste aller Tokens, \\\\num das Gewicht eines spezifischen Tokens zu finden \\\\n(oder zu verändern).  \\\\n–Erfordert im schlimmsten Fall quadratischen \\\\nZeitaufwand, um den Vektor für ein Dokument zu \\\\nberechnen:  \\\\n \\\\n)(2)1(2\\\\n1nOnnin\\\\ni\\\\uf03d\\\\uf02b\\\\uf03d\\\\uf0e5\\\\n\\\\uf03d\\', \\'19 Dünn besetzte Vektoren als Bäume  \\\\n•Indexiere Tokens eines Dokumentes in einem \\\\nbalancierten binären Baum oder einem Trie \\\\n(zeichenweiser Schlüsselvergleich), bei dem die \\\\nGewichte der Tokens an den Blättern gespeichert \\\\nsind.  \\\\n Speicher  \\\\n \\\\uf0b3 < \\\\n\\\\uf0b3 < \\\\uf0b3 < film variabel  \\\\nvariabel  \\\\n2 Speicher  \\\\n1 film \\\\n1 bit \\\\n2 Balancierter binärer Baum  \\', \\'20 Dünn besetzte Vektoren als Bäume (Forts.)  \\\\n•Overhead beim Speichern der Baumstruktur: \\\\n~2n Knoten.  \\\\n•Zeit: O(log n) um das Gewicht eines \\\\nspezifischen Tokens zu finden oder zu \\\\naktualisieren.  \\\\n•Zeit: O( n log n) um den Vektor zu erzeugen.  \\\\n \\', \\'21 Dünn besetzte Vektoren als Hash -Tabellen  \\\\n•Speichere die Tokens in einer Hash -Tabelle, mit \\\\nToken -String als Schlüssel und Gewicht als Wert.  \\\\n–Overhead beim Speichern in einer Hash -Tabelle ~1.5 n. \\\\n–Tabelle muss in Hauptspeicher passen.  \\\\n–Konstante Zeit, um das Gewicht eines spezifischen \\\\nTokens zu finden oder zu aktualisieren.  \\\\n(Kollisionen werden ignoriert.)  \\\\n–Zeit um den Vektor zu erzeugen: O( n) (Kollisionen \\\\nwerden ignoriert.)  \\\\n \\', \\'22 Dünn besetzte Vektoren in KSM  \\\\n•Hash -Tabelle, um die Terme eines \\\\nDokumentes zu verwalten:  \\\\n_termCounts: String \\\\uf0e0 Integer  \\\\n•_termCounts ist die interne Datenstruktur \\\\nder Dokumentklasse.  \\\\n \\\\n \\\\n \\', \\'23 Implementierung basierend auf \\\\ninvertierten Dateien  \\\\n•In der Praxis werden Dokumentvektoren \\\\nnicht direkt gespeichert; eine invertierte \\\\nOrganisation der Daten bietet eine deutlich \\\\nhöhere Effizienz.  \\\\n•Der Keyword -to-Document -Index kann als \\\\nHash -Tabelle, sortiertes Array oder als \\\\nBaumstruktur ( Trie, Tree) gespeichert \\\\nwerden.  \\\\n•Wichtig ist der Zugriff auf die Tokens in \\\\nlogarithmischer oder konstanter Zeit.  \\', \\'24 Invertierter Index  \\\\nSystem  Computer  \\\\nDatenbank  \\\\nWissenschaft  D2, 4 \\\\nD5, 2 D1, 3 D7, 4 Indexterm  df \\\\n3 \\\\n2 \\\\n4 \\\\n1 Dj, tfj \\\\nIndexdatei  Posting -Listen  \\\\uf0b7 \\\\uf0b7 \\\\uf0b7 \\', \\'25 Invertierter Index in KSM  \\\\nHashMap  \\\\ntokenHash  String  \\\\ntoken  TokenInfo  \\\\ndouble  \\\\nidf ArrayList  \\\\noccList  \\\\nTokenvorkommen  \\\\nDokumentReferenz  \\\\ndocRef  int \\\\ncount  \\\\nFile \\\\nfile double  \\\\nlength  Tokenvorkommen  \\\\nDokumentReferenz  \\\\ndocRef  int \\\\ncount  \\\\nFile \\\\nfile double  \\\\nlength  … \\', \\'26 Erzeugen eines invertierten Indexes  \\\\nErzeuge eine leere HashMap H; \\\\nFür jedes Dokument D (z.B. jede Datei in einem Input - Verzeichnis) : \\\\n       Erzeuge einen HashMap -Vector _termCounts für D; \\\\n    Für jedes (nicht -null) Token T in _termCounts:  \\\\n            Wenn T nicht bereits in H ist, erzeuge eine leere  \\\\n  TokenInfo für T und füge diese in H ein; \\\\n               Erzeuge ein Tokenvorkommen für T in D und     \\\\n                     füge es zur occList  in die TokenInfo für T; \\\\nBerechne IDF für alle Tokens in H; \\\\nBerechne Vektorlänge für alle Dokumente in H; \\', \\'27 Berechnung IDF  \\\\nN sei die Gesamtzahl aller Dokumente;  \\\\nFür jedes Token T in H: \\\\n      Bestimme die Gesamtzahl M der Dokumente,  \\\\n          in denen T vorkommt (die Länge occList  von T); \\\\n      Setze den IDF -Wert für T auf ln(N/M);  \\\\n \\\\n       Beachte, dass dies einen zweiten Durchgang durch \\\\nalle Tokens erfordert, nachdem alle Dokumente \\\\nindexiert worden sind.  \\\\n \\\\n \\', \\'28 Vektorlänge der Dokumente  \\\\n•Wdh. (aus der Linearen Algebra): Die Länge eines \\\\n(Dokument -)Vektors ist die Quadratwurzel der \\\\nSumme der Quadrate der Gewichte seiner Tokens.  \\\\n•Das Gewicht eines Tokens ist hier:  \\\\n     TF * IDF  \\\\n•Daher muss gewartet werden, bis alle IDF -Werte \\\\nbekannt sind (und demzufolge bis alle Dokumente \\\\nindexiert wurden), bevor die Dokumentlänge \\\\nbestimmt werden kann.  \\\\n \\', \\'29 Berechnung der Dokumentlängen  \\\\nGehe davon aus, dass die Länge aller \\\\nDokumentvektoren mit 0.0 initialisiert werden;  \\\\nFür jedes Token T in H: \\\\n    I sei das IDF -Gewicht von T; \\\\n    Für jedes Tokenvorkommen von T in Dokument D: \\\\n        C sei die Anzahl von T in D; \\\\n        Inkrementiere die Länge von D mit  ( I*C)2; \\\\nFür jedes Dokument D in H: \\\\n     Setze die Länge von D als die Quadratwurzel der \\\\naktuell gespeicherten Länge;  \\', \\'30 Zeitkomplexität beim Indexieren  \\\\n•Die Komplexität des Vektorerstellens und des \\\\nIndexierens für ein Dokument mit n Tokens ist \\\\nO(n). \\\\n•Indexieren von m Dokumenten kostet O( mn). \\\\n•Berechnung der IDF -Werte für jedes Token im \\\\nVokabular V kostet O(| V|). \\\\n•Der Aufwand für die Berechnung der einzelnen \\\\nVektorlängen beträgt ebenfalls O( mn). \\\\n•Wegen | V| \\\\uf0a3 mn beträgt der Zeitaufwand für den \\\\nkompletten Prozess O( mn), was auch der \\\\nKomplexität des Korpus -Einlesens entspricht.  \\', \\'31 Retrieval mit invertiertem Index  \\\\n•Tokens, die weder in der Anfrage noch im \\\\nDokument vorkommen, haben keinen Einfluss auf \\\\ndie Kosinus -Ähnlichkeit.  \\\\n–Das Produkt dieser Tokengewichte ist Null und trägt \\\\ndaher nicht zum Skalarprodukt bei.  \\\\n•Normalerweise ist die Anfrage ziemlich kurz und \\\\ndemzufolge ihr Vektor äußerst  dünn besetzt.  \\\\n•Verwende den invertierten Index, um die kleine \\\\nMenge von Dokumenten zu finden, die zumindest \\\\neines der Anfragewörter enthalten.  \\', \\'32 Effizienz von invertierten Anfragen  \\\\n•Angenommen, ein Anfragewort erscheint im \\\\nDurchschnitt in B Dokumenten:  \\\\n \\\\n \\\\n \\\\n•Dann beträgt die Retrievalzeit O(| Q| B) und ist \\\\ndamit im allgemeinen viel besser als das naive \\\\nRetrieval mit O(| V| N), das alle N Dokumente \\\\nüberprüft, da | Q| << | V| und B << N. Q   =   q1           q2           …       qn \\\\nD11…D1B D21…D2B Dn1…DnB \\', \\'33 Verarbeitung einer Anfrage  \\\\n•Berechne die Kosinus -Ähnlichkeit eines jedes \\\\nindexierten Dokumentes inkrementell, indem \\\\ndie Anfragewörter nacheinander abgearbeitet \\\\nwerden.  \\\\n•Um eine Bewertung für jedes Dokument zu \\\\nermitteln, speichert man die gefundenen \\\\nDokumente in einer Hash -Tabelle. Die \\\\nReferenz auf das Dokument wird der \\\\nSchlüssel und die bisher bestimmte \\\\nBewertung der Wert.  \\', \\'34 Algorithmus: Anfrage gegen invertierten Index  \\\\nErzeuge einen HashMap -Vektor Q für die Anfrage.  \\\\nErzeuge leere HashMap R, um gefundene Dokumente und deren Werte zu speichern.  \\\\nFür jedes Token T in Q: \\\\n     Sei i das IDF von T und k die Anzahl von T in Q; \\\\n     Setze das Gewicht von T in Q:   w = k¢i; \\\\n     Sei L die Liste der Tokenvorkommen von T in H;      (d.h. eine Zeile des inv. Index H) \\\\n     Für jedes Tokenvorkommen O in L: \\\\n         Sei D das Dokument zu O und c die Anzahl von T in O;                   (tf von T in D) \\\\n    Wenn D nicht bereits in R ist                                     (D wurde zuvor nicht gefunden)                       \\\\n               dann füge D zu R und initialisiere Wert auf 0,0;  \\\\n         Erhöhe den Wert von D um w¢i¢c;             (Produkt des Gewichtes von T in Q und D) \\', \\'35 Retrieval -Algorithmus (Forts.)  \\\\nBerechne die Länge l des Vektors Q (Quadratwurzel der Summe \\\\nder Quadrate seiner Gewichte).  \\\\nFür jedes gewonnene Dokument D in R: \\\\n       Sei s die aktuelle Bewertung von D; \\\\n          (s ist das Skalarprodukt von D und Q) \\\\n       Sei y die Länge von D, wie es in der Dokumentenreferenz \\\\n           gespeichert ist;  \\\\n      Normalisiere die endgültige Bewertung von D durch s/(l¢y); \\\\nSortiere die gewonnenen Dokumente in R anhand ihrer \\\\nBewertungen und lege das Ergebnisse in einem Array ab.  \\\\n \\', \\'36 Effizienz -Verbesserung  \\\\n•Um die Berechnungs -Effizienz zu steigern \\\\nund eine zusätzliche Iteration durch die \\\\nTokens in der Anfrage zu vermeiden, wird \\\\ndie Berechnung  der Länge des \\\\nAnfragevektors in die Verarbeitung der \\\\nAnfragetoken integriert.  \\', \\'37 Anwenderschnittstelle  \\\\nBis der Anwender mit einer leeren Anfrage abschließt:  \\\\n      Fordere den Anwender auf, eine Anfrage Q zu stellen.  \\\\n      Berechne die geordnete Liste R der gefundenen D für Q; \\\\n      Drucke die Namen der ersten n Dokumente in R; \\\\n      Bis der Anwender mit einem leeren Befehl abschließt:  \\\\n           Fordere den Anwender auf, einen der folgenden \\\\n Befehle als Ergebnis dieser Anfrage einzugeben:  \\\\n                   1) Zeige die nächsten n Elemente der Liste R; \\\\n                   2) Zeige das m-te gefundene Dokument;  \\\\n                        (Dokument wird im Browser -Fenster gezeigt)  \\', \\'38  \\\\nEffizientes Erstellen eines \\\\ninvertierten Indexes für sehr \\\\ngroße Datenmengen  \\\\nAus: Managing Gigabytes: Compressing and Indexing Documents and Images, Ian H. Witten, Alistair Moffat, and Timothy C. Bell, 1999 \\', \\'39 Beispieldatensatz  \\\\n•      5 GB Datensatz  \\\\n•    5 Mill. Dokumente  \\\\n•    1 Mill. unterschiedliche Wörter  \\\\n•800 Mill. Wörter insgesamt  \\\\n•400 Mill. Index -Einträge  \\\\n•    30 MB für das Lexikon  \\\\n•  400 MB für den komprimierten Index  \\\\nAus: Managing Gigabytes: Compressing and Indexing Documents and Images, Ian H. Witten, Alistair Moffat, and Timothy C. Bell, 1999 \\', \\'40 Hauptspeicher -basierter invertierter Index  \\\\n•Hashtabellen -basierter Ansatz mit einer Linkliste ist eine der \\\\neffizientesten Varianten.  \\\\n•Angenommen, man liest die Daten mit 2Mb/s von der Platte, \\\\ndann braucht man 40 min für 5GB.  \\\\n•Verarbeiten (tokenizing, stemming, etc.) ca. 4 h  \\\\n•Schreiben des invertierten Indexes ca. 40 min  \\\\n•Bei 10 Bytes für jeden Knoten und 400 Mill. Knoten braucht \\\\nman 4GB Hauptspeicher.  \\\\n  \\\\n! Zu viel! (Zumindest für 1999, aber zwischenzeitlich sind die \\\\nDokumentensammlungen ebenfalls signifikant gewachsen.)  \\', \\'41 Linked -Liste auf der Festplatte  \\\\n•1. Ansatz: Linked -Liste der Dokumentnummern \\\\nauf  Platte speichern.  \\\\n–Erste Schritte des Algorithmus sind weiter effizient.  \\\\n–Nach dem Aufbau der Linked -Liste muss diese zum \\\\nSchreiben des invertierten Indexes in Termordnung \\\\ndurchlaufen werden.  \\\\n–Annahme: 10 ms für jeden Zugriff auf die Platte um 10 \\\\nByte zu lesen.  \\\\n–Bei 400 Mill. Einträgen führt dies zu 4 Mill. Sekunden, \\\\nd.h. 6 Wochen.  \\\\n ! Zu hoher Zeitaufwand!  \\\\n \\', \\'42 Lösung: Sortierter Index auf Platte  \\\\n•Folgende wesentliche Schritte umfasst der \\\\nAlgorithmus:  \\\\n–Initialisiere Datenstrukturen im Hauptspeicher.  \\\\n–Lese Texte von Platte, verarbeite Dokumente \\\\nund schreibe sequentiell in eine tmp -Datei für \\\\njeden Term eines Dokumentes einen Datensatz  \\\\n<t, d, fd,t>. \\\\n–Sortiere die tmp -Datei nach Termen, um den \\\\ninvertierten Index daraus zu erzeugen.  \\\\n–Schreibe invertierten Index.  \\\\n \\', \\'43 Sortierter Index auf Platte  \\\\n•Initialisieren der Datenstrukturen im \\\\nHauptspeicher:  \\\\n–Erstelle eine leere Wörterbuchstruktur S. \\\\n–Erstelle einer leeren tmp -Datei auf der \\\\nFestplatte.  \\', \\'44 Sortierter Index auf Platte  \\\\n•Erstellen der tmp -Datei für jeden Term eines \\\\nDokumentes:  \\\\n–Für jedes Dokument d \\\\n•Lese und parse das Dokument d \\\\n•Für jeden Term t aus Dokument d \\\\n–Bestimme die Häufigkeit fd,t für Term t aus Dokument d \\\\n–Suche nach t in S; falls t nicht in S, füge t hinzu.  \\\\n–Schreibe den Datensatz < t, d, fd,t> in die tmp -Datei, wobei t \\\\ndurch seine Termnummer in S repräsentiert wird.  \\\\n \\', \\'45 Sortierter Index auf Platte  \\\\n•Sortieren der tmp -Datei:  \\\\n–Sei k die Anzahl an Datensätzen, die der \\\\nHauptspeicher aufnehmen kann.  \\\\n•Lese k Datensätze von tmp -Datei  \\\\n•Sortiere diese Datensätze aufsteigend nach t und d \\\\n•Schreibe den sortierten Teil zurück in tmp -Datei.  \\\\n•Wiederhole dies, bis keine Datensätze verbleiben.  \\\\n–Paarweises Mischen der vorigen Durchläufe, \\\\nbis der Datensatz vollständig sortiert ist.  \\', \\'46 Sortierter Index auf Platte  \\\\n•Schreiben des invertierten Indexes:  \\\\n–Für jeden Term t \\\\n•Beginne einen neuen Eintrag im invertierten Index  \\\\n•Lese alle Datensätze < t, d, fd,t> zu Term t aus der \\\\ntmp-Datei, und erzeuge einen Eintrag für Term t. \\\\n•(Wenn nötig, komprimiere diesen Eintrag.)  \\\\n•Hänge den Eintrag an den invertierten Index auf der \\\\nFestplatte an.  \\', \\'47 Sortierter Index auf Platte; Aufwand?  \\\\n•Bei 40 MB Hauptspeicher braucht man ca. 20 \\\\nStunden und 8 GB Speicher zusätzlichen \\\\nPlattenplatz.  \\\\n•5 Stunden, um Dokumente zu verarbeiten und die \\\\ntmp-Datei zu erstellen.  \\\\n•Bei 40MB Hauptspeicher werden k = 100 Blöcke \\\\ngebildet und in 4 Stunden sortiert.  \\\\n•„Mischen“ dieser 100 Blöcke in 7 Durchgängen \\\\ndauert ca. 9 Stunden.  \\\\n•Ca. 2 Stunden werden benötigt, um aus der tmp -\\\\nDatei den invertierten Index zu erstellen.  \\', \\'48 Zwei weitere Methoden zur Steigerung der \\\\nEffizienz  \\\\n•Kompression der tmp -Datei  \\\\n–Kompression reduziert den Overhead durch das Schreiben der \\\\ntmp-Datei auf die Platte.  \\\\n–Nur noch Abstände (Gaps) werden gespeichert.  \\\\n–Sowohl die Dokumentenliste und deren Häufigkeit als auch die \\\\nListe der Terme können komprimiert werden.  \\\\n–Bei 40MB Speicher: 680 MB Festplattenplatz, 26 Stunden  \\\\n \\\\n•Multiway Merge  \\\\n–Nicht nur 2 sondern bspw. 4 initiale Sortier -Durchläufe werden auf \\\\neinmal gemischt.  \\\\n–Dadurch reduziert sich die Anzahl der Durchläufe.  \\\\n–Bei 40MB Speicher: 540 MB Festplattenplatz, 11 Stunden  \\', \\'1Evaluierung der G üte\\\\nvon Information-Retrieval-Systemen\\\\nViele Folien in  diesem Abschnitt sind eine deutsche Übersetzung der Folien von \\\\nRaymond J. Mooney (http://www.cs.utexas.edu/users/mooney/ir-course/). \\', \\'2Warum Systemevaluierung?\\\\n•Es gibt viele Retrievalmodelle/Algorithmen/Systeme. \\\\n•Welches ist das Beste?\\\\n•Welches ist die beste Komponente für:\\\\n–Ranking-Funktion (Skalarprodukt, Kosinus, …)?\\\\n–Termselektion (Entfernen von Stopwörtern, Stemming …)?\\\\n–Termgewichtung (TF, TF-IDF, …)?\\\\n•Wie weit muss ein Anwender in einer geordneten \\\\nListe nach unten gehen, um einige/alle relevanten \\\\nDokumente zu finden?\\', \\'3Schwierigkeiten bei der Evaluierung von \\\\nIR Systemen\\\\n•Die Bewertung der Effektivität steht in engem Bezug zur \\\\nRelevanz von gefundenen Elementen.\\\\n•Relevanz ist typischerweise nicht binär, sondern eine stetige \\\\nGröße.\\\\n•Selbst wenn Relevanz binär ist, kann es schwierig sein, eine \\\\nBeurteilung abzugeben.\\\\n•Relevanz aus menschlicher Sicht ist...\\\\n–subjektiv: hängt von der spezifischen Beurteilung des Anwenders ab.\\\\n–situativ: bezieht sich auf die aktuellen Bedürfnisse des Anwenders.\\\\n–kognitiv: hängt von der menschlichen Wahrnehmung und dem \\\\nVerhalten ab.\\\\n–dynamisch: ver ändert sich im Laufe der Zeit.\\', \\'4Manuell indizierte Korpora\\\\n (Gold-Standards)\\\\n•Gegeben sei ein Korpus von Dokumenten.\\\\n•Sammle eine Liste von Anfragen für diesen \\\\nKorpus.\\\\n•Ein oder mehrere menschliche Experten \\\\nkennzeichnen (labeln) zu jeder Anfrage die \\\\nrelevanten Dokumente.\\\\n•Man geht (der Einfachheit halber) typischerweise \\\\nvon binären Relevanz-Beurteilungen aus.\\\\n•Erfordert für große Korpora mit vielen Anfragen \\\\nerheblichen menschlichen Aufwand.\\', \\'5Dokumente relevanten der GesamtzahlDokumente gefundenen relevanten der Anzahl\\\\n  Recall=\\\\nDokumente gefundenen der GesamtzahlDokumente gefundenen relevanten der Anzahl\\\\n  Precision =\\\\nRelevante \\\\nDokumenteGefundene \\\\nDokumenteVollständige \\\\nDokumenten-\\\\nsammlung\\\\ngefunden & \\\\nrelevantnicht gefunden \\\\naber relevantgefunden & \\\\nirrelevantnicht gefunden \\\\n& irrelevant\\\\ngefunden nicht gefundenrelevantirrelevantPrecision und Recall\\', \\'6Precision und Recall\\\\n•Precision\\\\n–Gibt den Anteil der relevanten Dokumente bzgl. \\\\naller Dokumente in der Ergebnismenge wieder.\\\\n•Recall\\\\n–Misst die Fähigkeit des Suchverfahrens, alle \\\\nrelevanten Dokumente im Korpus zu entdecken.\\', \\'7Bestimmen des Recalls ist schwierig\\\\n•Die Gesamtzahl der relevanten Elemente ist häufig \\\\nnicht verfügbar:\\\\n–Nimm einen Ausschnitt der Datenbank und führe eine \\\\nRelevanzbeurteilung anhand dieser Elemente durch.\\\\n–Wende verschiedene Retrievalalgorithmen auf die gleiche \\\\nDatenbank und für die gleiche Anfrage an. Die Vereinigung \\\\naller relevanten Elemente über alle Algorithmen wird als \\\\ndie Menge aller relevanten Elemente angesehen.\\', \\'8Kompromiss zwischen Recall und Precision\\\\n101\\\\nRecallPrecisionIdeales ErgebnisLiefert die meisten relevanten Dokumente, \\\\naber verfehlt viele nützliche.\\\\nLiefert die meisten relevanten\\\\nDokumente, aber enthält \\\\nzu viele irrelevante Elemente.\\', \\'9Berechnung von Recall/Precision\\\\n•Bilde für die gegebene Anfrage die Rankingliste.\\\\n•Das Abschneiden der Liste an unterschiedlichen \\\\nSchwellwerten führt zu verschiedenen Mengen \\\\nvon gefundenen Dokumenten und demzufolge zu \\\\nverschiedenen Recall/Precision-Ergebnissen.\\\\n•Markiere jedes Dokument der Rankingliste, das \\\\ngemäß dem Gold-Standard relevant ist.\\\\n•Berechne für jedes relevante Dokument das \\\\nRecall-/Precision-Paar, dass beim Abschneiden \\\\nder Liste an dieser Stelle entsteht. \\', \\'10R=3/6=0.5;     P=3/4=0.75Berechnung von Recall/Precision: \\\\nEin Beispiel\\\\nndoc #relevant\\\\n1588x\\\\n2589x\\\\n3576\\\\n4590x\\\\n5986\\\\n6592x\\\\n7984\\\\n8988\\\\n9578\\\\n10985\\\\n11103\\\\n12591\\\\n13772x\\\\n14990Gesamtzahl relevanter Doks. = 6\\\\nPrüfe für jeden neuen Recall-Punkt:\\\\nR=1/6=0.167; P=1/1=1\\\\nR=2/6=0.333; P=2/2=1\\\\nR=5/6=0.833; p=5/13=0.38R=4/6=0.667; P=4/6=0.667\\\\nEin relevantes Dokument \\\\nfehlt. 100% Recall \\\\nwird nie erreicht \\', \\'11Interpolation der Recall/Precision-Kurve\\\\n•Interpoliere den Precision-Wert für jeden Standard Recall-\\\\nLevel:\\\\n–rj ∈{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}\\\\n–r0 = 0.0, r1 = 0.1, …, r10=1.0\\\\n•Die interpolierte Precision am j-ten Standard Recall-Level \\\\nist die maximal bekannte Precision bei jedem Recall-Level \\\\nzwischen dem  j-ten und \\\\n(j + 1)-ten Level:\\\\n)( max)(\\\\n1rP rP\\\\njjrrrj\\\\n+≤≤=\\', \\'12Interpolation der Recall/Precision Kurve: \\\\nEin Beispiel\\\\n0.4 0.81.0\\\\n0.8\\\\n0.6\\\\n0.4\\\\n0.2\\\\n0.21.00.6 RecallPrecision\\', \\'13Durchschnittliche Recall/Precision-Kurve\\\\n•Zeigt die durchschnittliche Performance für eine \\\\ngroße Menge von Anfragen an.\\\\n•Berechne für jeden Standard-Recall-Level den \\\\nDurchschnitt der Precisionwerte über alle \\\\nAnfragen.\\\\n•Zeichne die Precision/Recall-Kurve, um die \\\\nGesamtsystemleistung für einen Dokumenten-\\\\nKorpus bei den auf ihn angewandten Anfragen zu \\\\nbewerten.\\', \\'14Vergleiche zwei oder mehrerer Systeme\\\\nDie Kurve, die am nächsten an der oberen \\\\nrechten Ecke des Graphen liegt, zeigt die \\\\nbeste Leistung an.\\\\n00.20.40.60.81\\\\n0.10.20.30.40.50.60.70.80.91\\\\nRe ca llPrecisionNoStemStem\\', \\'15Beispiel RP-Kurve für Cystic Fibrosis-Korpus\\\\n\\', \\'16R- Precision\\\\n•Precision an der R-ten \\\\nPosition im \\\\nErgebnisranking einer \\\\nAnfrage, die R relevante \\\\nDokumente hat.\\\\nR = # der relevanten Doku. = 6\\\\nR-Precision = 4/6 = 0.67ndoc #relevant\\\\n1588x\\\\n2589x\\\\n3576\\\\n4590x\\\\n5986\\\\n6592x\\\\n7984\\\\n8988\\\\n9578\\\\n10985\\\\n11103\\\\n12591\\\\n13772x\\\\n14990\\', \\'17F-Maß\\\\n•Ein Leistungsmaß, das sowohl Recall als auch \\\\nPrecision berücksichtigt.\\\\n•Harmonisches Mittel von Recall und Precision:\\\\n•Verglichen mit dem arithmetischen Mittel müssen \\\\nbeide Werte hoch sein, damit das harmonische \\\\nMittel hoch ist.PRRPPR\\\\nF\\\\n112 2\\\\n+=\\\\n+=\\', \\'18E-Maß (parametrisiertes F-Maß)\\\\n•Eine F-Maß-Variante, die eine Gewichtung von Precision \\\\nvs. Recall erlaubt:\\\\n•Wert für β kontrolliert den Kompromiss:\\\\n\\\\x13β = 1: gleichmäßige Gewichtsverteilung zwischen\\\\nPrecision und Recall (E=F).\\\\n\\\\x13β > 1: mehr Gewicht auf Precision.\\\\n\\\\x13β < 1: mehr Gewicht auf Recall.PRRPPR\\\\nE\\\\n12\\\\n22\\\\n2)1( )1(\\\\n++\\\\n=\\\\n++\\\\n=\\\\nββ\\\\nββ\\', \\'19Ausfallrate (Fallout Rate)\\\\n•Probleme sowohl mit Precision als auch \\\\nmit Recall:\\\\n–Anzahl irrelevanter Dokumente in der \\\\nSammlung wird nicht berücksichtigt.\\\\n–Recall ist undefiniert, wenn es kein \\\\nrelevantes Dokument in der Sammlung gibt.\\\\n–Precision ist undefiniert, wenn kein \\\\nDokument gefunden wird.\\\\n Sammlungder in  Elemente anternichtrelev  GesamtzahlElemente relevanter nicht  gefundener Anz.\\\\n  Fallout=\\', \\'20Subjektive Relevanzma ße\\\\n•Novelty Ratio : Der Anteil gefundener und vom \\\\nAnwender als relevant beurteilter Elemente, deren \\\\ner sich zuvor nicht bewusst war.\\\\n–Fähigkeit, neue Informationen zu einem Thema zu \\\\nfinden.\\\\n•Coverage Ratio : Der Anteil relevanter Elemente, \\\\ndie in den gesamten relevanten Dokumenten \\\\ngefunden wurden, die der Anwender vor der Suche \\\\nkannte.\\\\n–Relevant, wenn der Anwender Dokumente lokalisieren \\\\nmöchte, die er zuvor gesehen hat (z.B., der \\\\nBudgetbericht für das Jahr 2000).\\', \\'21Andere zu berücksichtigende Faktoren\\\\n•Anwenderaufwand : Arbeit, die vom Anwender zu \\\\nerledigen ist: Formulieren von Anfragen, Leiten der \\\\nSuche und Selektion des Outputs.\\\\n•Antwortzeit : Zeitintervall zwischen Absetzen einer \\\\nAnwenderanfrage und der Präsentation der \\\\nSystemantworten.\\\\n•Form der Präsentation : Einfluss der Ausgabeform einer \\\\nSuchanfrage auf die Fähigkeit des Anwenders, das \\\\ngefundene Material zu verwenden.\\\\n•Umfang der Sammlung : Ausmaß, in dem jegliche/alle \\\\nrelevanten Elemente im Dokumentenkorpus enthalten \\\\nsind.\\', \\'22Experimentelles Setup für Benchmarking\\\\n•Eine analytische  Leistungsevaluierung ist für \\\\nDokument-Retrieval-Systeme schwierig, da viele \\\\nEigenschaften wie Relevanz, Verteilung der Worte \\\\netc. nur schwer präzise zu beschreiben sind.\\\\n•Leistung wird durch Benchmarking gemessen.  \\\\nD.h. die Retrieval-Effektivität eines Systems wird \\\\nanhand einer gegebenen Liste von Dokumenten, \\\\nAnfragen  und Relevanzbeurteilungen evaluiert. \\\\n•Leistungsdaten gelten nur für die Umgebung, in der \\\\n  das System evaluiert ist. \\', \\'23Benchmarks\\\\n•Eine Benchmark-Sammlung umfasst:\\\\n–Eine Liste von Standarddokumenten und \\\\nAnfragen/Themen.\\\\n–Eine Liste relevanter Dokumente für jede Anfrage.\\\\n•Standard-Sammlungen für traditionelles IR:\\\\n–Smart collection: ftp://ftp.cs.cornell.edu/pub/smart\\\\n–TREC: http://trec.nist.gov/\\\\nStandard \\\\nDokument \\\\nSammlung\\\\nStandard \\\\nAnfragenAlgorithmen \\\\nim TestEvaluierung\\\\nStandard- \\\\nergebnisGefundenes  \\\\nErgebnisPrecision \\\\nund \\\\nRecall\\', \\'24Benchmarking − die Probleme\\\\n•Leistungsdaten gelten nur für einen \\\\nspeziellen Benchmark.\\\\n•Der Aufbau eines Testdatensatzes \\\\n(benchmark corpus) ist eine schwierige \\\\nAufgabe.\\\\n•Web-Testdatensätze sind gerade in der \\\\nEntwicklung.\\\\n•Testdatensätze mit nicht-englischen \\\\nDokumenten oder mehr als einer Sprache \\\\nsind gerade in der Entwicklung.\\', \\'25•Erste Experimente basierten auf der SMART- Sammlung, \\\\ndie ziemlich wenige Dokumente enthält. \\\\n(ftp://ftp.cs.cornell.edu/pub/smart)\\\\n     Sammlung Anzahl              Anzahl     Größe \\\\n     Name   Dokumente Anfragen             (Mbytes) \\\\n     CACM 3,204   64 1.5 \\\\n     CISI1,460 112 1.3 \\\\n     CRAN 1,400 225 1.6 \\\\n     MED 1,033   30 1.1 \\\\n     TIME    425   83 1.5 \\\\n•Verschiedene Forscher haben unterschiedliche \\\\nTestsammlungen und Evaluierungstechniken angewendet.  Frühe Testsammlungen\\', \\'26Die TREC Benchmark \\\\n•TREC: Text REtrieval Conference (http://trec.nist.gov/)\\\\n  Stammt aus dem TIPSTER-Programm, das  \\\\n  die Defense Advanced Research Projects Agency (DARPA)\\\\nsponsert.\\\\n•Wurde 1992 zu einer jährlichen Konferenz, mitgesponsert \\\\nvom National Institute of Standards and Technology (NIST)\\\\nund DARPA.\\\\n•Den Teilnehmern wurde zum Trainieren und Testen der \\\\nSysteme Teile einer Standardliste von Dokumenten und  \\\\nTHEMEN (wovon Anfragen abzuleiten sind)  in \\\\nverschiedenen Stadien gegeben.\\\\n•Die Teilnehmer legen die P/R-Werte für den endgültigen \\\\nDokument- und Anfrage-Korpus vor und präsentieren ihre \\\\nErgebnisse bei der Konferenz.\\', \\'27Die TREC-Ziele \\\\n•Schaffen einer gemeinsamen Grundlage für den Vergleich \\\\nverschiedener IR-Techniken.  \\\\n–Gleiche Dokumenten- und Anfrageliste und gleiche \\\\nEvaluierungsmethoden.\\\\n•Teilen von Ressourcen und Erfahrungen bei der \\\\nEntwicklung des Benchmarks.\\\\n–Hauptsponsoring durch die amerikanische Regierung, um große \\\\nBenchmark-Sammlungen zu entwickeln.\\\\n• Förderung der Beteiligung von Industrie und \\\\nWissenschaft.\\\\n• Entwicklung neuer Evaluierungstechniken, besonders für\\\\n neue Anwendungen.\\\\n–Retrieval, Routing/Filtering, nicht-englische Dokumente, web-\\\\nbasierte Sammlung, Fragenbeantwortung (question answering).\\', \\'28TREC: Vorteile\\\\n•Riesige Datensätze (verglichen mit ein paar MB in der SMART \\\\nCollection).\\\\n•Relevanzbeurteilung wird zur Verfügung gestellt.\\\\n•In ständiger Entwicklung mit Unterstützung der U.S.-Regierung.\\\\n•Große Beteiligung:\\\\n–TREC 1: 28 Papers 360 Seiten.\\\\n–TREC 4: 37 Papers 560 Seiten.\\\\n–TREC 7: 61 Papers 600 Seiten. \\\\n–TREC 8: 74 Papers.\\', \\'29TREC-Aufgaben\\\\n•Ad hoc: Neue Fragen werden zu einem statischen Datensatz gestellt. \\\\n•Routing: Die gleichen Fragen werden gestellt, aber neue \\\\nInformationen werden gesucht (neue Zeitungsausschnitte, Library \\\\nProfiling).\\\\n•Neue Aufgaben wurden nach TREC 5 hinzugefügt\\\\n–interaktiv, \\\\n–vielsprachig (multilingual), \\\\n–Verarbeitung natürlicher Sprache, \\\\n–Mischen mehrerer Datenbanken, \\\\n–Filtering, \\\\n–sehr große Korpora (20 GB, 7.5 Millionen Dokumente),\\\\n– Fragenbeantwortung (question answering).\\', \\'30Eigenschaften der TREC-1-Sammlung \\\\n•Sowohl lange als auch kurze Dokumente (von ein \\\\npaar hundert zu mehr als tausend unterschiedlichen \\\\nTermen in einem Dokument).\\\\n•Testdatensätze bestehen aus:  \\\\n    WSJWall Street Journal articles (1986-1992)      550 M \\\\n     AP   Associate Press Newswire (1989)      514 M\\\\n     ZIFFComputer Select Disks (Ziff-Davis Publishing) 493 M \\\\n     FR   Federal Register      469 M \\\\n     DOE Abstracts from Department of Energy reports 190 M  \\', \\'31Musterdokument (mit SGML)\\\\n<DOC> \\\\n<DOCNO> WSJ870324-0001 </DOCNO>  \\\\n<HL> John Blair Is Near Accord To Sell Unit, Sources Say </HL> \\\\n<DD> 03/24/87</DD> \\\\n<SO> WALL STREET JOURNAL (J) </SO>\\\\n<IN> REL TENDER OFFERS, MERGERS, ACQUISITIONS (TNM) \\\\nMARKETING, ADVERTISING (MKT) TELECOMMUNICATIONS, \\\\nBROADCASTING, TELEPHONE, TELEGRAPH (TEL) </IN> \\\\n<DATELINE>  NEW YORK </DATELINE>  \\\\n<TEXT>\\\\n     John Blair &amp; Co. is close to an agreement to sell its TV station advertising \\\\nrepresentation operation and program production unit to an investor group led \\\\nby James  H. Rosenfield, a former CBS Inc. executive, industry sources said. \\\\nIndustry sources put the value of the proposed acquisition at more than $100 \\\\nmillion. ... \\\\n</TEXT> \\\\n</DOC> \\', \"32Musteranforderung (mit SGML)\\\\n<top> \\\\n<head> Tipster Topic Description \\\\n<num> Number: 066 \\\\n<dom> Domain: Science and Technology \\\\n<title> Topic: Natural Language Processing \\\\n<desc> Description: Document will identify a type of natural language processing \\\\ntechnology which is being developed or marketed in the U.S. \\\\n<narr> Narrative: A relevant document will identify a company or institution \\\\ndeveloping or marketing a natural language processing technology, identify \\\\nthe technology, and identify one of more features of the company\\'s product.\\\\n<con> Concept(s):  1. natural language processing ;2. translation, language, \\\\ndictionary\\\\n<fac> Factor(s): \\\\n<nat> Nationality: U.S .</nat>\\\\n</fac> \\\\n<def> Definitions(s): \\\\n</top>\", \\'33TREC-Eigenschaften\\\\n•Sowohl Dokumente als auch Anforderungen \\\\nenthalten viele verschiedene Arten von \\\\nInformationen (Felder).\\\\n•Generierung der formalen Anfragen (Boolesche, \\\\nVektorraum, etc.) liegt in der Verantwortung des \\\\nSystems.\\\\n–Ein System kann sehr gut beim Anfragen und Ranking \\\\nsein, aber wenn es aus der Anforderung dürftige Anfragen \\\\nerzeugt, wird seine endgültige P/R dürftig sein.\\', \\'34Evaluierung \\\\n•Zusammenfassende Tabellenstatistiken : Anzahl \\\\nThemen, Anzahl gefundener Dokumente, Anzahl \\\\nrelevanter Dokumente.\\\\n•Durchschnittlicher Recall-Precision : Durchschnitts- \\\\nPrecision bei 11 Recall-Levels (0 bis 1 in 0.1er-\\\\nSchritten).\\\\n•Dokumentbasierter Durchschnitt : Durchschnitts- \\\\nPrecision wenn 5, 10, .., 100, … 1000 Dokumente \\\\ngefunden werden.\\\\n•Histogramm über die durchschnittliche Precision : \\\\nUnterschied zwischen der R-Precision für jedes \\\\nThema und der durchschnittlichen R-Precision aller \\\\nSysteme zu diesem Thema.\\', \\'35\\\\n\\', \\'36Cystic Fibrosis (CF) Sammlung\\\\n•1.239 Zusammenfassungen von medizinischen \\\\nZeitungsartikeln in CF.\\\\n•100 Informationsanforderungen (Anfragen) in \\\\nForm kompletter englischer Fragen.\\\\n•Relevante Dokumente, die von vier \\\\nMedizinexperten auf einer Skala von 0-2 bestimmt \\\\nund bewertet wurden: \\\\n–0: Nicht relevant.\\\\n–1: Marginal relevant.\\\\n–2: Sehr relevant.\\', \\'37CF Dokumentenfelder\\\\n•MEDLINE Zugangsnummer\\\\n•Autor\\\\n•Titel\\\\n•Quelle\\\\n•Wesentliche Themen\\\\n•Untergeordnete Themen\\\\n•Zusammenfassung (oder Auszug)\\\\n•Verweise auf andere Dokumente\\\\n•Zitate zu diesem Dokument\\', \"38Beispiel CF-Dokument\\\\nAN 74154352\\\\nAU Burnell-R-H.  Robertson-E-F.\\\\nTI Cystic fibrosis in a patient with Kartagener syndrome.\\\\nSO Am-J-Dis-Child. 1974 May. 127(5). P 746-7.\\\\nMJ CYSTIC-FIBROSIS: co.  KARTAGENER-TRIAD: co.\\\\nMN CASE-REPORT.  CHLORIDES: an.  HUMAN.  INFANT.  LUNG: ra.  MALE.\\\\n   SITUS-INVERSUS: co, ra.  SODIUM: an.  SWEAT: an.\\\\nAB A patient exhibited the features of both Kartagener syndrome and\\\\n   cystic fibrosis.  At most, to the authors\\' knowledge, this\\\\n   represents the third such report of the combination.  Cystic\\\\n   fibrosis should be excluded before a diagnosis of Kartagener\\\\n   syndrome is made.\\\\nRF 001   KARTAGENER M          BEITR KLIN TUBERK               83   489 933\\\\n   002   SCHWARZ V             ARCH DIS CHILD                  43   695 968\\\\n   003   MACE JW               CLIN PEDIATR                    10   285 971\\\\n   …\\\\nCT   1   BOCHKOVA DN           GENETIKA (SOVIET GENETICS)      11   154 975\\\\n     2   WOOD RE               AM REV RESPIR DIS              113   833 976\\\\n     3   MOSSBERG B            MT SINAI J MED                  44   837 977\\\\n     …\", \\'39Beispiele für CF-Anfragen\\\\nQN 00002\\\\nQU Can one distinguish between the effects of mucus hypersecretion and\\\\n   infection on the submucosal glands of the respiratory tract in CF?\\\\nNR 00007\\\\nRD  169 1000  434 1001  454 0100  498 1000  499 1000  592 0002  875 1011\\\\nQN 00004\\\\nQU What is the lipid composition of CF respiratory secretions?\\\\nNR 00009\\\\nRD  503 0001  538 0100  539 0100  540 0100  553 0001  604 2222  669 1010\\\\n    711 2122  876 2222\\\\nNR: Anzahl relevanter Dokumente\\\\nRD: Relevante Dokumente\\\\nRatingcode:  Vier 0-2 Ratings, eines von jedem Experten\\', \\'1 Anfrage -Operationen  \\\\nRelevance Feedback &  \\\\nAnfrage -Erweiterung  \\\\nViele Folien in  diesem Abschnitt sind eine deutsche Übersetzung der Folien von  \\\\nRaymond J. Mooney (http://www.cs.utexas.edu/users/mooney/ir -course/).  \\', \\'2 Relevance Feedback  \\\\n•Nachdem die ersten Ergebnisse präsentiert sind, \\\\nbietet das „Relevance Feedback“ dem Benutzer die \\\\nMöglichkeit, Feedback zur Relevanz einzelner oder \\\\nmehrerer Ergebnis -Dokumente zu geben.  \\\\n•Die Feedback -Informationen werden benutzt, um \\\\ndie Anfrage neu zu formulieren und neue \\\\nErgebnisse auf Basis der neuen Anfrage zu \\\\nberechnen.  \\\\n•Dies erlaubt einen interaktiven Prozess, der \\\\nmehrfach durchlaufen werden kann.  \\', \\'3 Relevance -Feedback -Architektur  \\\\nRankings  \\\\nIR \\\\nSystem  Document  \\\\ncorpus  \\\\nRanked  \\\\nDocuments  1. Doc1  \\\\n2. Doc2  \\\\n3. Doc3  \\\\n    . \\\\n    . 1. Doc1  \\\\uf0df \\\\n2. Doc2  \\\\uf0dd \\\\n3. Doc3  \\\\uf0df \\\\n    . \\\\n    . Feedback  Query \\\\nString  \\\\nRevise\\\\nd \\\\nQuery  ReRanked  \\\\nDocuments  \\\\n1. Doc2  \\\\n2. Doc4  \\\\n3. Doc5  \\\\n    . \\\\n    . Query  \\\\nReformulation  \\', \\'4 Veränderung der Anfrage  \\\\n•Änderung der Anfrage, um dem Feedback \\\\nRechnung zu tragen:  \\\\n–Erweiterung der Anfrage (Query Expansion) : Fügt \\\\nTerme aus den relevanten Dokumenten zur Anfrage \\\\nhinzu.  \\\\n–Veränderung der Termgewichte : Erhöht die Gewichte \\\\nvon Termen aus relevanten Dokumenten und reduziert \\\\ndie Gewichte von Termen nicht -relevanter Dokumente.  \\\\n•Verschiedene Algorithmen zum automatischen \\\\nNeuformulieren der Anfrage existieren.  \\', \\'5 Veränderung der Anfrage in KSM  \\\\n•Ändere den Anfragevektor mit Methoden der \\\\nLinearen Algebra.  \\\\n•Addiere  die Vektoren der relevanten  Dokumente \\\\nzu dem Anfragevektor.  \\\\n•Subtrahiere  die Vektoren der irrelevanten  \\\\nDokumente vom Anfragevektor.  \\\\n•Dadurch werden sowohl positiv als auch negativ \\\\ngewichtete Terme der Anfrage hinzugefügt. Die \\\\nursprünglichen Termgewichte können ebenfalls \\\\nmodifiziert werden.  \\', \\'6 Optimale Anfrage  \\\\n•Angenommen, die Menge Cr von relevanten \\\\nDokumenten wäre bekannt.  \\\\n•Dann wäre die beste Anfrage, die die \\\\nrelevanten Dokumente an die Spitze stellt:  \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n•Die Menge Cr ist aber nicht bekannt!  \\\\n\\\\uf0e5 \\\\uf0e5\\\\n\\\\uf0cf \\\\uf0ce\\\\uf02d\\\\uf02d \\\\uf03d\\\\nr j r j Cdj\\\\nr Cdj\\\\nropt dCNdCq\\\\n\\\\uf072 \\\\uf072\\\\uf072 \\\\uf072 \\\\uf072 1 1N = Gesamtzahl der Dokumente  \\', \\'7 Standard Rocchio -Methode  \\\\n•Wir ändern die ursprüngliche Anfrage q mit \\\\nWissen über die bekannten Mengen von \\\\nrelevanten ( Dr) und irrelevanten ( Dn) \\\\nDokumenten . \\\\n\\\\uf0e5\\\\uf0e5\\\\n\\\\uf0ce \\\\uf0ce\\\\uf02d \\\\uf02b\\\\uf03d\\\\nn j r j Ddj\\\\nn Ddj\\\\nrm dDdDq q\\\\n\\\\uf072 \\\\uf072\\\\uf072 \\\\uf072 \\\\uf072\\\\uf072 \\\\uf067 \\\\uf062\\\\uf061\\\\n\\\\uf061: Gewicht für ursprüngliche Anfrage.  \\\\n\\\\uf062: Gewicht für relevante Dokumente.  \\\\n\\\\uf067: Gewicht für irrelevante Dokumente.  \\', \\'8 Ide-Regular -Methode  \\\\n•Da mehr Feedback den Grad der Neuformu -\\\\nlierung erhöhen sollte, normalisiert Ide \\\\nRegular nicht den Betrag des Feedbacks.  \\\\n \\\\n\\\\uf0e5\\\\uf0e5\\\\n\\\\uf0ce \\\\uf0ce\\\\uf02d\\\\uf02b\\\\uf03d\\\\nn j r j Ddj\\\\nDdj m d d q q\\\\n\\\\uf072 \\\\uf072\\\\uf072 \\\\uf072\\\\uf072\\\\uf072\\\\uf067\\\\uf062\\\\uf061\\\\n\\\\uf061: Gewicht für ursprüngliche Anfrage.  \\\\n\\\\uf062: Gewicht für relevante Dokumente.  \\\\n\\\\uf067: Gewicht für irrelevante Dokumente.  \\', \\'9 Ide-“Dec Hi” -Methode  \\\\n•Beeinflusst in Richtung Ablehnung nur die \\\\nam höchsten gerankten der irrelevanten \\\\nDokumente:  \\\\n)( maxj relevant non\\\\nDdj m d d q q\\\\nr j\\\\uf072 \\\\uf072\\\\uf072\\\\uf072\\\\n\\\\uf072\\\\uf02d\\\\n\\\\uf0ce\\\\uf02d\\\\uf02b\\\\uf03d\\\\uf0e5\\\\uf067\\\\uf062\\\\uf061\\\\n\\\\uf061: Gewicht für ursprüngliche Anfrage.  \\\\n\\\\uf062: Gewicht für relevante Dokumente.  \\\\n\\\\uf067: Gewicht für irrelevante Dokumente.  \\', \\'10 Vergleich der Methoden  \\\\n•Vergleichende experimentelle Ergebnisse \\\\nzeigen keine klare Präferenz für eine der \\\\no.g. Methoden.  \\\\n•Alle Methoden mit Feedback verbessern im \\\\nallgemeinen die Retrieval -Performanz \\\\n(Recall & Precision).  \\\\n•Meist setzt man die Gewichte \\\\uf061, \\\\uf062, \\\\uf067  \\\\njeweils auf 1.  \\', \\'11 Evaluierung Relevance Feedback  \\\\n•Durch ihre Konstruktion wird eine neu formulierte \\\\nAnfrage explizit relevant gekennzeichnete Dokumente \\\\nhöher einstufen und explizit gekennzeichnete irrelevante \\\\nDokumente niedriger.  \\\\n•Eine Verbesserung bei diesen Dokumenten sollte nicht \\\\npositiv bewertet werden, da diese Information nicht vom \\\\nSystem erzeugt wurde.  \\\\n•Im Maschinellen Lernen/KDD wird dieser Fehler als \\\\n“Testen mit den Trainingsdaten” bezeichnet.  \\\\n•Die Evaluierung sollte nur die nicht bereits vom Benutzer \\\\nbewerteten Dokumente berücksichtigen.  \\', \\'12 Faire Evaluierung von Relevance \\\\nFeedback  \\\\n•Entferne alle Dokumente aus dem Korpus, für die  \\\\nFeedback geliefert wurde.  \\\\n•Messe Recall/Precision der verbleibenden \\\\nDokumentensammlung.  \\\\n•Verglichen mit einem kompletten Korpus können die \\\\nabsoluten Anzahlen von Recall/Precision abnehmen, da \\\\nrelevante Dokumente entfernt wurden.  \\\\n•Jedoch liefert die relative  Performance auf den \\\\nverbleibenden Dokumenten faire Information über die \\\\nEffektivität des Relevance Feedback.  \\', \\'13 Warum wird Feedback nicht oft \\\\nangewandt?  \\\\n•Die Anwender zögern manchmal, explizites \\\\nFeedback zu liefern.  \\\\n•Rel. Feedback produziert lange Anfragen, \\\\ndie mehr Rechenzeit benötigen. Suchma -\\\\nschinen verarbeiten oft viele Anfragen und \\\\nerlauben daher wenig Zeit für jede einzelne.   \\\\n•Rel. Feedback macht es schwieriger zu \\\\nverstehen, warum ein spezielles Dokument \\\\ngefunden wurde.  \\\\n \\', \\'14 Pseudo -Feedback  \\\\n•Verwendet Relevance -Feedback -Methoden \\\\nohne expliziten Anwender -Input.  \\\\n•Nimmt an, dass die ersten m gefundenen \\\\nDokumente  relevant sind, und verwendet \\\\nsie, um die Anfrage neu zu formulieren.  \\\\n•Erlaubt eine Anfrageerweiterung, die Terme \\\\numfasst, die in Korrelation zu den \\\\nursprünglichen Anfragetermen stehen.   \\', \\'15 Pseudo -Feedback -Architektur  \\\\nRankings  \\\\nIR \\\\nSystem  Document  \\\\ncorpus  \\\\nRanked  \\\\nDocuments  1. Doc1  \\\\n2. Doc2  \\\\n3. Doc3  \\\\n    . \\\\n    . Query \\\\nString  \\\\nRevise\\\\nd \\\\nQuery  ReRanked  \\\\nDocuments  \\\\n1. Doc2  \\\\n2. Doc4  \\\\n3. Doc5  \\\\n    . \\\\n    . Query  \\\\nReformulation  \\\\n1. Doc1  \\\\uf0dd \\\\n2. Doc2  \\\\uf0dd \\\\n3. Doc3  \\\\uf0dd \\\\n    . \\\\n    . Pseudo  \\\\nFeedback  \\', \\'16 Pseudo -Feedback -Ergebnisse  \\\\n•Pseudo -Feedback hat die Performanz in \\\\nTREC -Wettbewerben für ad -hoc Retrieval \\\\nverbessert.  \\\\n•Arbeitet noch besser, wenn die besten \\\\nDokumente zusätzlichen Booleschen \\\\nBedingungen entsprechen müssen, um im \\\\nFeedback verwendet zu werden.  \\', \\'17 Thesaurus  \\\\n•Ein Thesaurus liefert Informationen zu  \\\\nSynonymen und semantisch verwandten \\\\nWörtern und Phrasen.  \\\\n•Beispiel:  \\\\n    physician  \\\\n    syn: ||croaker, doc, doctor, MD, \\\\nmedical, mediciner, medico, ||sawbones  \\\\n    rel: medic, general practitioner, \\\\nsurgeon,  \\\\n \\\\n\\', \\'18 Thesaurus -basierte Anfrage -Erweiterung  \\\\n•Erweitere jede Anfrage für jeden in ihr \\\\nenthaltenen Term t mit Synonymen und \\\\nverwandten Wörtern von t aus dem Thesaurus.  \\\\n•Kann hinzugefügte Terme niedriger gewichten als \\\\nursprüngliche Anfrageterme.  \\\\n•Erhöht im Allgemeinen den Recall.  \\\\n•Kann die Precision signifikant mindern, besonders \\\\nbei mehrdeutigen Termen.  \\\\n–“interest rate” \\\\uf0ae “interest rate fascinate evaluate”  \\', \\'19 WordNet  \\\\n•Eine lexikalische Datenbank für Englisch.  \\\\n•Entwickelt von dem berühmten Kognitions -\\\\nPsychologen George Miller und einem \\\\nTeam an der Princeton University.  \\\\n•Enthält ca. 144.000 englische Nomina, \\\\nAdjektive, Verben, und Adverben, die in ca. \\\\n109.000 Mengen von Synonymen gruppiert \\\\nsind, die als synsets  bezeichnet werden.  \\', \\'20 Synset -Beziehungen in WordNet  \\\\n•Antonym : front \\\\uf0ae back  \\\\n•Attribute : benevolence \\\\uf0ae good (Nomen zu Adjektiv)  \\\\n•Pertainym : alphabetical \\\\uf0ae alphabet (Adjektiv zu Nomen)  \\\\n•Similar : unquestioning \\\\uf0ae absolute  \\\\n•Cause : kill \\\\uf0ae die \\\\n•Entailment : breathe \\\\uf0ae inhale  \\\\n•Holonym : chapter \\\\uf0ae text (Teil -von) \\\\n•Meronym : computer \\\\uf0ae cpu (Ganzes -von) \\\\n•Hyponym: tree \\\\uf0ae plant (Spezialisierung)  \\\\n•Hypernym:  fruit \\\\uf0ae apple (Verallgemeinerung)  \\', \\'21 WordNet -basierte Anfrage -Erweiterung  \\\\n•Füge Synonyme aus dem gleichen Synset \\\\nhinzu.  \\\\n•Füge Hyponyme hinzu, um speziellere \\\\nTerme zu ergänzen.  \\\\n•Füge Hypernyme hinzu, um eine Anfrage \\\\nzu verallgemeinern.  \\\\n•Füge weitere verwandte Terme hinzu, um \\\\ndie Anfrage zu erweitern.  \\', \\'22 Statistischer Thesaurus  \\\\n•Manuell entwickelte Thesauri sind nicht in \\\\nallen Sprachen verfügbar.  \\\\n•Manuell erstellte Thesuari sind vom Typ \\\\nund vom Umfang der Synonymität \\\\nbegrenzt, sowie in den semantischen \\\\nBeziehungen, die sie darstellen.  \\\\n•Semantisch verwandte Terme können \\\\nalternativ mit statistischen Korpus -Analysen \\\\nentdeckt werden.  \\', \\'23 Automatische Globalanalyse  \\\\n•Bestimme die Termähnlichkeit durch eine \\\\nvorberechnete statistische Analyse des \\\\nkompletten Korpus.  \\\\n•Berechne Assoziationsmatrizen, die die \\\\nKorrelation von Termen durch die \\\\nHäufigkeit ihres gemeinsamen Auftretens \\\\nquantifizieren.  \\\\n•Erweitere Anfragen mit den statistisch \\\\nähnlichsten Termen.  \\', \\'24 Assoziations -Matrix  \\\\nw1  w2  w3 …………………..wn \\\\nw1 \\\\nw2 \\\\nw3 \\\\n. \\\\n. \\\\nwn c11  c12  c13…………………c1n \\\\nc21 \\\\nc31 \\\\n. \\\\n. \\\\ncn1 \\\\ncij: Korrelationsfaktor zwischen Term  i und Term j \\\\n\\\\uf0e5\\\\n\\\\uf0ce\\\\uf0b4\\\\uf03d\\\\nDdjk ik ij\\\\nkff c\\\\nfik : Häufigkeit von Term i in Dokument k  \\', \\'25 Normalisierte Assoziations -Matrix  \\\\n•Ein auf Häufigkeit basierter Korrelations -\\\\nfaktor begünstigt häufigere Terme.  \\\\n•Normalisierter Assoziations -Score:  \\\\n \\\\n \\\\n•Normalisierter Score ist 1, wenn zwei \\\\nTerme in allen Dokumenten die gleiche \\\\nHäufigkeit besitzen.  \\\\nij jj iiij\\\\nijccccs\\\\uf02d\\\\uf02b\\\\uf03d\\', \\'26 Metrische Korrelations -Matrix  \\\\n•Die Assoziations -Korrelation berücksichtigt nicht die Nähe \\\\nder Terme in den Dokumenten, sondern nur die Häufigkeit \\\\ndes gemeinsamen Auftretens  in Dokumenten.  \\\\n•Die metrische Korrelation berücksichtigt auch die Termnähe.  \\\\n\\\\uf0e5\\\\uf0e5\\\\n\\\\uf0ce\\\\uf0ce\\\\uf03d\\\\ni u j vVk Vk v uijkkrc),(1\\\\nVi:  Menge aller V orkommen von Term i im Korpus.  \\\\nr(ku,kv): Wortabstände zwischen Wortvorkommen ku und kv \\\\n  (= \\\\uf0a5, falls  ku und kv  in verschiedenen Dokumenten vorkommen) . \\', \\'27 Normalisierte metrische Korrelations -\\\\nMatrix  \\\\n•Normalisierter Score zur Berücksichtigung \\\\nvon Termhäufigkeiten:  \\\\nj iij\\\\nijVVcs\\\\n\\\\uf0b4\\\\uf03d\\', \\'28 Anfrageerweiterung mit Korrelations - \\\\nMatrizen  \\\\n•Für jeden Term i in der Anfrage erweitere \\\\ndie Anfrage mit den  n Termen j, die den \\\\nhöchsten Wert von cij (oder sij) haben.  \\\\n•Dies fügt semantisch verwandte Terme aus \\\\nder “Nachbarschaft” der Anfrageterme \\\\nhinzu.  \\', \\'29 Probleme mit der Globalanalyse  \\\\n•Termmehrdeutigkeit kann irrelevante, \\\\nstatistisch korrelierte Terme einbeziehen.  \\\\n–“Apple computer” \\\\uf0ae “Apple red fruit computer”  \\\\n•Da diese Terme konstruktionsbedingt hoch \\\\nkorreliert sind, kann es sein, dass die \\\\nErweiterung keine zusätzlichen Dokumente \\\\nfindet.  \\\\n \\', \\'30 Automatische Lokalanalyse  \\\\n•Bestimme  zur Anfragezeit auf dynamische Weise \\\\ndie ähnlichen Terme:  \\\\n•Basiere für jede spezifische Anfrage die \\\\nKorrelationsanalyse nur auf der “lokalen” Menge \\\\nder am besten bewerteten Dokumente zu dieser \\\\nAnfrage.  \\\\n•Vermeide Mehrdeutigkeiten durch das Bestimmen \\\\nvon ähnlichen (korrelierten) Termen nur innerhalb \\\\nder relevanten Dokumente.  \\\\n–“Apple computer” \\\\uf0ae                                                          \\\\n“Apple computer Powerbook laptop”  \\\\n \\', \\'31 Global - versus Lokalanalyse  \\\\n•Die Globalanalyse erfordert nur einmal \\\\nbeim Aufbau des Systems eine aufwändige \\\\nTermkorrelations -Berechnung.  \\\\n•Die Lokalanalyse erfordert eine intensive \\\\nTermkorrelations -Berechnung zur Laufzeit \\\\nbei jeder Anfrage (obwohl die Anzahl der \\\\nTerme geringer ist als bei der Global -\\\\nanalyse).  \\\\n•Aber Lokalanalysen liefern bessere \\\\nErgebnisse.  \\', \\'32 Globalanalyse -Verbesserungen  \\\\n•Erweitere Anfragen nur mit den Termen, die allen \\\\nTermen in der Anfrage ähnlich sind.  \\\\n \\\\n \\\\n \\\\n–ergänze “fruit” nicht zu “Apple computer” da dies weit \\\\nvon Computer “computer” entfernt ist.  \\\\n–“fruit” wird ergänzt zu “apple pie” da “fruit” da sowohl \\\\nnahe an “apple” als auch an “pie”.  \\\\n•Verwende bei der Berechnung von Term -\\\\nkorrelationen weiterentwickelte Termgewichte \\\\n(anstatt nur Häufigkeit).  \\\\n\\\\uf0e5\\\\n\\\\uf0ce\\\\uf03d\\\\nQkij i\\\\njc Qksim ),(\\', \\'33 Schlussfolgerungen zur Anfrageerweiterung  \\\\n•Anfrageerweiterungen mit verwandten \\\\nTermen können die Performance – \\\\nbesonders den Recall – verbessern.  \\\\n•Jedoch müssen ähnliche Terme sehr \\\\nvorsichtig ausgewählt werden, um Probleme \\\\nwie geringere Precision zu vermeiden.  \\', \\'1  \\\\n \\\\nKapitel 6  \\\\nWeitere Anfrage -Paradigmen  \\\\n \\\\n \\\\nViele Folien in  diesem Abschnitt sind eine deutsche Übersetzung der Folien von  \\\\nRaymond J. Mooney (http://www.cs.utexas.edu/users/mooney/ir -course/). Ein \\\\nweiterer großer Anteil wurde mit freundlicher Genehmigung von Peter Becker \\\\nübernommen (http://www2.inf.fh -rhein -sieg.de/~pbecke2m/retrieval/).  \\', \\'2 Übersicht  \\\\n•Boolesche Anfragen  \\\\n•Näherungsanfragen  \\\\n•String -Matching  \\\\n•Reguläre Ausdrücke  \\\\n•Approximatives String -Matching  \\\\n•Phonetische Suche  \\\\n \\', \\'3 Boolesche Anfragen  \\\\n•Schlüsselwörter kombiniert mit Booleschen \\\\nOperatoren:  \\\\n–ODER:  ( e1 OR e2) \\\\n–UND: ( e1 AND e2) \\\\n–AUSSER: ( e1 BUT e2) erfüllt e1 , aber nicht  e2 . \\\\n•Negation wird nur eingeschränkt als AUSSER \\\\nerlaubt, um den invertierten Index nutzen zu \\\\nkönnen: effektives Bearbeiten von e1 durch inv. \\\\nIndex, dann Filtern der Ergebnisse.  \\\\n•Problem: Untrainierte Anwender haben Probleme \\\\nmit Boolescher Logik.  \\', \\'4 Boolesches Retrieval mit invertierten Indizes  \\\\n•Einzelnes Schlüsselwort : Finde die gesuchten \\\\nDokumente durch Verwendung des invertierten \\\\nIndex.  \\\\n•ODER :  Finde auf rekursive Weise e1 und e2 bilde \\\\ndie Vereinigung der Ergebnisse.  \\\\n•UND : Finde auf rekursive Weise e1 und e2 und \\\\nbilde den Durchschnitt der Ergebnisse.  \\\\n•AUSSER : Finde auf rekursive Weise e1 und \\\\nlösche dann alle Ergebnisse, auf die e2 zutrifft.  \\\\n \\', \\'5 “Natürlichsprachige” Anfragen  \\\\n•Volltext -Anfragen als beliebige Strings.  \\\\n•Typischerweise werden sie nur wie ein “bag \\\\nof words” im Vektorraum -Modell behandelt \\\\nund mit Standard -Vektor -Retrieval -\\\\nMethoden verarbeitet.  \\', \\'6 Phrasen -Anfragen  \\\\n•Findet  Dokumente , die spezifische  Phrasen  \\\\n(geordnete  Listen zusammenhängender  \\\\nWörter ) enthalten  \\\\n–“information theory”  \\\\n•Kann  intervenierende  Stopwörter  und/oder \\\\nStemming zulassen . \\\\n–“buy camera” matches:                                     \\\\n   “buy a camera”                                                 \\\\n   “buying the cameras”                                                \\\\n     etc. \\\\n \\', \\'7 Phrasen -Retrieval mit invertierten Indizes  \\\\n•Muss einen invertierten Index verwenden, der \\\\nauch die Positionen  der Schlüsselworte in den \\\\nDokumenten speichert.  \\\\n•Finde Dokumente und Positionen für jedes \\\\nindividuelle Wort, bilde Durchschnitt der \\\\nTreffermengen, und prüfe abschließend auf den \\\\nrichtigen Zusammenhang der Schlüsselwort -\\\\nPositionen.  \\\\n•Am besten beginnt man die Prüfung der \\\\nZusammenhänge mit dem am wenigsten häufigen \\\\nWort in der Phrase.  \\\\n \\', \\'8 Phrasensuche  \\\\nAufgabe: Gegeben sei die Menge D von Dokumenten, in der alle Schlüsselwörter  \\\\nk1…km vorkommen. (Diese Menge wird vorher per UND -Anfrage bestimmt.) \\\\nGesucht ist die Teilmenge R von D, in der diese Schlüsselwörter in einer Phrase \\\\n(k1…km) vorkommen.  \\\\n \\\\nIntitialisiere eine leere Menge R von gefundenen Dokumenten.  \\\\nFür jedes Dokument d in D: \\\\n    Suche  für jedes ki die Folge der Positionen Pi  der Vorkommen   in d \\\\n     Bestimme die kürzeste Folge Ps der Pi‟s \\\\n     Für jede Position p des Schlüsselworts ks in Ps \\\\n          Für jedes Schlüsselwort ki außer ks \\\\n               Verwende die binäre Suche, um eine Position ( p – s + i) in der  \\\\n Folge Pi zu finden.  \\\\n          Falls für jedes Schlüsselwort die korrekte Position gefunden wurde, füge d zu     \\\\n R hinzu  \\\\nGib  R aus. \\', \\'9 Näherungsanfragen  \\\\n•Liste von Termen mit zusätzlichen \\\\nmaximalen Abstandsbeschränkungen  \\\\nzwischen den Termen.  \\\\n•Beispiel: “dogs” und “race” in 4 Wörtern           \\\\npasst zu “…dogs will begin the race…”  \\\\n•Man kann zusätzlich Stemming verwenden \\\\nund/oder Stopwörter ignorieren.  \\', \\'10 Näherungs -Retrieval mit invertiertem Index  \\\\n•Benutze Ansatz ähnlich wie bei \\\\nPhrasensuche, um alle Dokumente zu \\\\nfinden, die die Schlüsselwörter enthalten.  \\\\n•Finde bei der binären Suche nach den \\\\nPositionen der verbleibenden \\\\nSchlüsselwörter die nächste Position von ki \\\\nnach p und überprüfe, dass diese innerhalb \\\\ndes maximal zulässigen Abstands ist.  \\\\n \\', \\'11 String -Matching  \\\\n•Erlaubt allgemeinere Stringvergleiche in \\\\nAnfragen, während der vorher diskutierte \\\\nAnsatz die Wörter als atomare Einheiten \\\\n(Tokens) betrachtet.  \\\\n•Erfordert für effiziente Bearbeitung \\\\nkomplexere Datenstrukturen und \\\\nAlgorithmen als invertierte Indizes.  \\', \\'12 String -Matching  \\\\n•Die Suche von einem Muster in einem Text \\\\nwird auch als String Matching oder Pattern \\\\nMatching bezeichnet.  \\\\n•Generell besteht die Aufgabe darin, einen \\\\nString (das Muster, Pattern ) der Länge m in \\\\neinem Text der Länge n zu finden, wobei  \\\\nn > m gilt. \\\\n•Je nach Freiheitsgraden bei der Suche \\\\nunterscheidet man verschiedene Arten von \\\\nString -Matching -Problemen.  \\\\n \\', \\'13 Arten von String -Matching -Problemen  \\\\n•Exaktes String -Matching: Wo tritt ein String pat \\\\nin einem Text text auf? Beispiel: fgrep  \\\\n•Matching von Wortmengen: Gegeben sei eine \\\\nMenge S von Strings. Wo tritt in einem Text ein \\\\nString aus S auf? Beispiel: agrep  \\\\n•Matching regulärer Ausdrücke: Welche Stellen \\\\nin einem Text passen auf einen regulären \\\\nAusdruck? Beispiel: grep, egrep  \\\\n•Approximatives String -Matching: Welche \\\\nStellen in einem Text passen am besten auf ein \\\\nMuster (Best -Match -Anfrage)?  \\\\n \\', \\'14 Arten von String -Matching -Problemen  \\\\n•Welche Stellen in einem Text stimmen mit \\\\neinem Muster bis auf d Fehler überein \\\\n(Distance -Match -Anfrage)? Beispiel: agrep  \\\\n•Editierdistanz: Wie kann man am \\\\n“günstigsten” einen String s in einen String \\\\nt überführen? Beispiel: diff  \\\\n \\', \\'15 Einfache Strukturen  \\\\n•Prefix : Struktur, die zum Wortanfang passt.  \\\\n–“anti” passt zu “antiquity”, “antibody”, etc.  \\\\n•Suffix : Struktur, die  zum Wortende passt:  \\\\n–“ix” passt zu “fix”, “matrix”, etc.  \\\\n•Substring : Struktur, die zu einer willkürlichen \\\\nTeilfolge von Zeichen passt.  \\\\n– “rapt” passt zu “enrapture”, “velociraptor” etc.  \\\\n•Range : Stringpaar, das zu jedem Wort passt, das \\\\nlexikographisch (alphabetisch) dazwischen liegt.  \\\\n–“tin” to “tix” passt zu “tip”, “tire”, “title”, etc.  \\\\n \\\\n \\', \\'16 Anwendungen  \\\\n•Wo braucht man String -Matching -\\\\nVerfahren? Z.B.:  \\\\n–Volltextdatenbanken  \\\\n–Retrievalsysteme  \\\\n–Suchmaschinen  \\\\n–Bioinformatik  \\\\n•In diesem Abschnitt lernen wir effiziente \\\\nAlgorithmen für String -Matching kennen.  \\\\n \\', \\'17 Bezeichnungen  \\\\n•Ein Alphabet  ist eine endliche Menge \\\\uf053 von \\\\nSymbolen. | \\\\uf053| bezeichnet die Kardinalität von \\\\uf053. \\\\n• Ein String (Zeichenkette,Wort) s  über einem \\\\nAlphabet  ist eine endliche Folge von Symbolen \\\\naus \\\\uf053. |s| bezeichnet die Länge von s. \\\\n• \\\\uf065 bezeichnet den leeren String . \\\\n• Wenn x und y Strings sind, dann bezeichnet xy die \\\\nKonkatenation von x und y. \\\\n \\', \\'18 Bezeichnungen  \\\\n•s[i] bezeichnet das i-te Element eines Strings s (1 ·  i \\\\n· |s|). \\\\n•s[i... j] bezeichnet den String s[i]s[i + 1]...s[j].  \\\\n•Für i > j gelte s[i... j] =\\\\uf065   \\\\n•Für zwei Strings x und y gilt x = y genau dann, wenn \\\\n|x| = |y| = m und x[i] = y[i] für alle 1 · i · m gilt. \\\\n•Wenn w = xyz ein String ist, dann ist x ein Präfix und \\\\nz ein Suffix von w. \\\\n•Gilt w\\\\uf0b9 x (w \\\\uf0b9 z), dann ist x (z) ein echter Präfix \\\\n(echter Suffix) von w. \\', \\'19 •Ein String x (mit m = |x|) heißt Teil-String \\\\n(Faktor) von y, wenn ein i existiert mit x = \\\\ny[i...i+m-1]. Andere Sprechweisen: x tritt in \\\\ny an Position i auf bzw. Position i ist ein \\\\nMatch für x in y. \\\\n \\', \\'20 Exaktes String -Matching  \\\\n•Problem 6.1. [Exaktes String -Matching] \\\\nGegeben sind die Strings pat und text. \\\\n(a) Man bestimme, ob pat ein Teil -String von text ist. \\\\n(b) Man bestimme die Menge aller Positionen, an denen \\\\npat in text auftritt. Diese Menge wird mit MATCH( pat; \\\\ntext) bezeichnet.  \\\\n•Im folgenden wird nur die Variante (a) von \\\\nProblem 6.1 betrachtet.  \\\\n•Algorithmen für die Variante (b) erhält man durch \\\\neinfache Modifikationen der Algorithmen für (a).  \\\\n•Im folgenden sei m = |pat| und n = |text|. \\\\n \\\\n \\', \\'21 Naiver Ansatz  \\\\n•Der naive Ansatz besteht darin, für jede Position von text (bzw. \\\\nsolange pat ab der aktuellen Position in text passt) von neuem zu \\\\ntesten, ob pat an dieser Position auftritt.  \\\\n•Das allgemeine Schema für solch einen naiven Algorithmus lautet:  \\\\n \\\\n•for i := 1 to n-m+1 do \\\\n  man prüfe, ob pat = text[i...i+m-1] gilt  \\\\n \\\\n•Die Prüfung kann nun “von links nach rechts” oder “von rechts nach \\\\nlinks” erfolgen.  \\\\n•Dies führt zu unterschiedlichen naiven Algorithmen und darauf \\\\naufbauend zu unterschiedlichen Ansätzen der Verbesserung:  \\\\n–„von links nach rechts“  \\\\uf0e0  Algorithmus von Morris und Pratt  \\\\n–„von rechts nach links“  \\\\uf0e0  Algorithmus von Boyer und Moore  \\\\n•Wir betrachten hier nur die zweite Variante.  \\', \\'22 Naiver Algorithmus von rechts nach links  \\\\n•Der Algorithmus von Boyer und Moore kann als eine \\\\nverbesserte Variante eines naiven String -Matching -\\\\nAlgorithmus angesehen werden, bei dem pat mit text von \\\\nrechts nach links verglichen wird.  \\\\n \\\\n•Algorithmus 6.1  [naives String -Matching von rechts \\\\nnach links]  \\\\ni := 1 \\\\nwhile i · n-m+1 do \\\\nj := m \\\\nwhile j ¸ 1 and pat[j] = text[i+ j-1] do j := j-1 end \\\\nif j = 0 then return true  \\\\ni := i + 1 \\\\nend \\\\nreturn false  \\', \\'23 Analyse des naiven Algorithmus  \\\\n•Satz 6.1. Der naive Algorithmus 6.1 löst Problem 6.1 \\\\nin Zeit O(nm) und Platz O(m). \\\\n \\\\n•Für pat = bam-1 und text = an benötigt Algorithmus 6.1 \\\\n(n-m+1)m = nm - m2 + m  Zeichenvergleiche. (Dies ist \\\\nder ‚worst case„.)  \\\\n \\\\n•Bei einem binären Alphabet und zufällig erzeugten \\\\npat und text (jedes Zeichen unabhängig und jedes \\\\nSymbol mit Wahrscheinlichkeit 1/2) ergibt sich für \\\\ndie durchschnittliche Anzahl an Zeichenvergleichen: \\\\n(2-2-m)n + O(1)  \\', \\'24 Analyse des naiven Algorithmus  \\\\n•Trotz der im Durchschnitt linearen Laufzeit \\\\nlohnt sich der Einsatz von “besseren” \\\\nString -Matching -Algorithmen, denn:  \\\\n–die nachfolgenden String -Matching -\\\\nAlgorithmen haben sich nicht nur in der \\\\nTheorie, sondern auch in der Praxis als \\\\nerheblich effizienter erwiesen, und  \\\\n–die Verteilung der Buchstaben des Alphabets \\\\nüber den Text ist in der Regel nicht statistisch \\\\nunabhängig.  \\\\n \\', \\'25 Der Algorithmus von Boyer und Moore  \\\\nDer Algorithmus von Boyer und Moore basiert auf der \\\\nfolgenden Überlegung:  \\\\n \\\\n•Tritt in Algorithmus 6.1 an Stelle j ein Mismatch  auf und \\\\nkommt pat[j+1...m], so wird pat gleich soweit nach rechts \\\\ngeschoben, bis ein Teilwort des Musters pat[j+1-s...m-s] \\\\nwieder auf  pat[j+1...m] passt.  \\\\n \\\\n•(Es muss der Fall abgefangen werden, dass j+1-s < 1.)  \\\\n \\', \\'26 Veranschaulichung  \\\\n\\', \\'27 Veranschaulichung  \\\\n•Kommt es in Algorithmus 6.1 an Stelle j von pat \\\\nzu einem Mismatch, so gilt  \\\\n pat[j+1...m] = text[i+j ... i+m-1] und  \\\\n pat[j] \\\\uf0b9  text[i+j -1]. \\\\n \\\\n•Dies kann wie folgt ausgenutzt werden: \\\\nAngenommen, pat tritt in text an einer Position  \\\\n i+s  mit  i < i+s < i+m  auf. Dann müssen die \\\\nbeiden folgenden Bedingungen gelten:  \\\\n (BM1)   8 j < k · m:   k · s  Ç  pat[k-s] = pat[k] \\\\n (BM2)   s < j ) pat[j-s]  \\\\uf0b9 pat[j] \\\\n \\', \\'28 Veranschaulichung  \\\\nFazit:  Wir brauchen also erst wieder bei einer Stelle zu testen, bei der die beiden Bedingungen gelten. \\\\nDamit man keinen Match verpasst, muss s möglichst klein gewählt werden.  \\', \\'29 \\\\nVeranschaulichung  \\\\nWdh.: Angenommen, es gibt einen Mismatch an Stelle j von pat, und pat tritt in \\\\ntext an einer Position  i+s  mit  i < i+s < i+m  auf. Dann gelten die beiden \\\\nfolgenden Bedingungen:  \\\\n (BM1)   8 j < k · m:   k · s  Ç  pat[k-s] = pat[k] \\\\n (BM2)   s < j !   pat[j-s]  \\\\uf0b9 pat[j] i \\', \\'30 Forts. Boyer -Moore -Algorithmus  \\\\n•Die entsprechenden Werte werden in einer \\\\nPreprocessingphase ermittelt und in der Shift-Tabelle D  \\\\nabgelegt. (Hierzu kann wiederum eine Variante des Boyer -\\\\nMoore -Algorithmus genutzt werden.)  \\\\n \\\\nD[j] := mins>0{s |  (BM1) und (BM2) gilt für j und s} \\\\n \\\\n•Der Algorithmus von Boyer und Moore verwendet nun im \\\\nFalle eines Mismatches an Position j den in D[j] \\\\nabgelegten Wert, um pat nach rechts zu verschieben.  \\\\n \\', \\'31 Algorithmus 6.2   [Algorithmus von Boyer und Moore]  \\\\ni := 1 \\\\nwhile i ≤ n – m + 1 do \\\\n j := m \\\\n while j ≥ 1 and pat[j] = text[ i + j – 1] do  \\\\n j := j – 1 end \\\\n if j = 0 then return true  \\\\n i := i + D[j] \\\\nend \\\\nreturn false  \\\\n \\', \\'32 Beispiel  \\\\nDefinition 6.1 Der n -te Fibonacci -String Fn (n ¸ 0) ist wie \\\\nfolgt definiert:  \\\\n• F0 = ε \\\\n• F1 = b \\\\n• F2 = a \\\\n• Fn = F n – 1Fn – 2 für n > 2  \\\\n \\\\n \\\\n•D[j] lautet für F 7: \\\\nj           1   2   3   4   5   6   7   8   9 10  11  12 13  \\\\npat[j]    a   b   a   a   b   a   b   a   a   b   a    a    b  \\\\nD[j]      8   8   8   8   8   8   8   3 11 11   6   13  1  \\\\n \\', \\'33 Weitere Verbesserung von Boyer -Moore  \\\\nAlgorithmus 6.2 kann noch weiter verbessert werden: Tritt an Stelle m \\\\nvon pat (also bereits beim ersten Vergleich) ein Mismatch  auf, so wird  \\\\npat momentan nur nur um soviele  Stellen nach rechts verschoben,  \\\\nwie der letzte Buchstabe des Wortes am Wortende vorkommt.  \\\\n \\\\nEs sei  \\\\n                                 last[c] := max 1≤j ≤ m {j| pat[j] = c}  \\\\nund last[c] := 0 falls c nicht in pat auftritt.  \\\\nlast[c] gibt für ein c \\\\uf0ce Σ die jeweils letzte Position von c in pat an. Kommt es nun \\\\nan Stelle j zu einem Mismatch , kann statt  \\\\n       i := i + D[j] \\\\ndie Anweisung  \\\\n          i := i + max(D[j]; j – last[text[i + j – 1]]) \\\\nverwendet werden. Damit ergeben sich noch größere Verschiebungen.  \\', \\'34 Bemerkungen  \\\\n•Verschiebungen der Länge j – last[text[i+j – 1]] heißen \\\\nOccurrence -Shift. \\\\n \\\\n•Wird nur der Occurence -Shift verwendet, d.h. die \\\\nVerschiebe -Anweisung lautet  \\\\n  i := i + max(1, j – last[text[i + j – 1]]) \\\\nso spricht man von einem vereinfachten Boyer -Moore -\\\\nAlgorithmus . \\\\n \\\\n• DieWorst -Case -Laufzeit des vereinfachten Boyer -Moore -\\\\nAlgorithmus beträgt O( nm). \\', \\'35 Bemerkungen  \\\\n\\', \\'36 Beispiel: Algorithmus von Boyer und Moore  \\\\nDer String F 7 wird in dem String \\\\nabaababaabacabaababaabaab  gesucht.  \\\\n \\\\n                        a b a a b a b a a b a a b \\\\n                  a b a a b a b a a b a a b \\\\n                a b a a b a b a a b a a b  \\\\n              a b a a b a b a a b a a b \\\\n  a b a a b a b a a b a a b \\\\na b a a b a b a a b a a b \\\\n \\\\na b a a b a b a a b a c a b a a b a b a a b a a b \\\\n \\', \\'37 Beispiel -Shift -Tabellen / Komplexität  \\\\n \\\\ndatenbank        retrieval           compiler  \\\\n999999991       999999991          88888881  \\\\n \\\\nkuckuck       rokoko   papa     abrakadabra  \\\\n3336661       662641   2241     77777771131  \\\\n                                       00 \\\\n \\\\nSatz 6.2. Algorithmus 6.2 löst Problem 6.1(a) in Zeit O (n + m) \\\\nund Platz O (m). Als scharfe obere Schranke für die Anzahl an \\\\nZeichenvergleichen ergibt sich 3 n. \\\\n \\\\n \\', \\'38 Reguläre Ausdrücke  \\\\n•Sprache zur Bildung von komplexeren Such -\\\\nMustern:  \\\\n–Ein individuelles Zeichen ist ein regex (regulärer \\\\nAusdruck) .  \\\\n–Vereinigung : Wenn e1 und e2 regexes sind, dann ist  \\\\n(e1 | e2 ) ein regex, der zu allem passt, was zu e1 oder zu e2 \\\\npasst.  \\\\n–Verknüpfung : Wenn e1 und e2 regexes sind, dann ist e1 e2 \\\\nein Regex, der zu einem String passt, der aus einem \\\\nSubstring besteht, der zu e1 passt, sofort gefolgt von einem \\\\nSubstring, der zu e2  passt.  \\\\n–Wiederholung  (Kleene -Abschluss): Wenn e1 ein regex ist, \\\\ndann ist e1* ein regex, der zu einer Folge von null oder \\\\nmehr Strings passt, die zu e1  passen. \\', \\'39 Beispiele regulärer Ausdrücke  \\\\n•(u|e)nabl(e|ing)  passt zu  \\\\n–unable  \\\\n–unabling  \\\\n–enable  \\\\n–enabling  \\\\n•(un|en)*able  passt zu  \\\\n–able \\\\n–unable  \\\\n–unenable  \\\\n–enununenable  \\\\n–... \\', \\'40 Erweiterte Regex‟s (Perl)  \\\\n•Perl enthält spezielle Terme für bestimmte \\\\nZeichentypen, wie alphabetische oder numerische \\\\noder allgemeine “Wildcards”.  \\\\n•Spezieller Wiederholungsoperator ( +) für 1 oder \\\\nmehrere Vorkommen.  \\\\n•Spezieller optionaler Operator ( ?) für 0 oder 1 \\\\nVorkommen.  \\\\n•Spezieller Wiederholungsoperator für spezifische \\\\nAnzahl von Vorkommen : {min,max} . \\\\n–A{1,5}  -  ein bis fünf A‟s.  \\\\n–A{5,}    -  fünf oder mehr A‟s  \\\\n–A{5}     -  genau fünf A‟s  \\', \\'41 Perl \\\\n•Zeichenklassen:  \\\\n–\\\\\\\\w (word char) jedes alpha -numerische Zeichen (Negation: \\\\\\\\W) \\\\n–\\\\\\\\d (digit char) jede Zahl (Negation: \\\\\\\\D) \\\\n–\\\\\\\\s (space char) jeder Zwischenraum (Negation: \\\\\\\\S) \\\\n–.   (wildcard) alles  \\\\n•Ankerpunkte:  \\\\n–\\\\\\\\b (boundary) Wortgrenze  \\\\n–^  Beginn eines Strings  \\\\n–$  Ende eines Strings  \\\\n \\', \\'42 Perl-Regex -Beispiele  \\\\n•U.S. Tel.Nr. ohne optionalen Bereichscode:  \\\\n–/\\\\\\\\b(\\\\\\\\(\\\\\\\\d{3} \\\\\\\\)\\\\\\\\s?)?\\\\\\\\d{3} -\\\\\\\\d{4} \\\\\\\\b/ \\\\n•(amer.) Email -Addresse:  \\\\n–/\\\\\\\\b\\\\\\\\S+@ \\\\\\\\S+(\\\\\\\\.com| \\\\\\\\.edu| \\\\\\\\.gov| \\\\\\\\.org|\\\\\\\\.net)\\\\\\\\b/ \\\\n \\', \\'43 Prinzip und Implementierung  \\\\n \\\\n•Reguläre Ausdrücke werden mit endlichen \\\\nAutomaten analysiert. (Siehe Vorlesung \\\\n“Theoretische Informatik”)  \\\\n \\\\n•Pakete zur Unterstützung von Perl regex‟s sind in \\\\nJava verfügbar.  \\\\n \\\\n \\\\n \\', \\'44 Approximatives String -Matching   \\\\n•Was ist, wenn ein Dokument Tippfehler \\\\noder falsche Buchstaben enthält?  \\\\n•Ähnlichkeitsmaße zwischen Wörtern \\\\n(beliebigen Strings):  \\\\n–Editier -Abstand (Levenstein distance)  \\\\n–Längste gemeinsame Teilfolge (longest \\\\ncommon substring, LCS)  \\\\n•Suche alle Strings, die näher sind als ein \\\\nvorgebener Schwellwert.  \\', \\'45 Approximatives String -Matching  \\\\n•Bisher haben wir String -Matching -Probleme \\\\nbetrachtet, bei denen das Muster pat exakt mit \\\\neinem Substring von text übereinstimmen musste.  \\\\n \\\\n•Beim Matching von regulären Ausdrücken lässt \\\\nman zwar Varianten zu, aber ebenfalls keine \\\\nFehler.  \\\\n \\\\n•In vielen praktischen Fällen ist es wünschenswert, \\\\ndie Stellen von text zu finden, die mit pat \\\\n“nahezu” übereinstimmen, d.h. man erlaubt \\\\nAbweichungen zwischen pat und text. \\\\n \\', \\'46 Anwendungsbeispiele  \\\\n•Molekularbiologie (Erkennung von DNA -\\\\nSequenzen)  \\\\n• Ausgleich verschiedener Schreibweisen \\\\n(Grafik vs. Graphik)  \\\\n• Ausgleich von Beugungen  \\\\n• Toleranz gegenüber Tippfehlern  \\\\n• Toleranz gegenüber OCR -Fehlern  \\\\n \\', \\'47 String -Metriken  \\\\n•Der Begriff “nahezu” wird durch eine Metrik auf \\\\nStrings formalisiert.  \\\\n•Zur Erinnerung: Sei M eine Menge. Eine Funktion \\\\nd : M £ M ! IR heißt Metrik , wenn die folgenden \\\\nBedingungen erfüllt sind:  \\\\n– d(x,y)  ¸ 0         für alle x,y 2 M \\\\n– d(x,y) = 0 ,  x = y  für alle x, y 2 M \\\\n– d(x,y) = d(y,x)   für alle x,y 2 M \\\\n– d(x,z) · d(x,y) + d(y,z) für alle x.y,z 2 M. \\\\n (M,d) ist dann ein metrischer Raum . \\\\n \\', \\'48 String -Metriken  \\\\nProblem 6.2. Gegeben seien ein String pat, ein String text, eine Metrik d \\\\nfür Strings und ein ganze Zahl k ≥ 0. Man finde alle Teil -Strings y von \\\\ntext mit d(pat, y) ≤ k. \\\\n \\\\nBemerkungen:  \\\\n• Für k = 0 erhält man das exakte String -Matching -Problem.  \\\\n \\\\n• Problem 6.2 ist zunächst ein “abstraktes” Problem, da nichts über die  \\\\n Metrik d ausgesagt wird.  \\\\n \\\\n• Zur Konkretisierung von Problem 6.3 und zur Entwicklung von \\\\nentsprechenden Algorithmen müssen zunächst sinnvolle Metriken  \\\\nbetrachtet werden.  \\', \\'49 Längste  gemeinsame  Teilfolge  (LCS)  \\\\n•Länge der längsten Teilfolge von Zeichen,  \\\\ndie zwei Strings gemeinsam ist.  \\\\n•Eine Teilfolge eines String wird durch das \\\\nLöschen von null oder mehreren Zeichen \\\\nerreicht.  \\\\n•Beispiele:  \\\\n–LCS(misspell, mispell) = 7  \\\\n–LCS(misspelled, misinterpretted) = 7                                           \\\\n“mis…p…e…ed”  \\', \\'Längste  gemeinsame  Teilfolge  (LCS)  \\\\n \\\\n•LCS ist keine Metrik (sondern ein \\\\nÄhnlichkeitsmaß, d.h. ein größerer Wert \\\\nbedeutet größere Ähnlichkeit).  \\\\n \\\\n•Eine Metrik ergibt sich durch  \\\\n 𝑑𝑥,𝑦=1−𝐿𝐶𝑆(𝑥,𝑦)\\\\nmax\\\\u2061(𝑥,𝑦) \\\\n \\\\n50 \\', \\'51 Hamming -Distanz  \\\\nDefinition 6.2. Für zwei Strings x und y mit |x| = |y| = m ergibt sich  \\\\ndie Hamming -Distanz ( Hamming  Distance ) durch:  \\\\n \\\\n   d(x, y) = |{1≤ i ≤ m|x[i] ≠ y[i]}| \\\\n \\\\nBemerkungen:  \\\\n Die Hamming -Distanz ist die Anzahl der Positionen, an denen sich x \\\\nund y unterscheiden. Sie ist nur für Strings gleicher Länge definiert.  \\\\nWird in Problem 6.2 für d die Hamming -Distanz verwendet, so spricht  \\\\nman auch von “ string  matching  with k mismatches ”. \\\\n \\\\nBeispiel: d(abcabb , cbacba ) = 4  \\\\n \\\\n \\', \\'52 Editier - (Levenstein -)Abstand  \\\\n•Die minimale Anzahl von Löschungen , \\\\nHinzufügungen  und Änderungen  von \\\\nZeichen, um zwei Strings gleich zu machen.  \\\\n–edit(misspell, mispell) = 1  \\\\n–edit(misspell, mistell) = 2  \\\\n–edit(misspell, misspelling) = 3  \\\\n \\\\n \\', \\'53 Editier -/Levenstein -Distanz  \\\\nDefinition 6.3.  \\\\nFür zwei Strings x und y ist die Editierdistanz (Edit Distance) edit (x, y) \\\\ndefiniert als die kleinste Anzahl an Einfüge - und Löschoperationen, die \\\\nnotwendig sind, um x in y zu überführen.  \\\\n \\\\nLäßt man zusätzlich auch die Ersetzung eines Symbols zu, so spricht man \\\\nvon einer Levenstein -Metrik (Levenshtein Distance) lev (x, y). \\\\n \\\\nNimmt man als weitere Operation die Transposition (Vertauschung  \\\\nzweier benachbarter Symbole) hinzu, so erhält man die Damerau - \\\\nLevenstein -Metrik dlev (x, y). \\\\n \\', \\'54 Editier -/Levenstein -Distanz  \\\\nBemerkungen:  \\\\n•Offensichtlich gilt stets  \\\\n dlev(x, y) · lev(x, y) · edit(x, y). \\\\n \\\\n•Die Damerau -Levenstein -Metrik wurde speziell zur \\\\nTippfehlerkorrektur entworfen.  \\\\n \\\\n•Wird in Problem 6.2 für d eine der Metriken aus Definition \\\\n6.3 verwendet, dann spricht man auch von “string \\\\nmatching with k differences” bzw. von “string matching \\\\nwith k errors”.  \\', \\'55 Beispiel:  \\\\nFür x  = abcabba und y  = cbabac gilt:  \\\\n \\\\n edit(x, y) = 5  \\\\n•abcabba → bcabba → cabba → cbba → cbaba → cbabac  \\\\n \\\\ndlev(x, y) = lev(x, y) = 4  \\\\n•abcabba → cbcabba → cbabba → cbaba → cbabac  \\\\n•abcabba → bcabba → cbabba → cbabab → cbabac  \\\\n \\', \\'56 Berechnung der String -Distanz  \\\\nProblem 6.3. Gegeben seien zwei Strings  x und y. Man \\\\nermittle edit(x, y) bzw.  lev(x, y) bzw. dlev(x, y) sowie die \\\\nzugehörigen Operationen zur Überführung der Strings.  \\\\n \\\\nBemerkungen:  \\\\n• Wenn x und y Dateien repräsentieren, wobei x[i] bzw. y[j] \\\\ndie i-te Zeile bzw. j-te Zeile darstellt, dann spricht man auch \\\\nvom File Difference Problem . \\\\n• Unter UNIX steht das Kommando diff zur Lösung des File \\\\nDifference Problems zur Verfügung.  \\\\n \\\\n•  Da die Metriken edit, lev und dlev sehr ähnlich sind, wird \\\\nim folgenden nur die Levenstein -Metrik betrachtet.  \\\\n• Algorithmen für die anderen Metriken erhält man durch \\\\neinfache Modifikationen der folgenden Verfahren.  \\', \\'57 Berechnung der String -Distanz  \\\\n•Im folgenden sei m = |x| und n = |y| und es \\\\ngelte m ≤ n. \\\\n \\\\n•Lösungsansatz: dynamische Programmierung  \\\\n•Genauer: berechne die Distanz der Teilstrings \\\\nx[1 … i] und y[1 … j] auf Basis bereits \\\\nberechneter Distanzen.  \\\\n \\', \\'58 Berechnung der String -Distanz  \\\\nDie Tabelle LEV sei definiert durch:  \\\\nLEV [i, j] := lev(x[1 … i], y[1 … j]) mit 0 ≤  i  ≤  m, 0 ≤  j ≤ n \\\\n \\\\nDie Werte für LEV [i, j] können mit Hilfe der folgenden \\\\nRekursionsformeln berechnet werden:  \\\\n• LEV [0, j] = j    für 0 ≤  j · n,   \\\\n• LEV [i, 0] = i    für 0 ≤ i ≤ m \\\\n \\\\n• LEV [i, j] = min {LEV [i – 1, j] + 1, \\\\n   LEV [i, j – 1] + 1,  \\\\n   LEV [i – 1, j – 1] + δ(x[i], y[j])} \\\\n \\\\n mit δ (a, b) =  {  0,  falls a = b  \\\\n                          {  1,  sonst  \\\\n \\', \\'59 Berechnung der String -Distanz  \\\\n•Beispiel:  \\\\n• Darstellung von  LEV als Matrix für   x = cbabac   \\\\n                                                    und  y = abcabbbaa : \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n         \\\\n \\\\n•Die zugehörigen Umwandlungen lauten:  \\\\ncbabac → ababac → abcabac → abcabbac → abcabbbac → abcabbbaa  \\\\n \\\\n\\', \\'60 Berechnung der String -Distanz  \\\\nBemerkungen:  \\\\n•Die Rekursionsformel spiegelt die drei \\\\nOperationen Löschen, Einfügen und Substitution \\\\nwider.  \\\\n•Die Stringdistanz ergibt sich als LEV [m, n]. \\\\n•Möchte man nur die Stringdistanz berechnen, so \\\\ngenügt es, sich auf Stufe i der Rekursion die Werte \\\\nvon LEV der Stufe i – 1 zu merken.  \\\\n•Benötigt man die zugehörigen Operationen, \\\\nspeichert man LEV als Matrix und ermittelt die \\\\nzugehörigen Operationen in einer “Rückwärts -\\\\nrechnung”.  \\\\n \\', \\'61 Berechnung der String -Distanz  \\\\nAlgorithmus 6.3. [Berechnung der Stringdistanz ] \\\\n \\\\nfor i := 0 to m do LEV [i, 0] := i end \\\\nfor j := 1 to n do LEV [0, j] := j end \\\\nfor i := 1 to m do \\\\n for j := 1 to n do \\\\n  LEV [i, j] := min {LEV [i –1, j] + 1,  \\\\n                                         LEV [i, j – 1] + 1,  \\\\n                               LEV [i – 1, j – 1] + \\\\uf064(x[i], y[j])} \\\\n end \\\\nend \\\\nreturn  LEV [m, n] \\\\n \\', \\'62 Berechnung der String -Distanz  \\\\n•Veranschaulichung: Die Berechnung der Stringdistanz kann als Pfad in einem \\\\nGraphen veranschaulicht werden.  \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\nDer dargestellte Pfad entspricht der folgenden (nicht optimalen) Umwandlung:  \\\\ncbabac → acbabac → abcbabac → abcabac → abcabbac → abcabbbac → abcabbbaa  \\\\n \\\\n \\\\n\\', \\'63 Komplexität  \\\\n \\\\nAus der Rekursionsformel und den \\\\nBemerkungen folgt:  \\\\n \\\\nSatz 6.3. Die Stringdistanz  (für edit, lev und \\\\ndlev) kann in Zeit O (mn) und Platz O (m) \\\\nberechnet werden. Problem 6.3 kann mit Platz \\\\nO(mn) gelöst werden.  \\\\n \\\\n \\', \\'64 Berechnung der String -Distanz  \\\\nMit einer kleinen Änderung kann die angegebene \\\\nRekursionsformel auch zur Lösung von Problem 6.2 \\\\neingesetzt werden:  \\\\n \\\\nEs sei MLEV  definiert durch:  \\\\n \\\\n  MLEV [ i, j] := min 1≤l ≤j {lev(pat[1 … i], text[l … j])} \\\\n \\\\nd.h., MLEV  [i, j] ist die kleinste Distanz zwischen pat[1 … i] \\\\n und einem Suffix von text[1, j]. \\\\n \\\\nEs gilt nun: MLEV  [0, j] = 0 für 0 ≤ j ≤ n, denn pat[1 … 0] = \\\\uf065 \\\\nund \\\\uf065 ist stets in text[1 … j] ohne Fehler enthalten.  \\', \\'65 Berechnung der String -Distanz  \\\\nAnsonsten berechnet sich MLEV  [i, j] wie LEV [i,; j], d.h.:  \\\\n MLEV  [i, 0] = i   für 0 ≤  i ≤ m \\\\n MLEV  [i, j]  =   min { MLEV  [i – 1, j] + 1,  \\\\n         MLEV  [i, j – 1] + 1,  \\\\n         MLEV  [i – 1, j – 1] + δ (x[i], y[j])} \\\\n \\\\nGilt nun MLEV  [m, j] ·  k, so endet in Position j ein Substring y von text mit \\\\nlev(pat, y) · k (wobei m die Patternlänge ist).  \\\\n \\', \\'66 Berechnung der String -Distanz  \\\\nBeispiel:   Die Tabelle MLEV  für   pat = ABCDE   und   text = \\\\nACEABPCQDEABCR.  \\\\n \\\\n   j  0   1   2   3    4   5  6   7   8  9   10  11  12  13  14  \\\\ni         A   C   E   A   B   P   C   Q   D   E   A   B   C   R  \\\\n0     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \\\\n1  A  1   0   1   1   0   1   1   1   1   1   1   0   1   1   1  \\\\n2  B  2   1   1   2   1   0   1   2   2   2   2   1   0   1   2  \\\\n3  C  3   2   1   2   2   1   1   1   2   3   3   2   1   0   1  \\\\n4  D  4   3   2   2   3   2   2   2   2   2   3   3   2   1   1  \\\\n5  E  5   4   3   2   3   3   3   3   3   3   2   3   3   2   2  \\\\n \\\\nFür k = 2 ergeben sich die Positionen 3, 10, 13 und 14. Die zugehörigen  \\\\nSubstrings von text sind ACE , ABPCQDE , ABC und ABCR . \\\\n \\\\n \\', \\'67 Das Soundex -Verfahren  \\\\nProblem:  Suche nach phonetisch ähnlichen \\\\nWörtern.  \\\\n \\\\n• Wörter werden hierzu auf interne Codes \\\\nabgebildet.  \\\\n \\\\n• Phonetisch ähnliche Wörter sollen auf möglichst \\\\ngleiche Codes abgebildet werden.  \\\\n \\\\n• Das bekannteste Verfahren ist der SOUNDEX -\\\\nAlgorithmus.  \\\\n \\', \\'68 Das Soundex -Verfahren  \\\\nSOUNDEX -Algorithmus  \\\\n \\\\nDer SOUNDEX -Algorithmus besteht aus den folgenden Schritten:  \\\\n1.Nimm den ersten Buchstaben des Wortes und transformiere die restlichen \\\\nBuchstaben (unabhängig von Groß - und Kleinschreibung) nach der folgenden \\\\nTabelle.  \\\\na e i o u h w y   ! 0 \\\\nb f p v   ! 1 \\\\nc g j k q s x z   ! 2 \\\\nd t    ! 3 \\\\nl    ! 4 \\\\nm n   ! 5 \\\\nr    ! 6 \\\\n \\\\n2.Streiche alle Nullen.  \\\\n3.Reduziere alle hintereinander vorkommenden gleichen Zahlen auf eine Zahl.  \\\\n4.Beschränke den ganzen Code auf maximal vier Stellen.  \\', \\'69 Beispiel  \\\\nDie Namen Neumann und Newman \\\\nwerden als gleich erkannt:  \\\\n•Neumann ! N005055 ! N555 ! N5 \\\\n•Newman  ! N00505   ! N55   ! N5 \\', \\'70 Bemerkungen  \\\\n•Der SOUNDEX -Algorithmus dient insbesondere \\\\nder Suche nach Namen.  \\\\n• Der Code ist nicht immer erfolgreich, denn \\\\nähnlich geschriebene Wörter werden oft auf \\\\nunterschiedliche Codes abgebildet:  \\\\n \\\\n–Beispiel: Rodgers und Rogers.  \\\\n–Rodgers ! R032062 ! R3262 ! R326  \\\\n–Rogers   ! R02062   !          R262  \\\\n \\\\n• In der Praxis wird der SOUNDEX -Algorithmus \\\\nz.T. in leicht abgeänderter Form eingesetzt.  \\\\n \\\\n \\', \\'1  \\\\n \\\\nStrukturelle Anfragen  \\\\n \\\\n \\\\nViele Folien in  diesem Abschnitt sind eine deutsche Übersetzung der Folien von  \\\\nRaymond J. Mooney (http://www.cs.utexas.edu/users/mooney/ir -course/).  \\', \\'2 Strukturelle Anfragen  \\\\n•Für Dokumente, die eine Struktur haben, die in einer Suche \\\\nverwertet werden kann.  \\\\n•Eine Struktur könnte sein:  \\\\n–Feste Menge von Feldern, z.B. Titel, Autor, Abstract, etc.  \\\\n–Hierarchische (rekursive) Baumstruktur (z.B. XML):  \\\\n \\\\n \\\\nchapter  \\\\ntitle section  title section  \\\\ntitle subsection  chapter  book  \\', \\'3 Anfragen mit Struktur  \\\\n•Erlaube Anfragen nach Text, der in spezifischen \\\\nFeldern erscheint:  \\\\n–Suche nach “nuclear fusion”, erscheinend in einer \\\\nKapitelüberschrift  \\\\n•SFQL: Relationelle Datenbank -Anfragesprache: \\\\nSQL erweitert mit “Volltext” -Suche.  \\\\n–SELECT Abstract FROM journals.papers  \\\\n WHERE author  contains “Teller” AND titel like \\\\n“nuclear fusion %” AND  date < 1/1/1950  \\\\n–Standard der Air Transport Association/Aircraft \\\\nIndustry Association  \\\\n \\\\n \\', \\'4 Z39.50 -Protokoll  \\\\n•Ursprünglich nur für bibliographische Informationen.  \\\\n•Inzwischen für andere Datentypen erweitert.  \\\\n•Anfragesprache zwischen Client - und Server -DBMS.  \\\\n•Nicht für menschlichen Gebrauch konzipiert.  \\\\n•ANSI - und NISO -Standard seit 1995  \\\\n•Version von 2003 unter \\\\nhttp://www.loc.gov/z3950/agency/Z39 -50-2003.pdf  \\\\n•Weit verbreitet, insbes. bei Bibliotheken.  \\\\n•Beispiele:  \\\\n–Z39.50 -Gateway der Deutschen Bibliothek: \\\\nhttp://z3950gw.dbf.ddb.de/  \\\\n–Karlsruher Virtueller Katalog:  \\\\nhttp://www.ubka.uni -karlsruhe.de/kvk.html  \\', \\'5 Metadaten  \\\\n•Metadaten enthalten Informationen über ein \\\\nDokument, die nicht Teil des Dokuments selbst \\\\nsein müssen (Daten über Daten).  \\\\n•Beschreibende  Metadaten  liegen außerhalb der \\\\nBedeutung des Dokuments:  \\\\n–Autor  \\\\n–Titel  \\\\n–Quelle (Buch, Illustrierte, Zeitung, Zeitschrift)  \\\\n–Datum  \\\\n–ISBN  \\\\n–Verleger  \\\\n–Länge  \\\\n \\', \\'6 Forts. Metadaten  \\\\n•Semantische  Metadaten betreffen den Inhalt:  \\\\n–Abstract  \\\\n–Schlüsselwörter  \\\\n–Klassifikationssysteme  \\\\n•Library of Congress  \\\\n•Dewey Decimal Classification  \\\\n•UMLS (Unified Medical Language System)  \\\\n•Klassifikationsterme können von spezifischen \\\\nOntologien kommen  (Taxonomie eines \\\\nstandardisierten Vokabulars).  \\\\n \\', \\'7 Web -Metadaten  \\\\n•META -Tag in HTML  \\\\n–<META NAME=“keywords”                  \\\\nCONTENT=“pets, cats, dogs”>  \\\\n \\\\n•META -Attribut  “HTTP -EQUIV” erlaubt dem \\\\nServer oder Browser, auf Informationen \\\\nzuzugreifen:  \\\\n–<META HTTP -EQUIV=“content -type”                                                                                                             \\\\nCONTENT=“text/html; charset=EUC -2”> \\\\n–<META HTTP -EQUIV=“expires”                           \\\\nCONTENT=“Tue, 01 Jan 02”>  \\\\n–<META HTTP -EQUIV=“creation -date”                         \\\\nCONTENT=“23 -Sep-01”>  \\', \\'8 Inhaltsbewertung durch Metadaten  \\\\n•PICS (Platform für die Selektion von \\\\nInternet -Inhalten)  \\\\n•http://www.w3.org/PICS/  \\\\n•Bewertungssystem, um das Zensieren \\\\nbasierend auf sexueller oder gewalttätiger \\\\nSprache, Inhalt, etc. zu ermöglichen.  \\\\n–<META HTTP -EQUIV=“PICS -label”                             \\\\nCONTENT=“PG13: SEX, VIOLENCE”>  \\', \\'9 Markup -Sprachen  \\\\n•Sprache, die verwendet wird, um Dokumente  \\\\nmit “Tags” zu annotieren, die Layout - oder \\\\nsemantische Informationen beinhalten.  \\\\n•Die meisten Dokumentsprachen (Word, RTF, \\\\nLatex, HTML) definieren primär das Layout . \\\\n•Historie der verallgemein. Markup -Sprachen:  \\\\nGML(1969)  SGML (1985)  \\\\nHTML (1993)  XML (1998)  Standard  \\\\nHyperText  eXtensible  \\', \\'10 Grundlegende SGML -Syntax  \\\\n•Textblöcke, die von Start - und End -Tags \\\\numgeben sind.  \\\\n–<Tagname Attribut=Wert  Attribut=Wert …>  \\\\n–</Tagname>  \\\\n•Markierte Textblöcke können verschachtelt \\\\nsein. \\\\n•Im HTML ist das Endtag nicht immer \\\\nnotwendig, aber in XML.  \\', \\'11 HTML  \\\\n•Entwickelt für Hypertext im Web.  \\\\n–<a href=“http://www.cs.utexas.edu”>  \\\\n•Kann einen Code wie Javascript in \\\\ndynamischem HTML (DHTML) enthalten.  \\\\n•Eingermaßen separates Layout durch die \\\\nVerwendung von Style sheets (Cascade \\\\nStyle Sheets, CSS) möglich.  \\\\n•Jedoch definiert es primär Layout und \\\\nFormat.  \\', \\'12 XML  \\\\n•Wie SGML eine Metasprache zur Definition \\\\nspezifischer Dokumentsprachen.  \\\\n•Vereinfachung des ursprünglichen SGML für das \\\\nWeb, die vom  WWW Consortium (W3C) \\\\nunterstützt wird.  \\\\n•Vollständige Trennung von semantischen \\\\nInformationen und Layout.  \\\\n•Liefert strukturierte Daten (wie relationale DB) in \\\\neinem Dokumentformat.  \\\\n•Ersatz für ein explizites Datenbankschema.  \\', \\'13 Forts. XML  \\\\n•Ermöglicht Programmen,  leicht auf den Inhalt eines \\\\nDokuments zuzugreifen, im Gegensatz zu HTML, was als \\\\nLayoutsprache zur Formatierung von Dokumenten für den \\\\nmenschlichen Leser  bestimmt ist.  \\\\n \\\\n•Neue Tags werden nach Bedarf definiert.  \\\\n•Strukturen können beliebig tief verschachtelt sein.  \\\\n \\\\n•Separate (optionale) Dokument Typ Definition  (DTD) \\\\ndefiniert Tags und Dokumentgrammatik.  \\\\n•Ausdrucksstärkere Alternative: XML Schema  \\', \\'14 XML -Beispiel  \\\\n<Person>  \\\\n   <Name> <Vorname>John</Vorname>  \\\\n                 <zweiter Name/>  \\\\n                 <Nachname>Doe</Nachname>  \\\\n   </Name>  \\\\n   <Alter> 38 </Alter>  \\\\n   <email> jdoe@austin.rr.com</email>  \\\\n</Person>  \\\\n \\\\n       <tag/> ist die Kurzschrift für leeres Tag <tag></tag>  \\\\n         Tag-Namen unterscheiden zwischen Groß - und \\\\nKleinschrift (im Gegensatz zu HTML)  \\\\n       Ein markiertes Textstück wird Element  genannt.  \\', \\'15 XML -Beispiel mit Attributen  \\\\n<Produktart =“food”>  \\\\n     <Name Sprache =“Spanish”> arroz  con pollo </Name>  \\\\n     <Preis  Währung =“peso”>2.30</ Preis > \\\\n</Produkt > \\\\n \\\\n    Attributwerte  müssen  in Anführungszeichen  gesetzte  \\\\nStrings sein. \\\\n    Für ein gegebenes  Tag kann  jeder  Attributname  nur einmal  \\\\nerscheinen .  \\\\n     \\', \\'16 XML - Diverses  \\\\n•XML Dokumente müssen mit einem speziellen Tag beginnen.  \\\\n–<?XML VERSION=“1.0”>  \\\\n•Grund -Datenstruktur in XML sind Bäume.  \\\\n•Die Attribute “id” und “idref” ermöglichen es, zusätzliche Links \\\\neinzubauen (und ermöglichen damit eine gewisse Modellierung von \\\\nGraph -Strukturen).  \\\\n   <state id=“s2”>  \\\\n        <abbrev> TX</abbrev>  \\\\n        <name>Texas</abbrev>  \\\\n     </state>  \\\\n     <city id=“c2”>  \\\\n         <aircode> AUS </aircode>  \\\\n         <name> Austin </name>  \\\\n         <state idref=“s2”/>  \\\\n      </city>  \\', \\'17 Document Type Definition (DTD)  \\\\n•Grammatik oder Schema zur Definition von \\\\nTags und Strukturen einer speziellen \\\\nDokumentart.  \\\\n•Ermöglicht es, durch Verwendung eines \\\\nregulären Ausdrucks, die Struktur eines \\\\nDokumentelements zu definieren.  \\\\n•Ein Ausdruck, der ein Element definiert, \\\\nkann rekursiv sein, was die Ausdrucksstärke \\\\neiner kontextfreien Grammatik erlaubt.  \\\\n \\', \\'18 DTD -Beispiel  \\\\n<!DOCTYPE db [  \\\\n    <!ELEMENT db (Person*)>  \\\\n    <!ELEMENT Person (Name,Alter,(Elternteil | Vormund)?)>  \\\\n    <!ELEMENT Name (#PCDATA)>  \\\\n    <!ELEMENT Alter   (#PCDATA)>  \\\\n    <!ELEMENT Elternteil (Person)>  \\\\n    <!ELEMENT Vormund (Person)>  \\\\n]>  \\\\n        *: 0 oder mehr Wiederholungen  \\\\n         ?: 0 oder 1 (optional)  \\\\n          | : Alternation (or)  \\\\n         PCDATEN: Parsed Character Daten (kann Tags enth.)  \\', \\'19 Beispiel: ein gültiges Dokument für die DTD  \\\\n<db>  \\\\n <Person>  \\\\n     <Name> <Vorname>John</ Vorname > <Nachname>Doe</ Nachname >  \\\\n     </Name>  \\\\n     <Alter> 26 </Alter>  \\\\n     <Elternteil>  \\\\n          <Person>  \\\\n             <Name><Vorname>Robert</Vorname > <Nachname>Doe</Vorname>  \\\\n             </Name>  \\\\n             <Alter> 55</Alter>  \\\\n          </Person>  \\\\n     </Elternteil>  \\\\n </Person>  \\\\n</db>  \\\\n \\', \\'20 Forts. DTD  \\\\n•Tag-Attribute werden  auch  definiert : \\\\n   <!ATTLIS Name Sprache  CDATA #REQUIRED>  \\\\n    <!ATTLIS Preis  Währung  CDATA #IMPLIED>  \\\\n                        CDATA:  Eigenschaftsdaten  (String)  \\\\n                        IMPLIZIERT : Optional  \\\\n•Man kann  die DTD in einer  separaten  Datei  \\\\ndefinieren : \\\\n    <!DOCTYPE db SYSTEM “/u/doe/xml/db.dtd”>  \\\\n \\', \\'21 XSL (Extensible Style Sheet Language)  \\\\n•Definiert das Layout für XML -Dokumente.  \\\\n•Definiert, wie man XML in HTML \\\\nübersetzt.  \\\\n•Das Style Sheet wird in einem Dokument \\\\nwie folgt aufgerufen:  \\\\n–<?xml -stylesheet href=“mystyle.css” type=“text/css”>  \\', \\'22 Standardisierte DTD’s  \\\\n•MathML : für mathematische Formeln.  \\\\n•SMIL  (Synchronized Multimedia Integration \\\\nLanguage ): Scheduling -Sprache für web -basierte \\\\nMulti -Media -Präsentationen.  \\\\n•RDF  \\\\n•TEI (Text Encoding Initiative): für literarische \\\\nWerke.  \\\\n•NITF : für Nachrichten -Artikel.  \\\\n•CML : für Chemikalien.  \\\\n•AIML : für astronomische Instrumente.  \\', \\'23 Parsen von XML  \\\\n•Wandelt XML -Datei in ein internes \\\\nDatenformat zur weiteren Verarbeitung um.  \\\\n•SAX (Simple API for XML ): Liest den \\\\nXML -Text, wobei Ereignisse  (z.B. Start - \\\\nund End -Tag) erfasst und zur Verarbeitung \\\\nan die Anwendung zurückgegeben werden.  \\\\n•DOM  (Document Object Model ): Parst \\\\nXML -Text in eine baumstrukturierte \\\\nobjektorientierte Datenstruktur.  \\', \\'24 DOM  \\\\n•Darstellung eines XML -Dokuments als ein Baum \\\\nvon Knoten -Objekten (z.B. Java Objekten).  \\\\n•Knoten -Klasse hat Unterklassen:  \\\\n–Element   \\\\n–Attribute  \\\\n–CharacterData  \\\\n•Knoten hat Methoden:  \\\\n–getParentNode () \\\\n–getChildNodes () \\', \\'25 Weitere Knoten -Methoden  \\\\n•Element -Knoten  \\\\n–getTagName()  \\\\n–getAttribute()  \\\\n–getAttribute(Stringname)  \\\\n•CharacterData -Knoten  \\\\n–getData()  \\\\n•Es gibt auch Methoden für das Hinzufügen und \\\\ndas Entfernen von Knoten und Text im DOM -\\\\nBaum, für das Bestimmen von Attributen, etc.  \\', \\'26 Apache Xerxes XML Parser  \\\\n•Parser für das Bilden des DOM -Baums \\\\neines XML -Dokuments.  \\\\n•Java-Version verfügbar unter:  \\\\n–http://xml.apache.org/xerxes -j/ \\\\n•Voll-Javadoc verfügbar unter:  \\\\n–http://xml.apache.org/xerxes -j/apiDocs/  \\', \\'27 Ein XML -Beispiel  \\\\n<library location=\"Bremen\">  \\\\n <author name=\"Henry Wise\">  \\\\n  <book title=\"Artificial Intelligence\"/>  \\\\n  <book title=\"Modern Web Services\"/>  \\\\n  <book title=\"Theory of Computation\"/>  \\\\n </author>  \\\\n <author name=\"William Smart\">  \\\\n  <book title=\"Artificial Intelligence\"/>  \\\\n </author>  \\\\n <author name=\"Cynthia Singleton\">  \\\\n  <book title=\"The Semantic Web\"/>  \\\\n  <book title=\"Browser Technology Revised\"/>  \\\\n </author>  \\\\n</library>  \\', \\'28 DOM -Darstellung  \\\\n\\', \\'29 Anfragen an  XML -Dokument e \\\\n•Das zentrale Konstrukt einer XML -\\\\nAnfragesprache sind Pfadausdrücke.  \\\\n–Sie legen fest, welche Knoten des DOM erreicht \\\\nwerden können.  \\\\n \\\\n•XPath ist die Basis für XML -Anfragesprachen.  \\\\n–Sprache, um Teile eines XML -Dokuments zu \\\\nadressieren.  \\\\n–Operiert im Baummodell von XML.  \\\\n–Hat eine Nicht -XML -Syntax.  \\\\n \\\\n \\', \\'30 Beispiele für Pfadausdrücke in XPath  \\\\n   /library/author  \\\\n•Addressiert alle author -Elemente, die \\\\nKinder von library -Elementen sind, die \\\\ndirekt unter der Wurzel hängen.  \\\\n•Allgemein sucht /t1/.../tn einen Pfad von \\\\nder Wurzel bis zu Element tn.  \\', \\'31 Beispiele für Pfadausdrücke in XPath  \\\\n \\\\n   //author  \\\\n•// gibt an, dass wir alle Elemente im Baum \\\\ndarauf testen sollen, ob sie vom Typ \\\\nauthor sind.  \\\\n•Dies sucht alle  author -Element e irgendwo \\\\nim Dokument.  \\', \\'32 Beispiele für Pfadausdrücke in XPath  \\\\n•Suche alle Bücher mit dem Titel  \\\\n“Artificial Intelligence”  \\\\n   \\\\n /book[@title=\"Artificial Intelligence\"]   \\\\n \\\\n•Der Test in den eckigen Klammern ist ein \\\\nFilterausdruck , der aus der Menge der \\\\nTreffer selektiert.  \\', \\'33 Resource Description Framework (RDF)  \\\\n•kompatibel mit XML -Syntax.  \\\\n•Hat ein Graphmodell anstelle eines Baummodells \\\\nals Semantik. (Ist also inkompatibel zur XML -\\\\nSemantik.)  \\\\n•Neuer Standard für Web -Metadaten.  \\\\n–Beschreibung des Inhalts  \\\\n–Beschreibung der Sammlung  \\\\n–Datenschutz -Informationen  \\\\n–Intellektuelle Besitzrechte (z.B. Copyright)  \\\\n–Rating des Inhalts  \\\\n–Digitale Unterschriften für Berechtigungen  \\', \\'34 Beispiel  \\\\n<uni:lecturer rdf:ID=\"949352\">  \\\\n <uni:name>Grigoris Antoniou</uni:name>  \\\\n</uni:lecturer>  \\\\n<uni:professor rdf:ID=\"949318\">  \\\\n  <uni:name>David Billington</uni:name>  \\\\n</uni:professor>  \\\\n<rdfs:Class rdf:about=\"#professor\">  \\\\n <rdfs:subClassOf rdf:resource=\"#lecturer\"/>  \\\\n</rdfs:Class>  \\\\n \\\\n•Eine Anfrage nach den Namen aller Dozenten (lecturer) \\\\nsollte sowohl Grigoris Antoniou als auch David Billington \\\\nergeben.  \\', \\'35 Beispiele   \\\\n•Select -from -where ähnlich wie in SQL:  \\\\n \\\\n•Suche nach den Telephonnummern aller \\\\nAngestellten:  \\\\nselect X,Y  \\\\nfrom {X}phone{Y}  \\\\n \\\\n•X und Y sind Variablen, und {X}phone{Y}  \\\\nrepräsentiert ein Objekt -Attribut -Wert -Tripel.  \\', \\'36 Beispiele  \\\\n•Suche alle Dozenten und ihre \\\\nTelefonnummern (mit implizitem Join):  \\\\nselect X,Y  \\\\nfrom lecturer{X}.phone{Y}  \\', \\'37 Beispiele  \\\\n•Gib die Namen aller Vorlesungen aus, die von \\\\ndem Dozenten mit ID 949352 gehalten werden \\\\n(mit explizitem Join):  \\\\nselect N  \\\\nfrom course{X}.isTaughtBy{Y}, {C}name{N}  \\\\nwhere Y=\"949352\" and X=C  \\', \\'38 Schemaanfragen in RDF  \\\\n•Schema -Variablen beginnen mit $ (für \\\\nKlassen) und @ (für Attribute).  \\\\n•Gib alle Ressourcen und Tripel mit dem \\\\nAttribut phone  aus (eingeschlossen die \\\\njeweiligen Unterklassen und Unterattribute):  \\\\nselect X,$X,Y,$Y  \\\\nfrom {X:$X}phone{Y:$Y}  \\', \\'39 Web Ontology Language (OWL)  \\\\n•OWL erweitert RDF um logische Konstrukte.  \\\\n•Ist ausdruckstärker als RDF.  \\\\n•Entspricht einem entscheidbaren Fragment der \\\\nPrädikatenlogik erster Stufe.  \\\\n•Basiert auf der Beschreibungslogik SHOIN(D).  \\\\n•Anfragen werden als logische Formeln gestellt.  \\\\n \\\\n•Weitere Details zu RDF und OWL:  \\\\n–http://www.w3.org/TR/rdf -concepts/  \\\\n–http://www.w3.org/2004/OWL/  \\', \\'1 Eigenschaften von Texten  \\', \\'2 Statistische Eigenschaften von Text  \\\\n•Wie ist die Häufigkeit verschiedener Wörter \\\\nverteilt?  \\\\n•Wie schnell wächst die Größe des \\\\nVokabulars mit der Größe eines Korpus?  \\\\n•Solche Faktoren beeinflussen die \\\\nPerformanz des Information Retrieval und \\\\nkönnen verwendet werden, um \\\\nangemessene Termgewichte und andere \\\\nAspekte eines IR -Systems auszuwählen.  \\', \\'3 Worthäufigkeiten  \\\\n•Einige Wörter sind sehr gebräuchlich.  \\\\n–2 der häufigsten Wörter (z.B. “the”, “of”) können ca.  \\\\n 10 % der Wortvorkommen ausmachen.  \\\\n \\\\n•Die meisten Wörter sind sehr selten.  \\\\n–Die Hälfte der Wörter in einem Korpus erscheint nur \\\\neinmal, dies nennt man hapax legomena   (Griechisch \\\\nfür “nur einmal gelesen”)  \\\\n \\\\n•Dies ergibt eine schiefe (“long tailed”) Verteilung.  \\', \\'4 Beispiel: Worthäufigkeiten  \\\\n(von B. Croft, UMass)  \\\\n\\', \\'5 Zipfs Gesetz  \\\\n•Rank (r): Die numerische Position eines Wortes in \\\\neiner nach abnehmender Häufigkeit ( n) sortierten \\\\nListe  \\\\n•Zipf (1949) “entdeckte” dass:  \\\\n \\\\n \\\\n•Wenn die Wahrscheinlichkeit der Rangstelle r  pr  \\\\nist und N die Gesamtzahl der Wortvorkommen:  \\\\n \\\\nrn1\\\\uf0b5\\\\n) Konstante(für   k krn\\\\uf03d\\\\uf0d7\\\\n1.0 Konst. e unabhängig- Korpusfür       \\\\uf0bb \\\\uf03d\\\\uf03d ArA\\\\nNnpr\\', \\'6 Zipf und Termgewichtung  \\\\n•Luhn (1958) wies darauf hin, dass sowohl sehr \\\\nhäufige als auch sehr seltene Wörter nicht sehr \\\\nnützlich für das Indexieren sind.  \\\\n\\', \\'7 Vorhersagen zu Wort -Häufigkeiten  \\\\n•Gemäß Zipf, hat ein Wort, das n mal erscheint, \\\\ndie Rangordnung rn ≈ AN/n  \\\\n•Mehrere Wörter können n-mal auftreten. Wir \\\\nnehmen an, dass die Rangordnung rn für das \\\\nletzte davon gilt.  \\\\n•Also kommen rn Wörter n-fach oder öfter vor \\\\nund rn+1 Wörter  n+1-fach oder öfter.  \\\\n•Dann beträgt die Anzahl der Wörter, die genau  \\\\nn Mal vorkommen:  \\\\n \\\\n \\\\n)1( 11\\\\uf02b\\\\uf03d\\\\uf02b\\\\uf02d\\\\uf0bb\\\\uf02d\\\\uf03d\\\\uf02bnnAN\\\\nnAN\\\\nnANrrIn n n\\\\n\\', \\'8 Forts. Vorhersagen von Worthäufigkeiten  \\\\n•Nehmen wir an, dass der am höchsten \\\\ngerankte Term einmal vorkommt und daher \\\\ndie Rangordnung  D = AN/1 hat.  \\\\n•Der Anteil der Wörter mit Häufigkeit n ist \\\\ndann:  \\\\n \\\\n \\\\n•Der Anteil der Wörter, die nur einmal \\\\nvorkommen, ist demzufolge ½.  \\\\n)1(1\\\\n\\\\uf02b\\\\uf03dnn DIn\\', \\'9 Daten zu Vorkommens -Häufigkeiten  \\\\n (von B. Croft, UMass)  \\\\n\\', \\'10 Passen reale Daten zu Zipfs Gesetz?  \\\\n•Ein Verhältnis der Form y = kxc wird als  \\\\nPotenzgesetz (“power law”) bezeichnet.  \\\\n•Zipf’s Gesetz ist ein Potenzgesetz mit c = –1 \\\\n•Bei einem log -log Plot ergeben Potenzgesetze eine \\\\ngerade Linie mit Neigung c. \\\\n \\\\n \\\\n•Zipf ist ziemlich genau, außer bei sehr hoher und \\\\nsehr niedriger Rangordnung.  \\\\n) log( log) log() log( x ck kx yc\\\\uf02b\\\\uf03d\\\\uf03d\\', \\'11 Vergleich von Zipf mit Brown -Korpus  \\\\nk = 100,000  \\\\n\\', \\'12 Mandelbrot -Korrektur  (1954)  \\\\n•Die folgende – allgemeinere – Form gibt \\\\neinen etwas besseren Angleich:  \\\\n\\\\uf072 \\\\uf072 ,, Konstantenfür       ) ( BP rPnB\\\\uf02d\\\\uf02b\\\\uf03d\\', \\'13 Mandelbrot -Fit \\\\nP = 105.4, B = 1.15, \\\\uf072 = 100  \\\\n\\', \\'14 Erläuterungen zum Zipf -Gesetz  \\\\n•Zipfs Erklärung war sein “Prinzip des geringsten \\\\nAufwands”: der Ausgleich zwischen dem Wunsch \\\\ndes Sprechers nach wenig und dem des Hörers \\\\nnach viel Vokabular.  \\\\n•Debatte (1955 -61) über die Erklärung zwischen \\\\nMandelbrot und H. Simon.  \\\\n•Li (1992) zeigt, dass nur willkürliches Tippen von \\\\nBuchstaben einschließlich Leerraum “Wörter” mit \\\\neiner Zipfschen Verteilung erzeugt.   \\\\n–http://linkage.rockefeller.edu/wli/zipf/     \\', \\'15 Die Auswirkung von Zipfs Gesetz auf IR  \\\\n•Gute Nachricht : Stopwörter machen einen großen \\\\nTeil des Textes aus, so dass deren Beseitigung die \\\\nSpeicherkosten des invertierten Indexes in großem \\\\nUmfang reduziert.  \\\\n•Schlechte Nachricht : Für die meisten Wörter ist es \\\\nschwierig, ausreichend Daten für eine aussage -\\\\nfähige statistische Analyse zu sammeln (z.B. für \\\\nKorrelationsanalysen für Anfrageerweiterungen), \\\\nda sie ziemlich selten sind.  \\', \\'16 Wachstum des Vokabulars  \\\\n•Wie wächst die Größe des gesamten \\\\nVokabulars (Anzahl eindeutiger Wörter) \\\\nmit der Größe des Korpus?  \\\\n•Dies bestimmt, wie die Größe des \\\\ninvertierten Indexes mit der Größe des \\\\nKorpus wächst.  \\\\n•Das Vokabular ist aufgrund von \\\\nEigennamen, Tippfehlern, etc. nicht \\\\nwirklich nach oben begrenzt.  \\', \\'17 Heaps Gesetz  \\\\n•Falls V die Größe des Vokabulars und n die \\\\nLänge des Korpus in Wörtern ist:  \\\\n \\\\n \\\\n•Typische Konstanten:  \\\\n–K \\\\uf0bb 10\\\\uf02d100 \\\\n–\\\\uf062 \\\\uf0bb 0.4\\\\uf02d0.6   (d.h. ungefähr Quadratwurzel)  \\\\n1 0  ,K Konstantenmit     KnV \\\\uf03c\\\\uf062\\\\uf03c \\\\uf03d\\\\uf062\\', \\'18 Daten, die Heaps Gesetz folgen  \\\\nTREC -2: Computer Select Disks, Associated Press, Wall Street Journal, Federal Register  \\', \\'19 Daten, die Heaps Gesetz folgen  \\\\n\\', \\'20 Erläuterung zu Heaps Gesetz  \\\\n•Kann von Zipfs Gesetz abgeleitet werden, \\\\nwobei angenommen wird, dass Dokumente \\\\nin einer willkürlichen Wortauswahl aus \\\\neiner Zipfschen Verteilung erzeugt werden.  \\', \\'1 Suchen im WWW  \\\\nEinführung  \\', \\'2 Das World Wide Web  \\\\n•1990 von Tim Berners -Lee am CERN entwickelt, \\\\num im Internet verfügbare Forschungsdokumente \\\\nzu organisieren.  \\\\n \\\\n•Verbindet zur Verlinkung von Dokumenten die \\\\nIdee von durch FTP verfügbaren Dokumenten mit \\\\nder Idee von Hypertext . \\\\n \\\\n•Entwickelte das ursprüngliche HTTP -Netzwerk -\\\\nProtokoll, URLs, HTML und den ersten \\\\n“Webserver”.  \\\\n \\', \\'3 Web -Vorgeschichte  \\\\n•Ted Nelson entwickelte 1965 die Idee des Hypertexts.  \\\\n \\\\n•Doug Engelbart erfand die Maus und baute die erste \\\\nImplementierung von Hypertext in den späten 60igern bei \\\\nSRI. \\\\n \\\\n•ARPANET wurde Anfang der 70iger Jahre entwickelt.  \\\\n \\\\n•Die grundlegende Technologie war in den 70igern \\\\netabliert; aber die PC -Revolution und verbreitete \\\\nVernetzung waren notwendig, um das Web zu inspirieren \\\\nund es nutzbar zu machen.  \\', \\'4 Web -Browser -Geschichte  \\\\n•Frühe Browser wurden 1992 (Erwise, ViolaWWW) \\\\nentwickelt.  \\\\n \\\\n•1993 entwickelten Marc Andreessen und Eric Bina am \\\\nUIUC NCSA den Mosaic Browser und sorgten für seine \\\\nweite Verbreitung.  \\\\n \\\\n•Andreessen verband sich mit James Clark (Stanford Prof. \\\\nund Silicon Graphics Gründer), um 1994 die Mosaic \\\\nCommunications Inc. zu gründen (die Netscape wurde, um \\\\neinen Konflikt mit UIUC zu vermeiden).  \\\\n \\\\n•Microsoft lizensierte das ursprüngliche Mosaic von UIUC \\\\nund verwendete es 1995 für die Erstellung des Internet \\\\nExplorers.  \\\\n \\', \\'5 Suchmaschinen Frühgeschichte  \\\\n•Ende der 80er waren viele Dateien durch \\\\nanonymes FTP verfügbar.  \\\\n \\\\n•1990 entwickelte Alan Emtage von McGill Univ. \\\\nArchie (Kurzform für “Archive”)  \\\\n–zusammengestellte Listen von auf vielen FTP -Servern \\\\nverfügbaren Dateien.  \\\\n–Erlaubte regex -Suche dieser Dateinamen.  \\\\n \\\\n•1993 wurden Veronica und Jughead entwickelt, \\\\num Namen von durch Gopher -Server verfügbaren \\\\nTextdateien zu suchen.  \\\\n \\', \\'6 Geschichte der Websuche  \\\\n•1993 wurden die ersten Webrobots (spiders) \\\\ngebaut, um URLs zu sammeln:  \\\\n–Wanderer  \\\\n–ALIWEB (Archie -ähnlicher Index des Web)  \\\\n–WWW Worm (indizierte URL’s und Titel für regex - \\\\nSuche)  \\\\n \\\\n•1994 begannen die Stanford -Studenten David Filo \\\\nund Jerry Yang mit der manuellen Sammlung  \\\\nbeliebter Webseiten in einer thematischen \\\\nHierarchie, genannt Yahoo.  \\\\n \\', \\'7 Forts. Geschichte Websuche  \\\\n•Anfang 1994 entwickelte Brian Pinkerton den WebCrawler als ein \\\\nLehrprojekt an der Univ. Wash. (wurde letzendlich Teil von Excite und \\\\nAOL).  \\\\n \\\\n•Einige Monate später entwickelte Fuzzy Maudlin, ein Student an der \\\\nCMU, Lycos - das erste Standard -IR-System, entwickelt im DARPA \\\\nTipster -Projekt.  Das erste System mit einer großen Menge von \\\\nindizierten Seiten.  \\\\n \\\\n•Ende 1995 entwickelte DEC Altavista. Sie benutzten eine große \\\\nServerfarm von Alpha -Maschinen, um schnell große Mengen von Daten \\\\nzu verarbeiten. Sie unterstützten Boolesche Operatoren, Phrasen und \\\\n“reverse pointer” -Anfragen.  \\', \\'8 Websuche - jüngste Geschichte  \\\\n•1998 starteten  Larry Page und Sergey Brin, \\\\nDoktoranden in Stanford, mit Google. Der \\\\nwesentliche Fortschritt ist die Nutzung einer \\\\nLink-Analyse zum Ranking der Ergebnisse, \\\\ndie auf der Weitergabe von Relevanz \\\\nentlang von Hyperlinks basiert.  \\', \\'9 Web -Herausforderungen für IR  \\\\n•Verteilte Daten : Dokumente sind über Millionen \\\\nverschiedener Webserver verteilt.  \\\\n \\\\n•Flüchtige Daten :  Viele Dokumente ändern sich oder \\\\nverschwinden schnell (z.B. tote Links).  \\\\n \\\\n•Großes Volumen : Milliarden von separaten \\\\nDokumenten.  \\\\n \\\\n•Unstrukturierte und redundante Daten : Keine \\\\neinheitliche Struktur, HTML -Fehler, bis zu 30% \\\\n(nahezu) doppelte Dokumente.  \\', \\'10 Web -Herausforderungen für IR  \\\\n \\\\n•Qualität der Daten : Keine redaktionelle Kontrolle, \\\\nfalsche Informationen, schlechte Schreibweise, \\\\nTippfehler, etc.  \\\\n \\\\n•Heterogene Daten : Multiple Medien -Typen (Bilder, \\\\nVideo, Sound), Sprachen, Zeichensätze, etc.  \\\\n \\', \\'11 Anzahl der Webserver (Juni 2009)  \\\\nhttp://news.netcraft.com/  \\\\n\\', \\'12 Anzahl der Webserver (Juni 2009)  \\\\nhttp://news.netcraft.com/  \\\\n\\', \\'13 Anzahl der Webseiten  \\\\n\\', \\'14 Anzahl der indizierten Webseiten  \\\\nUnter der Annahme von ca. 20KB pro Seite entsprechen  \\\\n1 Milliarde Seiten ca. 20 Terabyte Daten.  SearchEngineWatch, Aug. 15, 2001  \\\\n9.6.2010: neuer Google -Index “Caffeine” belegt 100 \\\\nMillionen Gigabyte (http://googleblog.blogspot.com/2010/06/our -new-search -index -caffeine.html)  \\', \\'15 Wachstum der indizierten Webseiten  \\\\nGoogle  listet die aktuelle Anzahl der besuchten Daten.  SearchEngineWatch, Aug. 15, 2001  \\', \\'16 Graph -Struktur des Webs  \\\\nhttp://www9.org/w9cdrom/160/160.html  (2000)  \\', \\'17 Zipf’s Gesetz im Web  \\\\n•Die Anzahl der in -links/out -links zu/von \\\\neiner Seite hat eine Zipfsche Verteilung.  \\\\n•Die Länge der Webseiten hat eine  Zipfsche \\\\nVerteilung.  \\\\n•Die Anzahl der Treffer zu einer Webseite \\\\nhat eine Zipfsche Verteilung.  \\', \\'18 Manuelle hierarchische Web -Taxonomie  \\\\n•Der Ansatz  von Yahoo  ist es, durch die Unterstützung \\\\nmenschlicher Herausgeber ein großes hierarchisch \\\\nstrukturiertes Verzeichnis von Webseiten \\\\nzusammenzustellen.  \\\\n–http://dir.yahoo.com/  \\\\n \\\\n•Das Open Directory Project  ist ein ähnlicher Ansatz, \\\\nder auf der Verteilung der Arbeit auf freiwillige \\\\nHerausgeber basiert (“net -citizens provide the \\\\ncollective brain”). Dies wird auch von vielen anderen \\\\nSuchmaschinen verwendet. Wurde von Netscape \\\\ninitiiert.  \\\\n–http://www.dmoz.org/  \\', \\'19 Automatische Dokument -Klassifizierung  \\\\n•Die manuelle Klassifizierung in eine gegebene Hierarchie ist \\\\narbeitsintensiv, subjektiv und fehleranfällig.  \\\\n \\\\n•Methoden zur Text -Kategorisierung bieten einen Weg, um Dokumente \\\\nautomatisch zu klassifizieren.  \\\\n \\\\n•Die besten Methoden basieren auf dem Trainieren eines Maschinellen -\\\\nLern - (Mustererkennung -)Systems mit einer vorklassifizierten Menge \\\\nvon Beispielen ( supervised learning ). \\\\n \\\\n•Textkategorisierung ist ein Thema, das wir zu einem späteren \\\\nZeitpunkt der Vorlesung diskutieren werden, sowie insbesondere in \\\\nder Vorlesung “Knowledge Discovery”.  \\', \\'20 Websuche unter Verwendung von IR  \\\\nAnfrage  \\\\n IR- \\\\nSystem  \\\\ngerankte  \\\\nDokumente  1. Seite1  \\\\n2. Seite2  \\\\n3. Seite3  \\\\n    . \\\\n    . \\\\n Dokument - \\\\nKorpus  \\\\nWeb \\\\n Spider  \\', \\'1 Websuche  \\\\nSpidering  \\', \\'2 Spider (Roboter/Bots/Crawler)  \\\\n•Beginne  mit einer umfassenden Menge von  Start -\\\\nURLs, von denen aus die Suche zu beginnen ist.  \\\\n•Folge rekursiv allen Links auf diesen Seiten, um \\\\nweitere Seiten zu finden.  \\\\n•Füge alle neu gefundenen Seiten zum invertierten \\\\nIndex hinzu.  \\\\n•Benutzer können ggf. selbst Seiten zur Indizierung \\\\nund/oder als Start -URLs anmelden.  \\\\n•Sitemaps: Protokoll für Webseitenbetreiber, um \\\\nSuchmaschinen eine Liste ihrer Seiten zu übermitteln  \\\\n(http://www.sitemaps.org/)  \\', \\'3 Suchstrategien  \\\\nBreitensuche  \\', \\'4 Forts. Suchstrategien  \\\\nTiefensuche  \\', \\'5 Vor-/Nachteile der Suchstrategien  \\\\n•Die Breitensuche sammelt jeweils alle Knoten, die gleich \\\\nweit von der Ursprungsseite entfernt sind.  \\\\n–Erfordert Speicherung aller Knoten der vorhergehenden Ebene, \\\\nd.h. der Speicherbedarf wächst exponentiell mit der Tiefe.   \\\\n–Dies ist der Standard -Crawling -Ansatz.  \\\\n•Die Tiefensuche erfordert nur die Speicherung der Knoten \\\\nab der letzten Verzweigung, d.h. ist linear in der Tiefe.  \\\\n–Verfahren geht aber bei der Verfolgung eines einzigen  Threads \\\\n“verloren”.  \\\\n•Beide Strategien können mit einer Warteschlange für \\\\nURLs implementiert werden.  \\', \\'DFS \\\\n \\\\n6 \\\\nhttp://xkcd.com/761/  \\\\n\\', \\'7 Vermeidung von Seiten -Duplikaten  \\\\n•Man muss beim erneuten Besuch einer Seite erkennen, dass sie bereits \\\\nbesucht wurde, denn das Web ist ein Graph und kein Baum.  \\\\n•Man muss besuchte Seiten effizient indizieren, um einen schnellen \\\\nWiedererkennungstest zu ermöglichen.  \\\\n–Baumindexierung (z.B. Trie)  \\\\n–Hashtabelle  \\\\n•Indiziere Seiten mit URL als Schlüssel.  \\\\n–URLs müssen normalisiert werden (z.B. “/” -Endung entfernen)  \\\\n–Kein Erkennen duplizierter oder gespiegelter Seiten.  \\\\n•Indiziere Seite mit textlichem Inhalt als Schlüssel.  \\\\n–Erfordert zunächst das Herunterladen der Seite.  \\', \\'8 Spider -Algorithmus  \\\\nInitialisiere eine Warteschlange ( Q) mit der Menge der bekannten  URL’s:  \\\\n \\\\nBis Q leer oder das Seiten - bzw. Zeitlimit erschöpft ist:  \\\\n      Hole URL L vom Anfang von Q. \\\\n      Wenn L keine HTML -Seite ist (.gif, .jpeg, .ps, .pdf, .ppt, etc.)  \\\\n   gehe zum Schleifenanfang.  \\\\n      Wenn L bereits besucht wurde,  \\\\n   gehe zum Schleifenanfang.  \\\\n      Lade Seite P mit URL L runter. \\\\n      Wenn P nicht runtergeladen werden kann  \\\\n   (z.B. 404 Fehler, Roboter ausgeschlossen),  \\\\n   gehe zum Schleifenanfang.  \\\\n      Indiziere P (z.B. zum invertierten Index hinzufügen oder  \\\\n            speichere Zwischenkopie).  \\\\n      Analysiere P, um eine Liste neuer Links N zu erhalten.  \\\\n      Füge N an das Ende von Q an. \\', \\'9 Warteschlangen -Strategien  \\\\n•Die Art des Anfügens von N an Q bestimmt die \\\\nSuchstrategie.  \\\\n•FIFO ( N an das Ende von Q angefügt) ermöglicht \\\\nBreitensuche.  \\\\n•LIFO ( N an den Anfang von Q angefügt) \\\\nermöglicht Tiefensuche.  \\\\n•Das Anordnen von Q nach einer Heuristik \\\\nermöglicht einen “fokussierten Crawler”, der seine  \\\\nSuche auf  “interessante” Seiten fokussiert.  \\\\n \\', \\'10 Einschränken des Crawlens  \\\\n•Begrenze das Spidern auf eine bestimmte \\\\nSite. \\\\n–Entferne Links anderer Sites von Q. \\\\n•Begrenze das Spidern auf ein bestimmtes \\\\nVerzeichnis.  \\\\n–Entferne Links, die nicht im spezifizierten \\\\nVerzeichnis vorhanden sind.  \\\\n•Befolge die Beschränkungen von \\\\nSeiteninhabern (Crawler -Ausschluss).  \\', \\'11 Linkextraktion  \\\\n•Muss alle Links auf einer Seite finden und \\\\ndie URLs extrahieren.  \\\\n–<a href=“http://www.cs.utexas.edu/users/mooney/ir -course”>  \\\\n–<frame src=“site -index.html”>  \\\\n•Muss relative URLs vervollständigen unter \\\\nVerwendung der URL der aktuellen Seite:  \\\\n–<a href=“proj3”>   wird zu            \\\\nhttp://www.cs.utexas.edu/users/mooney/ir -course/proj3  \\\\n–<a href=“../cs343/syllabus.html”>  wird zu  \\\\nhttp://www.cs.utexas.edu/users/mooney/cs343/syllabus.html  \\\\n \\\\n \\', \\'12 URL -Syntax  \\\\n•Eine URL hat die folgende Syntax:  \\\\n–<Schema>://<Autorität><Pfad>?<Anfrage>#<Fragment>  \\\\n•Eine Autorität  hat die Syntax:  \\\\n–<Host>:<Port -Nummer>  \\\\n•Eine Anfrage  übergibt variable Werte aus HTML - \\\\nFormularen und hat die Syntax:  \\\\n–<Variable>=<Wert>&<Variable>=<Wert>…  \\\\n•Ein Fragment  wird auch als Referenz  bezeichnet und ist \\\\nein Verweis innerhalb des Dokuments auf einen Punkt, der \\\\ndurch ein Anker -Tag folgender Form spezifiziert ist:  \\\\n–<A NAME=“<Fragment>”>  \\', \\'13 Java-Klassen Spider  \\\\n      String       HTMLPageRetriever  \\\\n            getHTMLPage()  \\\\n      LinkExtractor  \\\\n            page  \\\\n              extract()       Link  \\\\n      url       URL          HTMLPage  \\\\n           link \\\\n             text \\\\n            outLinks  \\\\n            \\', \\'14 Link -Normalisierung  \\\\n•Äquivalente Verzeichnis -Endungen werden \\\\ndurch Entfernung des End -Schrägstrichs \\\\nnormalisiert:  \\\\n–http://www.cs.utexas.edu/users/mooney/  \\\\n–http://www.cs.utexas.edu/users/mooney  \\\\n•Interne Seitenfragmente (ref’s) werden \\\\nentfernt:  \\\\n–http://www.cs.utexas.edu/users/mooney/welcome.html#courses  \\\\n–http://www.cs.utexas.edu/users/mooney/welcome.html  \\\\n \\\\n \\', \\'15 Link -Extraktion in Java  \\\\n•Java Swing enthält einen HTML -Parser. (Alternativen sind JTidy und \\\\nNekoHTML.)  \\\\n•Der Swing -Parser verwendet “call -back” -Methoden.  \\\\n•Parser bekommt ein Objekt überreicht, das diese Methoden hat:  \\\\n–HandleText(char[] text, int position)  \\\\n–HandleStartTag(HTML.Tag tag, MutableAttributeSet          \\\\n attributes, int position)  \\\\n–HandleEndTag(HTML.Tag tag, int position)  \\\\n–HandleSimpleTag (HTML.Tag tag,                        \\\\nMutableAttributeSet attributes, int position)  \\\\n•Wenn der Parser auf ein Tag oder einen eingebetteten Text trifft, wird \\\\ndie für dieses Objekt geeignete Methode aufgerufen.  \\', \\'16 Forts. Link -Extraktion in Java  \\\\n•Nimm in HandleStartTag, falls es ein “A” -Tag ist, \\\\nden HREF -Attributwert als ursprüngliche URL.  \\\\n•Vervollständige die URL durch Verwendung der  \\\\nBasis -URL:  \\\\n–new URL(URL baseURL, String relativeURL)  \\\\n–Scheitert, falls Basis -URL in einem Verzeichnisnamen \\\\nendet, aber dies wird nicht durch ein abschließendes “/” \\\\nangezeigt.  \\\\n–Füge ein “/” zur Basis -URL, wenn diese nicht in einem \\\\nDateinamen mit Erweiterung endet (und demzufolge \\\\nvermutlich ein Verzeichnis ist).  \\\\n \\', \\'17 Zwischenspeichern mit Basis -URL  \\\\n•Speichere die Kopie einer Seite in einem lokalen \\\\nVerzeichnis für das eventuelle Indexieren zwecks \\\\nRetrieval.  \\\\n•BASE -Tag im Kopfbereich einer HTML Datei \\\\nverändert die Basis -URL für alle relativen Pointer:  \\\\n–<BASE HREF=“<base -URL>”>  \\\\n•Dies ist ein spezielles HTML -Konstrukt zur \\\\nVerwendung in Dokumenten, die von ihrem \\\\nursprünglichen Ort entfernt wurden.  \\', \\'18 Ankertext -Indizierung  \\\\n•Extrahiere Ankertext (zwischen <a> und </a>) \\\\njedes verfolgten Links.  \\\\n•Ankertext ist gewöhnlich beschreibend für das \\\\nDokument, auf das er verweist.  \\\\n•Füge Ankertext zum Inhalt der Bestimmungsseite \\\\nhinzu, um ggf. neue Schlüsselwörter zu erhalten.  \\\\n•Wird von Google verwendet:  \\\\n–<a href=“http://www.microsoft.com”>Evil Empire</a>  \\\\n–<a href=“http://www.ibm.com”>IBM</a>  \\', \\'19 Forts. Ankertext -Indizierung  \\\\n•Hilft, wenn der beschreibende Text in einer \\\\nBestimmungsseite eher in Bildern/Logos als in \\\\nzugänglichem Text eingebettet ist.  \\\\n•Oftmals ist Ankertext nicht nützlich:  \\\\n–“klicke hier”  \\\\n•Vergrößert die Beschreibung insbes. für beliebte \\\\nSeiten mit vielen eingehenden Links, führt zu \\\\nerhöhtem Recall bei diesen Seiten.  \\\\n•Man kann sogar dem Ankertext höhere Gewichte \\\\ngeben als dem Inhalt der Originalseite, da er \\\\nschwerer zu manipulieren ist.  \\\\n \\', \\'20 Multi -Threaded Spidering  \\\\n•Engpass ist die Netzwerkverzögerung beim Herunterladen \\\\nvon individuellen Seiten.  \\\\n•Das Beste ist, mehrere Threads parallel laufen zu lassen, \\\\nwobei jeder eine Seite von einem anderen Host anfordert.  \\\\n•Verteile URLs auf Threads, um die gerechte Verteilung \\\\nvon Anfragen über die verschiedenen Hosts zu \\\\ngewährleisten, den Durchsatz zu maximieren und die \\\\nÜberlastung einzelner Server zu vermeiden.  \\\\n•Frühe Google -Spider hatten zahlreiche koordinierte \\\\nCrawler mit je ca. 300 Threads, die zusammen in der Lage \\\\nwaren, mehr als 100 Seiten pro Sekunde herunterzuladen.  \\', \\'21 Gerichtetes/fokussiertes Spidering  \\\\n•Sortiere die Warteschlange, um zunächst \\\\nmehr “interessante” Seiten zu erforschen.  \\\\n•Zwei Arten von Fokussierung:  \\\\n–Themen -gerichtet  \\\\n–Link -gerichtet  \\', \\'22 Themen -gerichtetes Spidering  \\\\n•Nimm an, dass die gewünschte \\\\nThemenbeschreibung oder interessante \\\\nMusterseiten gegeben sind.  \\\\n•Sortiere die Linkreihe nach der Ähnlichkeit \\\\n(z.B. mit Kosinus -Maß) ihrer \\\\nUrsprungsseiten und/oder Ankertext zu \\\\ndieser Themenbeschreibung.  \\\\n•Lade vorzugsweise Seiten herunter, die sich \\\\nauf das gegebene Thema beziehen.  \\', \\'23 Link -gerichtetes Spidering  \\\\n•Überwache die Links und speichere die Ein - \\\\nund Ausgrade jeder angetroffenen Seite.  \\\\n•Sortiere die Warteschlange, um beliebte \\\\nSeiten mit vielen eingehenden Links \\\\nvorzuziehen. ( Autoritäten ). \\\\n•Sortiere die Warteschlange, um \\\\nzusammenfassende Seiten mit vielen \\\\nausgehenden Links ( hubs ) zu bevorzugen.  \\', \\'24 Gecrawlte Seiten auf dem neusten Stand \\\\nhalten  \\\\n•Das Web ist sehr dynamisch: viele neue Seiten, aktualisierte \\\\nSeiten, entfernte Seiten, etc.  \\\\n•Periodische Prüfung aller gespiderten Seiten hinsichtlich \\\\nAktualisierungen und Beseitigungen:  \\\\n–Schau nur auf Kopfinfo (z.B. META -Tags bei der letzten \\\\nAktualisierung), um zu bestimmen, ob die Seite sich \\\\ngeändert hat. Lade die gesamte Seite erst wieder, wenn \\\\nnotwendig.  \\\\n•Verfolge, wie oft jede Seite aktualisiert wurde und kehre \\\\nvorzugsweise zu Seiten zurück, die dynamischer sind.  \\\\n•Aktualisiere vorzugsweise Seiten, auf die öfter zugegriffen wird, \\\\num die Aktualität der beliebteren Seiten zu optimieren.  \\', \\'25 Roboter -Ausschluss  \\\\n•Web -Server und -Seiten können \\\\nspezifizieren, dass Roboter gewisse \\\\nBereiche nicht crawlen/indizieren sollen.  \\\\n•Zwei Komponenten:  \\\\n–Roboter -Ausschluss -Protokoll : Seitenweise \\\\nSpezifizierung ausgeschlossener Verzeichnisse.  \\\\n–Roboter META Tag : Individuelles Dokument -\\\\nTag zum Ausschluss bei der Indexierung oder \\\\nder Weiterverfolgung von Links.  \\', \\'26 Roboter -Ausschluss -Protokoll  \\\\n•Site-Administrator erstellt eine “robots.txt” -Datei \\\\nan der Wurzel des Verzeichnisbaums.  \\\\n–http://www.ebay.com/robots.txt  \\\\n–http://www.cnn.com/robots.txt  \\\\n•Datei enthält eine Liste aller für einen gegebenen \\\\nRobot ausgeschlossenen Verzeichnisse.  \\\\n–Schließt alle Bots von der kompletten Site aus:  \\\\n        User-agent: *  \\\\n    Disallow: /  \\\\n \\', \\'27 Roboter -Auschluss -Protokoll: Beispiele  \\\\n•Schließt spezifische Verzeichnisse aus:  \\\\n     User-agent: *  \\\\n   Disallow: /tmp/  \\\\n   Disallow: /cgi -bin/ \\\\n   Disallow: /users/paranoid/  \\\\n•Schließt einen speziellen Bot aus:  \\\\n     User-agent: GoogleBot  \\\\n   Disallow: /  \\\\n•Läßt einen speziellen Bot zu:  \\\\n     User-agent: GoogleBot  \\\\n   Disallow:  \\\\n \\\\n   User-agent: *  \\\\n   Disallow: /  \\', \\'28 Roboter -Ausschluss -Protokoll: Details  \\\\n•Verwende nur Leerzeilen, um für \\\\nverschiedene Benutzeragenten die jeweils \\\\nnicht erlaubten Verzeichnisse zu trennen.  \\\\n•Ein Verzeichnis pro “Disallow” -Zeile.  \\\\n•Keine regulären Ausdrücke in \\\\nVerzeichnisnamen erlaubt.  \\', \\'29 Roboter -META -Tag \\\\n•Schließe META -Tag in den HEAD -Bereich \\\\neines spezifischen HTML -Dokuments ein.  \\\\n–<meta name=“robots” content=“none”>  \\\\n•“content” enthält Wertepaar für zwei \\\\nDimensionen:  \\\\n–index  | noindex :  Zulassen/nicht Zulassen der \\\\nIndexierung dieser Seite.  \\\\n–follow  | nofollow : Verfolgen/nicht Verfolgen \\\\nder Links auf dieser Seite.  \\\\n \\', \\'30 Forts. Roboter -META -Tag \\\\n•Spezielle Werte:  \\\\n–all = index,follow  \\\\n–none = noindex,nofollow  \\\\n•Beispiele:  \\\\n <meta name=“robots” content=“noindex,follow”>  \\\\n    <meta name=“robots” content=“index,nofollow”>  \\\\n    <meta name=“robots” content=“none”>  \\', \\'31 Roboter -Ausschluss: Diskussion  \\\\n•META Tag ist neuer und weniger gut angepasst \\\\nals “robots.txt”.  \\\\n•Standards sind Konventionen, die von “guten \\\\nRobotern” befolgt werden.  \\\\n•Firmen sind für die “Nichtbeachtung” dieser \\\\nKonventionen und das “unerlaubte Betreten” von \\\\nprivatem virtuellem Raum belangt worden.  \\\\n•“Gute Roboter” versuchen auch, individuelle Sites \\\\nnicht mit vielen, schnellen Anfragen zu \\\\n“bombardieren”.  \\\\n–keine “Denial of Service” -Angriffe.  \\', \\'1Web-Suche \\\\nBenutzer-Schnittstelle \\', \\'2Websuche: Benutzer-Schnittstelle \\\\n•Web-Suchmaschinen brauchen natürlich eine web-basierte Benutzer-Schnittstelle. \\\\n•Die Suchseite muss einen Anfragestring entgegennehmen und diesen mittels eines HTML- < form> ulars übertragen. \\\\n•Das Programm auf dem Server muss Anfragen verarbeiten und eine HTML-Seite für die am höchsten gelisteten Dokumente (mit Links zu den ursprünglichen und/oder zwischengespeicherten Webseiten) erzeugen. \\\\n\\', \\'3Eingabeformulare \\\\n•HTML unterstützt verschiedene Arten der Programmeingabe in Formularen einschließlich: \\\\n–Textbox \\\\n–Menüs\\\\n–Prüfbox \\\\n–Auswahlbuttons \\\\n•Wenn ein Anwender ein Formular abschickt, werden Stringwerte für verschiedene Parameter \\\\nzur Verarbeitung an den Server übertragen. \\\\n•Der Server nutzt diese Werte, um eine geeignete HTML-Antwortseite zu berechnen. \\\\n\\', \\'4Ein einfaches Suchformular \\\\n<form action=\"http://titan.cs.utexas.edu:8082/servl et/irs.Search\" method=\"POST\"> \\\\n<p>Geben Sie eine Anfrage ein: <input type=“text\" name= “query\"></p> \\\\n<p>Suchdatenbank:\\\\n<select name=“directory\"> \\\\n<option selected value=\"/corpora/UniK/\">Universität Kassel</option> \\\\n<option value=\"/corpora/KDE/\">FG Wissensverarbeitun g</option> \\\\n</select> \\\\n</p> <input type=“hidden\" name=“start\" value=\"0\"> <input type=“submit\" value=“Anfrage ausführen\"> <input type=“reset\" value=“Formular zurücksetzen\"> \\\\n</form> \\', \\'5Web-Anwendungen \\\\n•Web-Anwendungen sind Anwendungen, die über das Web benutzt werden, d.h. \\\\n–HTTP als Transportprotokoll \\\\n–HTML als Benutzer-Schnittstelle.\\\\n•Verknüpfung von HTML und Businesslogik \\\\n–dynamisch generierte HTML-Seiten \\\\n•Beispiele: \\\\n–CGI-Skripte (Perl, Python, ...) \\\\n–PHP \\\\n–Content-Management-Systeme: Zope, Typo3, ... \\\\n–Java Servlets \\', \\'6Webserver \\\\n•Dienst zur Auslieferung von Webseiten über das HTTP-Protokoll.\\\\n•Einfachste Variante: \\\\n–statisches HTML im Dateisystem \\\\nGET /foo.html HTTP/1.1 \\\\nWebserver Dateisystem .../www/foo.html Port 80 <html>...</html> \\', \\'7Web-Applikationsserver \\\\n•Webserver + Business-Logik \\\\n–Reaktion auf Benutzereingaben \\\\n–Datenbankzugriffe \\\\n•Mischen von HTML und Logik \\\\n–Logik in HTML (vgl. PHP) \\\\n–HTML wird vom Programm erzeugt (vgl. CGI) \\\\n–Mischformen \\\\n•beinhaltet oft weitere Dienste \\\\n–Datenbankanbindung \\\\n–Lastverteilung \\\\n–Hochverfügbarkeit \\', \\'8Was ist ein Servlet?\\\\n•Javas Antwort auf CGI-Programmierung zur Verarbeitung von Webformular-Anfragen.  \\\\n•Das Programm läuft auf Webservern und erzeugt dynamische/angepasste Seiten. \\\\n•Wann verwendet man Servlets?\\\\n–Seite basiert auf vom Benutzer eingegebenen Daten, z.B. Suchmaschinen. \\\\n–Daten ändern sich oft, z.B. Wetterberichte. \\\\n–Seite verwendet Informationen von einer Datenbank, z.B. Online-Speicher. \\\\n•Erfordert das Betreiben eines Webservers, der Servlets unterstützt. \\', \\'9Servlet-Container \\\\n•Webserver, der Servlets beheimatet \\\\n–übernimmt Netzwerkverkehr \\\\n–bietet Infrastruktur \\\\n•Konfiguration, Skalierbarkeit, ... \\\\nGET /foo.html HTTP/1.1 \\\\nServlet -Container Port 80 <html>...</html> Servlet \\\\nServlet Servlet - Eingabe lesen \\\\n- Berechnung \\\\ndurchführen \\\\n- HTML erzeugen \\', \\'10 Formularbeispiele \\\\n\\', \\'11 Servlet-Ausgabe \\\\n\\', \\'12 HTML-Post-Form <form action=\"/servlet/hall.ThreeParams\"\\\\nmethod=\"post\"> \\\\nFirst Parameter:  <input type=\"text\" \\\\nname=\"param1\"><BR> \\\\nSecond Parameter: <input type=\"text\" \\\\nname=\"param2\"><BR> \\\\nThird Parameter:  <input type=\"text\" \\\\nname=\"param3\"><BR> \\\\n<center>\\\\n<input type=\"submit\"> \\\\n</center>\\\\n</form> \\', \\'13 Reading Parameters \\\\npublic class ThreeParams extends HttpServlet { \\\\npublic void doGet(HttpServletRequest request, \\\\nHttpServletResponse response) throws ServletExcepti on, \\\\nIOException { response.setContentType(\"text/html\"); PrintWriter out = response.getWriter(); out.println(… +\"<UL>\\\\\\\\n\" + \\\\n\"<LI>param1: \" + request.getParameter(\"param1\") + \"\\\\\\\\\\\\nn\" +              \\\\n\"<LI>param2: \" + request.getParameter(\"param2\") + \"\\\\\\\\n\" +    \\\\n\"<LI>param3: \" + request.getParameter(\"param3\") + \"\\\\\\\\n\" + \\\\n\"</UL>\\\\\\\\n\" + …); \\\\n} public void doPost(HttpServletRequest request, \\\\nHttpServletResponse response) throws ServletExcepti on, \\\\nIOException { \\\\ndoGet(request, response); \\\\n}\\\\n} \\', \\'14 Grundsätzliche Servlet-Struktur import java.io.*; import javax.servlet.*; import javax.servlet.http.*; public class SomeServlet extends HttpServlet { // Handle get request \\\\npublic void doGet(HttpServletRequest request, HttpServlet Response \\\\nresponse) throws ServletException, IOException { // request request request request – access incoming HTTP headers and HTML form data \\\\n// response response response response - specify the HTTP response line and headers \\\\n// (e.g. specifying the content type, setting cookies). \\\\nPrintWriter out = response.getWriter(); //out out out out - send content to \\\\nbrowser \\\\n}\\\\n} \\', \\'15 Lesen aller Parameter \\\\n•Liste aller Parameternamen, die Werte haben:\\\\nEnumeration paramNames = request.getParameterNames( ); \\\\n–Parameternamen in unspezifizierter Reihenfolge.      \\\\n•Parameter können viele Werte haben :\\\\nString[] paramVals = \\\\nrequest.getParameterValues(paramName); \\\\n–Auflistung von Werten, die mit paramName \\\\nverbunden sind.\\', \\'16 Einfaches Such-Servlet \\\\n•Erzeugt oder wählt einen vorhandenen InvertedIndex für den entsprechenden Korpus aus (z.B. directory Parameter). \\\\n•Verarbeitet die Anfrage ( query Parameter) mit KSM, um \\\\neine gerankte Ergebnismenge zu ermitteln. \\\\n•Gibt in HTML eine geordnete Liste von 10 Ergebnissen aus, beginnend mit der Rangstelle des start Parameters. \\\\n•Jede Position enthält: \\\\n–Ursprüngliche Basis-URL, die vom Spider am Anfang des Dokuments im BASE-Tag gespeichert wurde. \\\\n–Seiten-<TITLE>, der aus der Datei extrahiert wurde. \\\\n–Zusätzlicher Verweis zur lokal zwischengespeicherten Datei. \\\\n•Erstellt ein Formular, um “ Weitere Ergebnisse ” beginnend mit \\\\nder nächsten gelisteten Position anfordern zu können, falls noch nicht alle Ergebnisse angezeigt wurden. \\', \\'17 Schnittstellenverbesserungen fürdie KSM \\\\n•Verarbeitet gegenwärtig die Anfragen erneut, auch wenn nach “\\\\nWeitere Ergebnisse ” gefragt wird. \\\\n–Könnte die gegenwärtig gerankte Liste zusammen mit der Anwendersitzung speichern. \\\\n•Könnte weitere Interaktionen durch Relevance Feedback integrieren. \\\\n•Könnte Anfrage: “ Bestimme ähnliche Seiten ” für \\\\njedes gefundene Dokument (wie in Google) liefern. \\\\n–Verwende einfach gegebenen Dokumententext als eine Anfrage. \\', \\'18 Andere Verbesserungen der Suchschnittstelle \\\\n•Hebe Suchbedingungen in dem angezeigten Dokument hervor. \\\\n–Wie bei gecachter Datei in Google. \\\\n•ermögliche “erweiterte” Suche: \\\\n–Phrasensuche (“...”)\\\\n–Verbindliche Terme (+) \\\\n–Negierte Terme (-)\\\\n–Sprachpräferenzen \\\\n–Rückwärts-Link \\\\n–Datenpräferenzen \\\\n•Maschinelle Übersetzung von Seiten. \\\\n\\', \\'19 Andere Verbesserungen der Suchschnittstelle \\\\n•Gruppensuchergebnisse in kohärenten “Clustern”:\\\\n–“Mikrowelle”\\\\n•Eine Gruppe über Ernährungsrezepte und Küchengeschi rr. \\\\n•Eine andere Gruppe über den Empfang von Satellitenf ernsehen. \\\\n–“Huskies”\\\\n•Eine Gruppe über Hunde. \\\\n•Eine Gruppe über das Kasseler Eishockey-Team. \\\\n•Northern Light gruppiert Ergebnisse in “Ordnern”, die auf \\\\neiner voretablierten Kategorisierung von Seiten basieren (wie Yahoo- oder DMOZ-Kategorien). \\\\n•Eine Alternative ist, Suchergebnisse dynamisch in Gruppen ähnlicher Dokumente zusammenzufassen.  \\', \\'Meta-Suchmaschinen \\\\n•Suchmaschine, die eine Anfrage an einige andere Suchmaschinen weitergibt und die Ergebnisse integriert.\\\\n–Unterbreite Anfrage an verschiedene Suchmaschinen \\\\n–Parse die resultierenden HTML-Seiten, um die Suchergebnisse zu extrahieren.\\\\n–Integriere die unterschiedlichen Klassifizierungen in einer “Konsens”-Klassifikation.\\\\n–Präsentiere dem Anwender das integrierte Ergebnis.\\\\n•Beispiele:\\\\n–Metacrawler \\\\n–SavvySearch \\\\n–Dogpile \\\\n\\', \\'21 Anwenderverhalten \\\\n•Anwender neigen dazu, kurze Anfragen einzugeben: \\\\n–Eine Studie in 1998 ergab eine Durchschnittslänge von 2.35 Wörtern. \\\\n•Anwender neigen dazu, keine erweiterten Suchoptionen zu verwenden. \\\\n•Anwender müssen bei der Verwendung komplexerer Anfragen eingewiesen werden. \\', \\'1Web-Suche \\\\nLink-Analyse \\', \\'2Bibliometrik: Zitat-Analyse \\\\n•Viele Dokumente enthalten Bibliographien (oder \\\\nReferenzen ), d.h. eindeutige Zitierungen anderer, \\\\nvorher veröffentlichter Dokumente.\\\\n•Bei Verwendung von Zitaten als Links können solche Korpora als gerichteter Graph betrachtet werden.\\\\n•Die Struktur dieses Graphen kann unabhängig vom Inhalt interessante Informationen über die Ähnlichkeit von Dokumenten und die Struktur der Korpora liefern. \\', \\'3Einflussfaktor (Impact Factor) \\\\n•Von Garfield in 1972 entwickelt, um die Bedeutung (Q ualität, Einfluss) \\\\nvon wissenschaftlichen Zeitschriften zu messen.\\\\n•Maß dafür, wie oft Artikel einer Zeitschrift von anderenWissenschaftlern zitiert werden. \\\\n•Wird jährlich von Thomson Reuters ( http://www.isinet.com/ ) berechnet \\\\nund veröffentlicht.\\\\n•Der Einflussfaktor einer Zeitschrift Jim Jahr Yist die durchschnittliche \\\\nAnzahl von Zitaten (von allen indizierten Dokumenten, d ie im Jahr Y\\\\nveröffentlicht wurden) eines Artikels, der in Jim Jahr Y −1 oder Y −2 \\\\nveröffentlicht wurde.\\\\n•Berücksichtigt nicht die Qualität des zitierenden Artik els.\\\\n•Siehe auch http://citeseer.ist.psu.edu/impact.html für einen ähnlichen \\\\nIndex. \\', \\'4Bibliographische Kopplung \\\\n•Maß für die Ähnlichkeit von Dokumenten, das 1963 von Kessler eingeführt wurde.\\\\n•Die bibliographische Kopplung von zwei Dokumenten A und Bist die Anzahl der \\\\nDokumente, die sowohl von Aals auch von Bzitiert \\\\nwerden, d.h. der Umfang des Durchschnitts ihrer Bibliographien (ggf. normiert durch die Größe der Bibliographien). \\\\nA B\\', \\'5Ko-Zitatation \\\\n•Ein alternatives auf Zitaten basierendes Maß der Ähnlichkeit, das 1973 von Small eingeführt wurde.\\\\n•Anzahl der Dokumente, die sowohl Aals auch B\\\\nzitieren, ggf. normalisiert durch die gesamte Anzahl von Dokumenten, die entweder Aoder B \\\\nzitieren. \\\\nA B\\', \\'6Zitate im Vergleich zu Links \\\\n•Weblinks sind anders als Zitate:\\\\n–Links sind navigationsfähig.\\\\n–Viele Seiten mit hohem In-Grad sind Portale und keine Inhaltsanbieter.\\\\n–Nicht alle Links (aber auch nicht alle Zitate) sind Bestätigungen. (vgl. “miserable failure”)\\\\n–Firmenwebseiten verweisen nicht auf ihre Konkurrenten, Zitate relevanter Literatur werden hingegen durch Peer-Reviewing erzwungen.\\', \\'7Autoritäten \\\\n•Autoritäten sind Seiten, die anerkannt sind, und \\\\ndie signifikante, vertrauenswürdige und nützliche Information zu einem Thema liefern.\\\\n•In-Grad (Anzahl von Links auf eine Seite) ist ein \\\\neinfaches Maß der Autorität.\\\\n•Jedoch behandelt der In-Grad alle Links gleich.  \\\\n•Sollten nicht Links von Seiten, die selbst Autoritäten sind, mehr zählen?\\', \\'8Hubs \\\\n•Hubs sind Indexseiten, die viele nützliche \\\\nLinks auf relevante Inhaltsseiten (Themenautoritäten) liefern.\\\\n•Hubseiten zum Thema “Information Retrieval” sind z.B unter \\\\nhttp://www.cs.utexas.edu/users/mooney/ir-course \\\\nzu finden.\\', \\'9HITS – Hypertext Induced Topic Search \\\\n•Algorithmus, der 1998 von Kleinberg entwickelt wurde.\\\\n•Er versucht, Hubs und Autoritäten zu einem bestimmten Thema rechnerisch durch die Analyse eines relevanten Subgraphen des Webs zu bestimmen. \\\\n•HITS basiert auf einer rekursiven Definition: \\\\n–Hubs verweisen auf viele Autoritäten.\\\\n–Auf Autoritäten wird von vielen Hubs verwiesen.\\\\n\\', \\'10 Hubs und Autoritäten \\\\n•Zusammen neigen sie dazu, einen bipartiten Graphen zu bilden:\\\\nHubs Autoritäten \\', \\'11 HITS-Algorithmus \\\\n•Aufgabe: Berechne Hubs und Autoritäten für ein bestimmtes Thema, das durch eine Anfrage spezifiziert ist.\\\\n•Bestimme zuerst eine Menge relevanter Seiten für die Anfrage, die als Basis- Menge \\\\nSbezeichnet wird.\\\\n•Analysiere die Linkstruktur des durch S\\\\ninduzierten Teilgraphen, um Autoritäts- und Hubseiten in dieser Menge zu finden. \\', \\'12 Konstruieren eines Basis-Subgraphen \\\\n•Für eine spezifische Anfrage Qsei die Wurzel- Menge Rdie Menge der \\\\nvon einer Standard-Suchmaschine (z.B. KSM) zurückgege benen \\\\nDokumente.\\\\n•S := R .\\\\n•Füge zu S alle Seiten hinzu, auf die mindestens eine Seite in Rverweist.\\\\n•Füge zu Salle Seiten hinzu, die auf mindestens eine Seite in R verweisen.\\\\nRS\\', \\'13 Aufwandsbegrenzung \\\\n•Um den rechnerischen Aufwand zu begrenzen:\\\\n–Begrenze die Anzahl der Wurzelseiten auf die besten 200 Seiten, \\\\ndie für die Anfrage gefunden wurden.\\\\n–Begrenze die Anzahl der “Rückwärts-Link”-Seiten auf ein e \\\\nwillkürliche Menge von höchstens 50 Seiten, die von ei ner \\\\n“Rückwärts-Link”-Anfrage zurückgegeben wurden.\\\\n•Um reine Navigationslinks zu eliminieren:\\\\n–Eliminiere Links zwischen zwei Seiten auf dem gleichen Ho st. \\\\n•Um “nicht-autoritätsfördernde” Links zu eliminieren:\\\\n–Erlaube max. m(m ≅4−8) Seiten von jedem Host als Zeiger auf \\\\nein beliebige individuelle Seite.\\', \\'14 Autorität und In-Grad \\\\n•Selbst in der Basismenge Seiner gegebenen \\\\nAnfrage sind die Knoten mit dem höchsten In-Grad nicht notwendigerweise Autoritäten (sondern evtl. nur allgemein bekannte Seiten wie Yahoo oder Amazon). \\\\n•Auf ‘wahre’ Autoritätsseiten wird von mehreren Hubs verwiesen (dies sind Seiten, die auf viele Autoritäten verweisen.) \\', \\'15 HITS – Iterativer Algorithmus \\\\n•Iterativer Algorithmus, der sich langsam einer sich gegenseitig verstärkenden Menge von Hubs und Autoritäten nähert.\\\\n•Aufgabe: Bestimme für jede Seite p ∈S\\\\n–den Autoritätswert ap(zusammengefasst in einem \\\\nVektor a)\\\\n–und den Hubwert hp       (Vektor h)\\', \\'16 HITS-Algorithmus \\\\n1. Initialisiere alle ap:= h p:= 1 \\\\n2. Normalisiere die Werte, so dass gilt: \\\\n3. Auf Autoritäten wird durch viele gute Hubs verwiesen:\\\\n4. Hubs verweisen auf viele gute Autoritäten:\\\\n5. Solange die Vektoren sich (signifikant) ändern, gehe zu Schritt 2. ∑\\\\n→=\\\\npqqq p h a\\\\n:\\\\n∑\\\\n→=\\\\nqpqq p a h\\\\n:()12=∑\\\\n∈Spph ()12=∑\\\\n∈Sppa\\', \\'17 Illustrierte Update-Regeln \\\\n2\\\\n3a4:= h 1+ h 2+ h 31\\\\n5\\\\n764\\\\n4 h4:= a 5+ a 6+ a 7\\', \\'18 HITS im Detail \\\\nInitialisiere für alle p ∈S:   ap:= hp:= 1 \\\\nBis Änderung kleiner als gegebener Schwellwert:\\\\nFür alle p ∈S:               \\\\n/* aktualisiere Autoritätswerte */ \\\\nFür alle p ∈S: \\\\n/* aktualisiere Hubwerte */ \\\\nFür alle p ∈S: ap:= ap/c mit \\\\n/* anormalisieren */ \\\\nFür alle p ∈S: hp: = h p/c  mit \\\\n/* hnormalisieren */ ∑\\\\n→=\\\\npqqq p h a\\\\n::\\\\n∑\\\\n→=\\\\nqpqq p a h\\\\n::\\\\n∑\\\\n∈=\\\\nSppa c2:\\\\n∑\\\\n∈=\\\\nSpph c2:\\', \\'19 Darstellung in linearer Algebra \\\\n•Definiere Aals Adjazenzmatrix für den \\\\ndurch Sinduzierten Subgraphen .\\\\n–Aij = 1 für i ∈S, j∈S  gdw. i→jim Graphen.\\\\n•Die Autoritätswerte apwerden in einem \\\\nVektor a zusammengefasst, und die \\\\nHubwerte hp in einem Vektor h. \\\\n•Die Schritte der Iteration ergeben sich zu \\\\n–h:= Aa\\\\n–a:= ATh\\', \\'20 Konvergenz \\\\n•Algorithmus konvergiert zu einem Fixpunkt, falls unendlich wiederholt.\\\\n•Autoritätsvektor akonvergiert gegen den \\\\nersten Eigenvektor von ATA. \\\\n•Hubvektor, h, konvergiert gegen den ersten \\\\nEigenvektor von AA T.\\\\n•In der Praxis liefern 20 Wiederholungen ziemlich stabile Ergebnisse. \\', \\'21 Ergebnisse \\\\n•Autoritäten für Anfrage “Java”\\\\n–java.sun.com \\\\n–comp.lang.java FAQ \\\\n•Autoritäten für Anfrage “search engine”\\\\n–Yahoo.com \\\\n–Excite.com \\\\n–Lycos.com \\\\n–Altavista.com \\\\n•Autoritäten für Anfrage “Gates”\\\\n–Microsoft.com \\\\n–roadahead.com \\\\n(Nach [Kleinberg 1998]) \\', \\'22 Beobachtung \\\\n•In den meisten Fällen waren die endgültigen Autoritäten nicht in der anfänglichen Wurzelmenge, die mit Altavista bestimmt wurde.\\\\n•Autoritäten wurden durch Vor- und Rückwärtslinks hinzugefügt (und dann durch HITS als Autorität bestimmt). \\', \\'23 Finden ähnlicher Seiten durch Verwendung der \\\\nLinkstruktur \\\\n•Aufgabe: Bestimmung ähnlicher Seiten zu einer Seite P. \\\\n(Dieser Ansatz findet Autoritäten in der “Link-Nachbarschaft” von P.) \\\\n•Sei t  gegeben (z.B. t = 200). \\\\n•Sei Reine Menge von tSeiten, die auf P verweisen (die \\\\nWurzelmenge). \\\\n•Bestimme die Basismenge Svon R wie o.a.\\\\n•Lasse HITS auf S laufen.\\\\n•Gebe die besten Autoritäten in Sals die “ähnlichsten Seiten \\\\nvon P” zurück.\\', \\'24 Ergebnisse der Ähnlichkeitssuche \\\\n•Gegeben “honda.com”\\\\n–toyota.com \\\\n–ford.com \\\\n–bmwusa.com \\\\n–saturncars.com \\\\n–nissanmotors.com \\\\n–audi.com \\\\n–volvocars.com \\', \\'25 PageRank \\\\n•Alternative Link-Analyse-Methode, die von  Google verwendet wird \\\\n(Brin & Page, 1998) .\\\\n•Versucht nicht, die Unterscheidung zwischen Hubs und Autoritäten zu erfassen, sondern klassifiziert Seiten nur nach Autorität.\\\\n•Wird eher auf das gesamte Web angewandt als auf eine lokale Nachbarschaft von Seiten, die die Ergebnisse einer Anfrage umgeben.\\\\nLarry Page und Sergei Brin \\', \\'26 Grundidee PageRank \\\\n•Die Messung des In-Grades alleine (Zitatzählung) berücksichtigt nicht die Autorität der Quelle eines Links. \\\\n•(Vereinfachte) PageRank-Gleichung für Seite p:\\\\n–Nqist die Gesamtzahl der Out-Links von Seite q.\\\\n–Eine Seite qgibt einen gleichen Anteil ihrer Autorität\\\\nan alle Seiten weiter, auf die sie verweist (z.B. auf p). ∑\\\\n→=\\\\npqq qNqRpR\\\\n:)()(\\', \\'27 Grundidee PageRank \\\\n•PageRank “fließt” entlang der Kanten:\\\\n.1 \\\\n.09 .05 .05 \\\\n.03 \\\\n.03 .03 .08 \\\\n.08 \\\\n.03 \\', \\'28 Grundidee PageRank \\\\n•Wiederhole den Fluss-Prozess bis zur Konvergenz:\\\\nSei S die Gesamtmenge der Seiten.\\\\nInitialisiere für alle p∈S:  R (p) = 1/| S| \\\\nBis sich Werte nicht mehr (viel) ändern \\\\n(Konvergenz )\\\\nFür jedes p∈S: \\\\nnormalisiere R∑\\\\n→=′\\\\npqq qNqRpR\\\\n:)()(\\', \\'29 Beispiel: stabiler Fixpunkt \\\\n0.4 \\\\n0.4 0.2 \\\\n0.2 0.2 \\\\n0.2 \\\\n0.4 \\', \\'30 Lineare-Algebra-Version \\\\n•Betrachte r:= (R(p)) p∈Sals einen Vektor in R|S|.\\\\n•Sei Adie | S|×|S|-Matrix mit \\\\nAvu := 1/ Nufalls u →v, und Avu := 0 sonst.\\\\n•Dann gilt am Ende des Algorithmus r = Ar,  \\\\nd.h.rkonvergiert zu dem Eigenvektor von A, der \\\\nzum Eigenwert 1 gehört.\\', \\'31 Problem mit anfänglicher Idee \\\\n•Eine Gruppe von Seiten, die nur auf sich selbst verweist, aber auf die durch andere Seiten verwiesen wird, agiert als eine Gewichts-Senke, die das ganze Gewicht absorbiert.\\\\n•“Suchmaschinenoptimierer” nutzen diesen Effekt in “Linkfarmen” aus. \\\\nPageRank fließt im Kreis und kann nicht heraus \\', \\'32 Gewichts-Quelle \\\\n•Führe eine Gewichts-Quelle Eein, die \\\\nkontinuierlich den Rank jeder Seite pdurch \\\\neinen festen Betrag E(p) ergänzt.\\\\n•Mit α∈[0,1] kann der Einfluss von E \\\\ngesteuert werden, Brin & Page haben mit α\\\\n= 0.85 gute Ergebnisse erzielt.)() 1 ()()(\\\\n:pENqRpR\\\\npqq qα\\\\nα −+ =∑\\\\n→\\', \\'33 PageRank-Algorithmus \\\\nSei S die Gesamtmenge der Seiten.\\\\nSei α∈(0,1), z.B. α= 0.85. \\\\nFür alle p∈S:E(p) := 1/| S| \\\\nFür alle p∈Sinitialisiere R(p) := 1/| S| . \\\\nBis sich die Gewichte nicht mehr (viel) ändern (Konvergenz ):\\\\n)() 1 ()()(\\\\n:pENqRpR\\\\npqq qα\\\\nα −+ =∑\\\\n→\\', \\'34 Lineare-Algebra-Version \\\\n•Nach Konvergenz gilt r= αAr+ (1- α)E.\\\\n•Wegen || r|| 1=1 gilt  r= ( αA+ (1- α)E×1)r\\\\nwobei 1 der Vektor ist, der nur aus 1en besteht.\\\\n•Somit ist rein Eigenvektor von αA+ (1- α)E×1.\\', \\'35 Random-Surfer-Modell \\\\n•PageRank kann als Modelierung eines “willkürlichen Surfers” betrachtet werden, der auf einer beliebigen Seite startet und dann entweder \\\\n–mit der Wahrscheinlicheit E(p) willkürlich auf die Seite p springt \\\\n–oder willkürlich einem Link auf der aktuellen Seite folgt .\\\\n•R(p) modelliert dann die Wahrscheinlichkeit, dass sich \\\\ndieser willkürliche Surfer zu jeder gegebenen Zeit auf der Seite pbefindet.\\\\n•Die “Sprünge” in Ewerden benötigt, um zu vermeiden, dass \\\\nder willkürliche Surfer in Web-Senken “gefangen” wird, aus denen kein Link herausführt. \\', \\'36 Konvergenz \\\\n•Frühe Experimente bei Google verwendeten 322 Millionen Links. \\\\n•PageRank-Algorithmus konvergiert (mit einer kleinen Toleranz) in ca. 52 Wiederholungen.\\\\n•Die Anzahl der für Konvergenz erforderlichen Wiederholungen ist empirisch O(log n) (wobei n\\\\ndie Anzahl der Links ist). \\\\n•Daher ist die Berechnung ziemlich effizient.\\', \\'37 Einfache Titelsuche mit PageRank \\\\n•Verwende zunächst die einfache Boolesche Suche, um Titel von Webseiten zu suchen und sortiere die gefundenen Seiten dann nach ihrem PageRank.\\\\n•Beispiel-Suche nach “university” (aus [Page, Brin 1998]): \\\\n–Altavista gab eine beliebige Menge von Seiten mit “university” im Titel wieder (schien kurze URLs zu bevorzugen). \\\\n–Primitives Google gab die Homepages der amerikanischen Top-Universitäten wieder. \\', \\'38 Google-Suche \\\\n•Komplette Google-Suche umfasste vor der Kommerzialisierung (basierend auf wissenschaftlichen Veröffentlichungen): \\\\n–Vektorraummodell \\\\n–Abstandsmaß zu Schlüsselwörtern \\\\n–HTML-Tag-Gewichtung (z.B. Titelpräferenz)\\\\n–PageRank \\\\n•Details zu aktuellen Google-Komponenten sind Betriebsgeheimnisse.\\', \\'39 Personalisierter PageRank \\\\n•PageRank kann durch Ändern von Ebeeinflusst \\\\n(personalisiert) werden: Beschränken des “Random Surfers” auf eine Menge als relevant spezifizierter Seiten.\\\\n•Zum Beispiel durch Setzen von E(p) := 0, außer \\\\nauf der eigenen Homepage, wo E(p) := α\\\\n•Dies führt zu einer Ausrichtung auf Seiten, die im Webgraphen näher zu der eigenen Homepage sind. \\', \\'40 PageRank-basiertes Spidering \\\\n•Verwende PageRank, um den Spider auf “wichtige” Seiten zu leiten (zu fokussieren). \\\\n•Berechne PageRank unter Verwendung der aktuellen Menge der bearbeiteten Seiten.\\\\n•Sortiere die Anfrage-Warteschlange des Spiders auf der Basis des aktuell geschätzten PageRanks.\\', \\'41 Schlussfolgerungen zur Linkanalyse \\\\n•Die Linkanalyse verwendet als Suchhilfe Informationen über die Struktur des Webgraphen.\\\\n•Dies ist eine der wesentlichen Innovationen bei der Websuche \\\\n•... und der primäre Grund für den Erfolg von  Google. \\', \\'Information Retrieval in Folksonomies: \\\\nSearch and Ranking \\\\nAndreas Hotho, Robert Jäschke, \\\\nChristoph Schmitz, Gerd Stumme \\\\nPublished in York Sure and John Domingue, editor(s),  The Semantic Web: Research \\\\nand Applications, LNAI 4011, pages 411-426, Springer , Heidelberg, 2006. Ausblick : Suche \\\\nim \\\\nW\\\\neb 2.0 \\', \\'43 \\\\nBibSonomy - a Folksonomy/Web 2.0 System \\\\nSocial Resource sharing systems: \\\\n/square6Collaborative annotation of web resources \\\\n/square6“Tagging” of resources with freely chosen keywords \\\\n/square6Ease of use, open for everybody \\\\n/square6Direct advantage with low additional expenses \\\\n/square6Complementing semantic web effort \\\\n/square6Emergent semantics \\', \\'44 \\\\nBibSonomy - a Folksonomy/Web 2.0 System \\\\nSearch in Social Bookmark systems: \\\\n/square6search for tag and user/tag possible \\\\n/square6result list is usually very long and ranked only by date (e.g. web2.0) \\\\n/square6restriction with additional tags possible (e.g. ajax)\\\\n/square6a good ranking would be very helpful \\\\n/square6main information in a folksonomy: user posting items with a certain tag if it is of interest \\', \\'45 \\\\nSearch in Folksonomies \\\\n/square6PageRank in the web: pages are important if a lot of important \\\\npages are linking to them \\\\n/square6authority values in a folksonomy are propagated along th e \\\\nhyperlink structure of the folksonomy \\\\nWeb-Graph                                                           Folksonomies User 3 \\\\nUser 4 User 2 \\\\nUser 3 \\\\nUser 4 User 2 \\\\nUser 3 \\\\nUser 4 User 1 \\\\nUser 2 \\\\nUser 3 \\\\nUser 4 User 3 \\\\nUser 4 User 2 \\\\nUser 3 \\\\nUser 4 User 2 \\\\nUser 3 \\\\nUser 4 Tag 1 \\\\nTag 2 \\\\nTag 3 \\\\nRes 1 \\\\nRes 2 \\\\nRes 3 \\', \\'46 \\\\nFormal Model \\\\nA folksonomy is a tuple F:= ( U,T,R,Y , p) where \\\\nU, T, and Rare finite sets, whose elements are called users, \\\\ntags and resources, resp.\\\\nYis a ternary relation between them, i.e. Y ⊆U ×T ×R, called \\\\ntag assignments (TAS for short) \\\\nand ≺is a user specific subtag/supertag relation, i.e. \\\\n≺⊆U×T×T, called subtag/supertag relation. \\', \\'47 \\\\nConverting a Folksonomy into an Undirected Graph \\\\nSet Vof nodes consists of the disjoint union of the sets  of tags, \\\\nusers and resources: \\\\nV= U∪T∪R\\\\nAll co-occurrences of users and tags, tags and reso urces, users \\\\nand resources become edges between the respective n odes: \\\\nE= {{ u,t } | ∃r∈R: ( u,t,r ) ∈Y}  ∪\\\\n{{ t,r }  | ∃u∈U: ( u,t,r ) ∈Y} ∪\\\\n{{ u,r } | ∃t∈T: ( u,t,r ) ∈Y}\\', \\'48 \\\\nRecall: PageRank \\\\nOriginal PageRank: \\\\nComputation of fixed point rof the weight spreading function \\\\nr:= αAr + (1- α)e\\\\n•Ais the row-normalized adjacency matrix reflecting t he graph \\\\n•e: random surfer vector \\\\n•α: weighting factor, eg α= 0.85 \\\\nAdaptation to folksonomy: \\\\n/square6each undirected edge /barb2righttwo directed edges \\', \\'49 \\\\nFolkRank: Thematic Ranking in Folksonomies \\\\nProblem with the adapted PageRank version: \\\\nGraph is undirected /barb2rightweight flows in one direction and directly \\\\n“swashes back”\\\\nIdea to solve this is to apply a differential approach: \\\\nLet RAP be the fixed point with α= 1 \\\\nLet Rpref be the fixed point with α< 1 \\\\nR:= Rpref –RAP is the final weight vector \\\\nAdditionally: different weights in random surfer vector allow for \\\\ntopic-specific ranking.\\', \\'50 \\\\nEvaluation on del.icio.us dataset \\\\nCrawl of del.icio.us from July 27 to 30,  2005 resulted in a folksonomy with |U| = 75,242 users, \\\\n|T| = 533,191 tags and \\\\n|R| = 3,158,297 resources, related by in total \\\\n|Y| = 17,362,212 tag assignments (TAS). \\', \\'51 \\\\nResults: adapted PageRank \\\\n\\', \\'52 \\\\nResults: adapted PageRank \\\\n0,0000984 http://www.engadget.com/0,0000992 http://www.lucazappa.com/brilliantMaker/buttonImage .php 0,0001009 http://www.lifehacker.com/0,0001015 http://www.technorati.com/0,0001020 http://www.alvit.de/web-dev/0,0001034 http://www.techsupportalert.com/best_46_free_utilit ies.htm 0,0001035 http://www.beelerspace.com/index.php?p=890 0,0001058 http://postsecret.blogspot.com/0,0001059 http://www.alistapart.com/0,0001070 http://pro.html.it/esempio/nifty/0,0001108 http://wellstyled.com/tools/colorscheme2/index-en.h tml 0,0001149 http://www.csszengarden.com/0,0001160 http://www.43folders.com/ 0,0001349 http://www.goodfonts.org/0,0001376 http://www.flickr.com/0,0001407 http://en.wikipedia.org/wiki/Main_Page 0,0001593 http://johnvey.com/features/deliciousdirector/0,0001654 http://www.adaptivepath.com/publications/essays/arc hives/000385.php 0,0001770 http://script.aculo.us/0,0002320 http://pchere.blogspot.com/2005/02/absolutely-delic ious-complete-tool.html 0,0002613 http://slashdot.org/\\\\n0,0000984 http://www.engadget.com/0,0000992 http://www.lucazappa.com/brilliantMaker/buttonImage .php 0,0001009 http://www.lifehacker.com/0,0001015 http://www.technorati.com/0,0001020 http://www.alvit.de/web-dev/0,0001034 http://www.techsupportalert.com/best_46_free_utilit ies.htm 0,0001035 http://www.beelerspace.com/index.php?p=890 0,0001058 http://postsecret.blogspot.com/0,0001059 http://www.alistapart.com/0,0001070 http://pro.html.it/esempio/nifty/0,0001108 http://wellstyled.com/tools/colorscheme2/index-en.h tml 0,0001149 http://www.csszengarden.com/0,0001160 http://www.43folders.com/ 0,0001349 http://www.goodfonts.org/0,0001376 http://www.flickr.com/0,0001407 http://en.wikipedia.org/wiki/Main_Page 0,0001593 http://johnvey.com/features/deliciousdirector/0,0001654 http://www.adaptivepath.com/publications/essays/arc hives/000385.php 0,0001770 http://script.aculo.us/0,0002320 http://pchere.blogspot.com/2005/02/absolutely-delic ious-complete-tool.html 0,0002613 http://slashdot.org/\\', \\'53 \\\\nResults: boomerang \\\\nPageRank without preference PageRank with preference Folk Rank with preference Preference for tag: boomerang \\\\n\\', \\'54 \\\\nResults: Semantic Web \\\\nPageRank without preference PageRank with preference Folk Rank with preference Preference for ressource: http://www.semanticweb.or g \\', \\'55 \\\\nResults: Semantic Web \\\\n0,0001052 http://www.shirky.com/writings/semantic_syllogism.h tml 0,0001059 http://pchere.blogspot.com/2005/02/absolutely-delic ious-complete-tool.html 0,0001060 http://www.federalconcierge.com/WritingBusinessCase s.html 0,0001102 http://www.alistapart.com/0,0001167 http://jena.sourceforge.net/0,0001195 http://shirky.com/writings/ontology_overrated.html 0,0001216 http://www.daml.org/0,0001224 http://www.letterjames.de/index.html 0,0001224 http://www.google.be/0,0001256 http://itip.evcc.jp/itipwiki/0,0001395 http://simile.mit.edu/0,0001613 http://www.aaai.org/AITopics/html/ontol.html 0,0001617 http://www.ontoweb.org/0,0001637 http://www.adaptivepath.com/publications/essays/arc hives/000385.php 0,0001712 http://mspace.ecs.soton.ac.uk/0,0001745 http://del.icio.us/register 0,0002162 http://infomesh.net/2001/swintro/0,0003216 http://www.w3.org/2001/sw/0,0003828 http://simile.mit.edu/piggy-bank/0,0005566 http://flink.semanticweb.org/0,3761957 http://www.semanticweb.org/\\\\n0,0001052 http://www.shirky.com/writings/semantic_syllogism.h tml 0,0001059 http://pchere.blogspot.com/2005/02/absolutely-delic ious-complete-tool.html 0,0001060 http://www.federalconcierge.com/WritingBusinessCase s.html 0,0001102 http://www.alistapart.com/0,0001167 http://jena.sourceforge.net/0,0001195 http://shirky.com/writings/ontology_overrated.html 0,0001216 http://www.daml.org/0,0001224 http://www.letterjames.de/index.html 0,0001224 http://www.google.be/0,0001256 http://itip.evcc.jp/itipwiki/0,0001395 http://simile.mit.edu/0,0001613 http://www.aaai.org/AITopics/html/ontol.html 0,0001617 http://www.ontoweb.org/0,0001637 http://www.adaptivepath.com/publications/essays/arc hives/000385.php 0,0001712 http://mspace.ecs.soton.ac.uk/0,0001745 http://del.icio.us/register 0,0002162 http://infomesh.net/2001/swintro/0,0003216 http://www.w3.org/2001/sw/0,0003828 http://simile.mit.edu/piggy-bank/0,0005566 http://flink.semanticweb.org/0,3761957 http://www.semanticweb.org/\\', \\'56 \\\\nConclusion \\\\nFolksonomies might overcome the knowledge aquisition \\\\nbottleneck through ease of use and growing amount of user s.\\\\nOur ranking is just based on the structure of the folksono my \\\\n– the content of the resources is not used.\\\\nSuitable for intranets, where \\\\n/square6resources are typically not hyperlinked,\\\\n/square6community building is important.\\', \\'1Recommender-Systeme \\\\nKollaboratives Filtern &\\\\ninhaltsbasierte Empfehlungen \\', \\'2Empfehlungs-Systeme \\\\n•Systeme, um Nutzern Dinge zu empfehlen (z.B. Bücher, Filme, CDs, Webseiten, Newsgroup-Nachrichten), die auf ihren vorigen Präferenzen basieren. \\\\n•Viele Online-Läden liefern Empfehlungen (z.B. Amazon, CDNow). \\\\n•Recommender haben den Umsatz von Online-Läden erheblich gesteigert. \\\\n•Es gibt zwei grundlegende Ansätze für Empfehlungen: \\\\n–kollaboratives Filtern (a.k.a. soziales Filtern) \\\\n–inhaltsbasiertes Filtern \\\\n\\', \\'3Buch-Recommender \\\\nRed Mars \\\\nJuras-sic Park \\\\nLost World \\\\n2001 Found ation \\\\nDiffer-ence Engine \\\\nMachinelles \\\\nLernen Anwender \\\\nProfil \\\\nNeuro-mancer 2010 \\', \\'4Personalisierung \\\\n•Recommender sind spezielle Personalisierungs-Software. \\\\n•Die Personalisierung betrifft die Anpassung an individuelle Bedürfnisse, Interessen und Präferenzen jedes Anwenders. \\\\n•Recommender umfassen: \\\\n–Empfehlen \\\\n–Filtern \\\\n–Vorhersagen (z.B. Formular-Vervollständigungen) \\\\n•Aus geschäftlicher Perspektive werden Recommender als Teil des Customer Relationship Management (CRM) angesehen. \\', \\'5Machinelles Lernen und Personalisierung \\\\n•Machinelles Lernen kann das Lernen eines \\\\nAnwendermodells oder Profils eines bestimmten \\\\nBenutzers unterstützen, basierend auf: \\\\n–Interaktionsmustern \\\\n–bewerteten Beispielen \\\\n•Dieses Modell oder Profil kann dann verwendet werden um: \\\\n–Objekte zu empfehlen \\\\n–Informationen zu filtern \\\\n–Verhalten vorherzusagen \\', \\'6Kollaboratives Filtern \\\\n•Pflegen einer Datenbank mit Anwender-bewertungen einer Vielzahl von Objekten.\\\\n•Finde für einen gegebenen Anwender andere, ähnliche Anwender, deren Bewertungen stark mit dem aktuellen Anwender korrelieren.\\\\n•Empfehle Objekte, die von diesen ähnlichen Anwendern hoch eingestuft werden, aber vom aktuellen Anwender noch nicht bewertet wurden.\\\\n•Nahezu alle vorhandenen kommerziellen Recommender verwenden diesen Ansatz (z.B. Amazon). \\', \\'7Kollaboratives Filtern \\\\nA  9 B  3 C:    : Z  5 A  B  C  9 :    : Z 10 A  5 B  3 C:    :  Z  7 A  B  C  8 :   : Z  A  6 B  4 C:    : Z  A 10 B  4 C  8 .   . Z  1 \\\\nAnwender-Datenbank \\\\nAktiver \\\\nAnwender Korrelations-\\\\nÜbereinstimmung \\\\nA  9 B  3 C  .   . Z  5 A  9 B  3 C:    : Z  5 A 10 B  4 C  8 .   . Z  1 \\\\nextrahiere \\\\nEmpfehlungen C\\', \\'8Kollaborative Filtermethode \\\\n•Gewichte alle Anwender in Bezug auf ihre Ähnlichkeit mit dem aktiven Anwender. \\\\n•Wähle eine Teilmenge der Anwender aus (Nachbarn ), um sie zur Vorhersage zu verwenden. \\\\n•Normalisiere Bewertungen und berechne eine Vorhersage aus einem gewichteten Mittel der ausgewählten Nachbar-Bewertungen. \\\\n•Präsentiere Objekte mit den höchsten vorhergesagten Bewertungen als Empfehlungen. \\', \\'9Ähnlichkeitsgewichtung \\\\n•Verwende typischerweise den Pearson-Korrelations-koeffizienten zwischen Bewertungen für den aktiven Anwender aund einem weiteren Anwender u.\\\\nu ar rua\\\\nuarrcσ\\\\nσ),( covar \\\\n,=\\\\nraund rusind die Bewertungsvektoren für die mObjekte, die  \\\\nsowohl von aals auch von ubewertet wurden.\\\\nru,j ist die Bewertung von Anwender ufür das Objekt j. \\', \\'10 Kovarianz und Standard-Abweichung \\\\n•Kovarianz:\\\\n•Standard-Abweichung:mr rr r\\\\nrrm\\\\niu iu a ia\\\\nua∑\\\\n=− −\\\\n=1, , ) )( (\\\\n),( covar \\\\nmr\\\\nrm\\\\niix\\\\nx∑\\\\n==1,\\\\nmr rm\\\\nix ix\\\\nrx∑\\\\n=−\\\\n=12\\\\n, ) (\\\\nσ\\', \\'11 Signifikanz-Gewichtung \\\\n•Es ist wichtig, keinen Korrelationen zu vertrauen, die nur auf sehr wenigen gemeinsam bewerteten Objekten basieren.\\\\n•Verwende Signifikanzgewichte sa,u , die auf \\\\nder Anzahl gemeinsam bewerteter Objekte mbasieren.\\\\nuaua ua cs w, , ,=\\\\n\\\\uf8f4\\\\uf8fe\\\\uf8f4\\\\uf8fd\\\\uf8fc\\\\n\\\\uf8f4\\\\uf8f3\\\\uf8f4\\\\uf8f2\\\\uf8f1\\\\n≤>\\\\n=50  falls  50 50  falls     1\\\\n, mmm\\\\nsuamit \\', \\'12 Nachbar-Selektion \\\\n•Auswahl der zum aktiven Anwender aam \\\\nstärksten korrelierten Anwender, die dann als Quelle der Vorhersagen dienen. \\\\n•Der Standardansatz ist, die nähnlichsten \\\\nAnwender uzu verwenden, basierend auf  \\\\nden Ähnlichkeitsgewichten wa,u .\\\\n•Ein alternativer Ansatz ist es, alle Anwender einzuschließen, deren Ähnlichkeit zu aüber \\\\neiner gegebenen Schwelle liegt. \\', \\'13 Bewertungs-Vorhersage \\\\n•Sage unter Verwendung der n ausgewählten Nachbar-Anwender u ∈\\\\n{1,2,…, n} für den aktiven Anwender aeine Bewertung pa,i für jedes Objekt i\\\\nvoraus .\\\\n•Um die verschiedenen Bewertungsniveaus unterschiedlicher Anwender zu berücksichtigen, basieren wir die Vorhersagen auf der Differenz zumDurchschnitt aller Bewertungen des jeweiligen Nutzers. \\\\n•Gewichte die Bewertungsbeiträge des Anwenders nach ihrer Ähnlichkeit mi t \\\\ndem aktiven Anwender. \\\\n∑∑\\\\n==−\\\\n+=n\\\\nuuan\\\\nuu iuua\\\\na ia\\\\nwr r w\\\\nr p\\\\n1,1, ,\\\\n,) (\\', \\'14 Probleme mit kollaborativem Filtern \\\\n•Kaltstart : Es müssen bereits genügend andere Anwender im System se in, um \\\\neine Übereinstimmung zu finden.\\\\n•Seltenheit : Wenn viele Objekte empfohlen werden sollen, ist die Anw ender/ \\\\nBewertungsmatrix dünn besetzt – selbst wenn es viele Anwend er gibt – und es \\\\nist schwierig, Anwender zu finden, die die gleichen Obje kte bewertet haben.\\\\n•Erster Beurteiler : kollaboratives Filtern kann kein Objekt empfehlen, das  \\\\nnicht zuvor bewertet worden ist.\\\\n–Neue Objekte \\\\n–Exotische Objekte \\\\n•Popularitäts-Ausrichtung : kollaboratives Filtern kann jemandem mit sehr \\\\nspeziellen Vorlieben keine Objekte empfehlen.\\\\n–Die Methode neigt dazu, populäre Objekte zu empfehlen.\\', \\'15 Inhaltsbasiertes Empfehlen \\\\n•Empfehlungen basieren hier eher auf Informationen über den \\\\nInhalt (die Eigenschaften) \\\\nvon Objekten als auf den Meinungen anderer Anwender. \\\\n•Verwendet einen Algorithmus für maschinelles Lernen, um ein Profil der Anwenderpräferenzen aus Beispielen zu erzeugen, die auf Merkmalsbeschreibungen des Inhalts basieren.\\\\n•Einige existierende Anwendungen:\\\\n–Newsweeder (Lang, 1995) \\\\n–Syskill und Webert (Pazzani et al., 1996) \\', \\'16 Vorteile eines inhaltsbasierten Ansatzes \\\\n•Kein Bedarf an Daten über andere Anwender.\\\\n–Kein Kaltstart-Problem und keine Seltenheitsprobleme.\\\\n•Ist fähig, Anwendern mit eindeutigen Vorlieben Empfehlungen auszusprechen \\\\n•Ist fähig, neue und unpopuläre Objekte zu empfehlen \\\\n–Kein Erst-Beurteiler-Problem.\\\\n•Kann Erläuterungen zu den empfohlenen Objekten durch die Auflistung der Inhaltsmerkmale liefern, die die Empfehlung bewirkten. \\', \\'17 Nachteile der inhaltsbasierten Methode \\\\n•Erfordert Inhalt, der sinnvoll durch Merkmale kodiert werden kann.\\\\n•Anwender-Vorlieben müssen als lernbare Funktion dieser Inhaltsmerkmale dargestellt werden können. \\\\n•Nicht fähig, Qualitätsbeurteilungen anderer Anwender auszuwerten.\\\\n–Es sei denn, diese sind irgendwie in den Inhaltsmerkmalen enthalten.\\', \\'18 LIBRA \\\\nLearning Intelligent Book Recommending Agent \\\\n•Inhaltsbasierender Recommender für Bücher, der Informationen über Titel verwendet, die von Amazon extrahiert wurden. \\\\n•Verwendet Informations-Exktration aus dem Web, um Text in Feldern zu organisieren:\\\\n–Autor \\\\n–Titel \\\\n–Redaktionelle Gutachten \\\\n–Kommentare von Kunden \\\\n–Themenbezeichnungen \\\\n–Verwandte Autoren \\\\n–Verwandte Titel \\', \\'19 LIBRA System \\\\nAmazon-Seiten \\\\nbewertete \\\\nBeispiele \\\\nAnwenderprofil Machinelles Lernen \\\\nLerner Informations-\\\\nExtraktion LIBRA \\\\nDatenbank \\\\nEmpfehlungen \\\\n1.~~~~~~ 2.~~~~~~~ 3.~~~~~ :::\\\\nPrädiktor \\', \\'20 \\\\n\\', \\'21 Bsp.: von Amazon extrahierte Informationen \\\\nTitel : <The Age of Spiritual Machines: When Computers Ex ceed Human Intelligence> \\\\nAutor :  <Ray Kurzweil>\\\\nPreis : <11.96> \\\\nDatum der Veröffentlichung : <Januar 2000> \\\\nISBN : <0140282025> \\\\nzugehörige Titel :  <Titel: <Robot: Mere Machine or Transcendent Min d> \\\\nAutor: <Hans Moravec> > \\\\n…\\\\nReviews : <Autor: <Amazon.com Reviews> Text: <How much do w e humans…> > \\\\n…\\\\nStellungnahmen : <Stars: <4> Autor: <Stephen A. Haines> Text:<Kurz weil has …> > \\\\n…\\\\nzugehörige Autoren : <Hans P. Moravec> <K. Eric Drexler>…\\\\nThemen : <Science/Mathematics> <Computers> <Artificial Int elligence> …\\', \\'22 Libra: Inhaltsinformationen \\\\n•Libra verwendet diese Information, um die folgenden Slots mit “bags of words” zu füllen:\\\\n–Autor \\\\n–Titel \\\\n–Beschreibung (Stellungnahmen und Kommentare)\\\\n–Themen \\\\n–verwandte Titel \\\\n–verwandte Autoren \\', \\'23 Libra: Übersicht \\\\n•Anwender bewertet ausgewählte Titel auf einer Skala von 1 bis 10. \\\\n•Libra verwendet einen Naive-Bayes-Text-Kategorisierungs-Algorithmus, um daraus ein Profil zu erlernen.\\\\n–Bewertung 6–10:  positiv \\\\n–Bewertung 1–5:    negativ \\\\n•Von den anderen Büchern werden diejenigen als Empfehlungen klassifiziert, bei denen die errechnete Wahrscheinlichkeit für eine positive Einschätzung großgenug ist.\\\\n•Der Anwender kann auch explizite positive/negative Schlüsselwörter liefern, die dann höher gewichtet werden.\\', \\'24 Bayessche Kategorisierung in LIBRA \\\\n•Das Modell ist verallgemeinert, um einen Vektor von \\\\n“bags of words” zu erzeugen (ein “bag” für jeden Slot). \\\\n–Instanzen desselben Worts in verschiedenen Slots werden als separate Merkmale behandelt:\\\\n•“Chrichton” bei Autor – “Chrichton” in der Beschreibung \\\\n•Trainingsbeispiele werden als positiv oder negativ gewichtete Beispiele bei der Schätzung der bedingten \\\\nWahrscheinlichkeitsparameter behandelt:\\\\n–Gegeben ist ein Beispiel mit Bewertung 1 ≤r≤10: \\\\npositive Wahrscheinlichkeit: ( r– 1)/9 ( wr)\\\\nnegative Wahrscheinlichkeit: (10 – r)/9 \\\\nWenn ein Wort in nTrainingsbeispielen mit Rating rvorkommt: \\\\n- für die positive Klasse wird dann n*w rgezählt \\', \\'25 Implementierung \\\\n•Stopwörter wurden von allen “bags of words”entfernt.\\\\n•Buchtitel und Autoren werden auch zu den Slots “verwandte Titel” bzw. “verwandte Autoren”hinzugefügt.\\\\n•Alle Wahrscheinlichkeiten werden wegen der kleinen Datenbasis mit der Laplace-Schätzung geglättet.\\\\n•Lisp-Implementierung ist ziemlich effizient:\\\\n–Training : 20 Datensätze in 0.4 s, 840 in 11.5 s \\\\n–Test : 200 Bücher pro Sekunde \\', \\'26 Erläuterung von Profilen und \\\\nEmpfehlungen \\\\n•Stärke des Auftretens von Wort wkin Slot sj:\\\\n)s negative, |(), positive |(log ),( strength \\\\nj kj k\\\\nj kwPs wPsw =\\', \\'27 Experimentelle Daten \\\\n•Bücher aus verschiedenen Genres von Amazon. \\\\n•Titel, die zumindest eine Stellungnahme oder einen Kommentar haben, wurden ausgewählt.\\\\n•Datensätze:\\\\n–Literaturfiktion: 3061 Titel \\\\n–Mystery: 7285 Titel \\\\n–Wissenschaft: 3813 Titel \\\\n–Science Fiction: 3813 Titel \\', \\'28 Bewertete Daten \\\\n•Vier Anwender bewerteten zufällige Beispiele innerhalb eines Genres, indem sie die  Amazon-Seiten hinsichtlich der Titel durchsahen:\\\\n–LIT1   936 Titel \\\\n–LIT2   935 Titel \\\\n–MYST 500 Titel \\\\n–SCI      500 Titel \\\\n–SF        500 Titel \\', \\'29 Experimentelle Methode \\\\n•10-fache Kreuz-Validierung, um Lernkurven zu erzeugen.\\\\n•Auf unabhängigen Testdaten wurde gemessen: \\\\n–Präzision der Top 3 : % der besten 3, die positiv sind \\\\n–Bewertung der Top 3 :  Durchschnittsbewertung, die den \\\\nbesten 3 zugeordnet ist \\\\n–Klassifizierungs-Korrelation : Spearmans rszwischen \\\\nvollständigen Klassifizierungen von System und Anwender.\\\\n•Test ohne die Slots “verwandter Autor” und “verwandter Titel” (LIBRA-NR). \\\\n–Testet den Einfluss von Informationen, die durch den kollaborativen Ansatz von Amazon erzeugt wurden.\\', \\'30 Experimentelle Ergebnisse: \\\\nZusammenfassung \\\\n• Präzision der Top 3 ist nach nur 20 Beispielen mit \\\\num die 90 % ziemlich konsistent.\\\\n• Bewertung der Top 3 ist nach nur 20 Beispielen \\\\nmit über 8ziemlich konsistent. \\\\n•Alle Ergebnisse sind nach nur 5 Beispielen immer \\\\nbedeutend besser als eine zufällige Auswahl.\\\\n• Klassifizierungs-Korrelation liegt nach nur 10 \\\\nBeispielen generell über 0.3 (gemäßigt) .\\\\n• Klassifizierungs-Korrelation liegt nach nur 40 \\\\nBeispielen generell über 0.6 (hoch) .\\', \\'31 Präzision der Top 3 für “Science”\\\\n\\', \\'32 Bewertung der Top 3 für “Science”\\\\n\\', \\'33 Klassifizierungs-Korrelation für “Science”\\\\n\\', \\'34 Anwenderstudie \\\\nNutzer \\\\n•wurden gebeten, Libra zu verwenden und Empfehlungen anzufragen.\\\\n•wurden zu mehreren Feedback-Runden ermutigt.\\\\n•bewerteten alle Bücher der endgültigen Empfehlungs-Liste.\\\\n•wählten zwei Bücher zum Kauf.\\\\n•schickten nach Lesen der Auswahl Stellungnahmen zurück.\\\\n•vervollständigten Fragebögen über das System. \\', \\'35 Verbinden von Inhalt und Kollaboration \\\\n•Inhaltsbasierte und kollaborative Methoden haben komplementäre Stärken und Schwächen. \\\\n•Kombiniere Methoden, um das Beste von beiden zu erhalten. \\\\n•Es gibt verschiedene hybride Ansätze: \\\\n–Wende beide Methoden an und verknüpfe die Empfehlungen. \\\\n–Verwende kollaborative Daten als Inhalt. \\\\n–Verwende inhaltsbasierte Empfehlungen als weiteren Nutzer, dessen Verhalten in weitere Vorhersagen einfließt. \\\\n–Verwende einen inhaltsbasierten Empfehler, um kollaborative Daten zu vervollständigen.  \\', \\'36 Anwendung: Filme \\\\n•EachMovie Datensatz [Compaq Research Labs] \\\\n–enthält Anwenderbewertungen für Filme auf einer Skala von 0 bis 5. \\\\n–72,916 Anwender (mit durchschn. 39 Bewertungen). \\\\n–1,628 Filme. \\\\n–Spärliche besetzte Anwender-Bewertungsmatrix – (zu 2.6% besetzt). \\\\n•Internet Movie Database ( IMDb )\\\\n–Wurde für die Titel aus EachMovie gecrawlt.\\\\n•Wesentliche Filminformationen: \\\\n–Titel, Direktor, Rollenbesetzung, Genre, etc. \\\\n•Populäre Meinungen: \\\\n–Nutzerkommentare, Zeitungs- und Newsgroup-Berichte, etc. \\', \\'37 Content-Boosted kollaboratives Filtern \\\\nIMDb EachMovie Web Crawler \\\\nMovie \\\\nContent \\\\nDatabase \\\\nFull User \\\\nRatings Matrix \\\\nCollaborative \\\\nFiltering Active \\\\nUser Ratings User Ratings \\\\nMatrix (Sparse) Content-based \\\\nPredictor \\\\nRecommendations \\', \\'38 Content-Boosted CF - I\\\\nContent-Based \\\\nPredictor Training Examples \\\\nPseudo User-ratings Vector \\\\nItems with Predicted Ratings User-ratings Vector \\\\nUser-rated Items Unrated Items \\', \\'39 Content-Boosted CF - II \\\\n•Berechne Pseudo-Anwenderbewertungsmatrix \\\\n–Volle Matrix – approximiert die konkreten Nutzerbewertungen \\\\n•Führe kollaboratives Filtern aus \\\\n–Unter Verwendung des Pearson-Korr.-Koeeffizient zwischen den Pseudo-Anwender-Bewertungsvektoren \\\\nUser Ratings \\\\nMatrix \\\\nPseudo User \\\\nRatings Matrix Content-Based \\\\nPredictor \\', \\'40 Experimentelle Evaluierung \\\\n•Teilmenge von EachMovie wurde verwandt: \\\\n–7,893 Anwender; 299,997 Bewertungen \\\\n•Testmenge: 10% der Anwender wurden zufällig ausgewählt.\\\\n–Sie bewerteten je mindestens 40 Filme.\\\\n–Training auf der verbleibenden Menge.\\\\n•“Hold-out”-Menge: 25% der Objekte für jeden Testanwender.\\\\n–Vorhersage der Bewertung für jedes Objekt in der “Hold-out”-Menge.\\\\n•Vergleich CBCF mit anderen Ansätzen:\\\\n–Reines CF \\\\n–Rein inhaltsbasiert \\\\n–hybrider Naïve Bayes (Durchschnitts-CF und inhaltsbasierende Vorhersagen)\\', \\'41 Maße\\\\n•Mittlerer absoluter Fehler (MAE) \\\\n–Vergleicht die Vorhersagen mit Anwenderbewertungen \\\\n•ROC-Empfindlichkeit [Herlocker 99] \\\\n–Wie gut helfen die Vorhersagen den Anwendern, eine gute Auswahl zu treffen? \\\\n–Bewertungen ≥4 als “gut” betrachtet; < 4 als “schlecht”\\\\nbetrachtet \\\\n•Gepaarter T-Test stellt die statistische Signifikan z \\\\nfest. \\', \\'42 Ergebnisse - I\\\\n0,9 0,92 0,94 0,96 0,98 11,02 1,04 1,06 MAE \\\\nAlgorithm MAE \\\\nCF \\\\nContent \\\\nNaïve \\\\nCBCF \\\\nCBCF ist signifikant besser (4% über CF) bei (p < 0.001)\\', \\'43 Ergebnis - II \\\\n0,58 0,6 0,62 0,64 0,66 0,68 ROC-4 \\\\nAlgorithm ROC Sensitivity \\\\nCF \\\\nContent \\\\nNaïve \\\\nCBCF \\\\nCBCF übertrifft Rest (5% Verbesserung gegenüber CF) \\', \\'44 Aktives Lernen \\\\n•Wird verwendet, um die Anzahl der Trainingsbeispiele zu verringern. \\\\n•System fordert Bewertungen für spezifische Objekte, von denen es vermutet, am meisten zu lernen. \\\\n•Verschiedene existierende Methoden: \\\\n–Uncertainty Sampling \\\\n–Komitee-basiertes Sampling \\', \\'45 Halbüberwachtes Lernen \\\\n(weakly supervised, Bootstrapping) \\\\n•Verwende die große Anzahl ungekennzeichneter Beispiele, um das Lernen von einer kleinen Menge von gekennzeichneten Beispielen zu unterstützen.\\\\n•Einige neue Methoden:\\\\n–Halbüberwachte EM (Erwartungsmaximierung)\\\\n–Ko-Training \\\\n–Transduktive SVM’s\\', \\'46 Schlussfolgerungen \\\\n•Empfehlungen und Personalisierung sind wichtige Ansätze zur Bekämpfung der Informations-Überfrachtung. \\\\n•Machinelles Lernen ist ein wichtiger Teil der Systeme zur Lösung diese Aufgaben. \\\\n•Kollaboratives Filtern hat Probleme. \\\\n•Inhaltsbasierende Methoden sprechen diese Probleme an (haben aber eigene Probleme). \\\\n•Das Beste ist, beides zu integrieren. \\', \\'1Textkategorisierung \\\\n(Text Classification)\\', \\'2Kategorisierung \\\\n•Gegeben: \\\\n–Eine Beschreibung einer Instanz, x∈X, wobei X\\\\nder Raum der Instanzen ist. \\\\n–Eine festgelegte Menge von Kategorien/Klassen:                  \\\\nC= {c1, c2, …, cn}\\\\n•Bestimme: \\\\n–Die Kategorie c(x)∈Cvon x. \\\\nDie Funktion c:X →Cwird dann \\\\nKategorisierungsfunktion genannt. \\', \\'3Lernen einer Kategorisierung \\\\n•Ein Trainingsbeispiel ist eine Instanz x∈X, gepaart \\\\nmit seiner korrekten Kategorie c(x): \\\\n<x, c(x)> für eine unbekannte Kategorisierungs-\\\\nfunktion c. \\\\n•Gegeben ist eine Menge Dvon \\\\nTrainingsbeispielen. \\\\n•Finde eine hypothetische Kategorisierungs-funktion h(x), so dass gilt: \\\\n)()(:  )(, xcxhD xcx\\\\n= ∈> <∀\\\\nKonsistenz \\', \\'4Beispiel für ein Kategorie-Lernproblem \\\\n•Instanzraum: <Größe, Farbe, Form> \\\\n–Größe ∈{schmal, mittel, groß}\\\\n–Farbe ∈{rot, blau, grün} \\\\n–Form ∈{quadratisch, kreisförmig, dreieckig} \\\\n•C = {positiv, negativ} \\\\n•D:\\\\nnegativ dreieckig rot schmal 3positiv kreisförmig rot groß 2positiv kreisförmig rot schmal 1\\\\nnegativ kreisförmig blau groß 4Kategorie Form Farbe Größe Beispiel \\', \\'5Allgemeine Aspekte beim Lernen \\\\n•Gewöhnlich sind viele Hypothesen mit den Trainingsdaten konsistent. \\\\n•Tendenz (bias)\\\\n–Jedes andere Kriterium (außer der Konsistenz), das verwendet wird, um eine Hypothese auszuwählen. \\\\n•Klassifizierungsgenauigkeit (% Fälle sind korrekt klassifiziert). \\\\n–gemessen an unabhängigen Testdaten. \\\\n•Trainingszeit (Effizienz von Trainingsalgorithmen).\\\\n•Testzeit (Effizienz nachträglicher Klassifizierung) . \\', \\'6Verallgemeinerung \\\\n•Hypothesen müssen verallgemeinern, um die nicht in den Trainingsdaten vorhandenen Fälle korrekt zu klassifizieren. \\\\n•Einfaches Speichern von Trainings-beispielen ist eine konsistente Hypothese, die aber nicht verallgemeinert. \\\\n•Das Ökonomieprinzip :\\\\n–Das Finden einer einfachen Hypothese hilft, die \\\\nVerallgemeinerung zu gewährleisten. \\', \\'7Textkategorisierung \\\\n•Zuordnung von Dokumenten zu einer festgelegten Menge von Kategorien. \\\\n•Anwendungen: \\\\n–Webseiten \\\\n•Empfehlungen \\\\n•Yahoo-artige Klassifizierung \\\\n–Newsgroup-Nachrichten \\\\n•Empfehlungen \\\\n•Spamfiltern \\\\n–Zeitungsartikel \\\\n•Personalisierte Zeitung \\\\n–Email-Nachrichten \\\\n•Routing \\\\n•Priorisieren \\\\n•In Ordnern ablegen \\\\n•Spamfiltern \\', \\'8Textkategorisierung lernen – Warum?\\\\n•Die manuelle Entwicklung von Funktionen zur Textkategorisierung ist schwierig. \\\\n•Lernalgorithmen: \\\\n–Bayesian (naïve)\\\\n–Neuronales Netz \\\\n–Relevanz-Feedback (Rocchio)\\\\n–regelbasierender Lerner (Ripper)\\\\n–Nächster Nachbar (fallbasierend)\\\\n–Support-Vektor-Maschinen (SVM) \\', \\'9Nutzung von Relevance Feedback (Rocchio)\\\\n•Relevanz-Feedback-Methoden können zur Textkategorisierung angepasst werden. \\\\n•Verwende TF-IDF-gewichtete Vektoren, um Textdokumente darzustellen (normalisiert durch die maximale Termhäufigkeit). \\\\n•Berechne für jede Kategorie einen Prototyp-\\\\nVektor durch Addieren der Vektoren der Trainingsdokumente einer Kategorie. \\\\n•Ordne die Textdokumente der Kategorie mit dem nächsten Prototyp-Vektor basierend auf der Kosinusähnlichkeit zu. \\', \\'10 Rocchio Textkategorisierungs-\\\\nAlgorithmus (Training) \\\\nGegeben die Menge der Kategorien { c1, c2, …, cn}. \\\\nFür ivon 1 bis nsei pi= <0, 0,…,0>  (init. Prototypvektoren )\\\\nFür jedes Trainingsbeispiel < x, c(x)> ∈D\\\\nSei d der normalisierte Häufigkeits-TF-IDF-Termvektor des \\\\nDokumentes x\\\\nSetze iso dass ci= c(x)\\\\n(addiere alle Dokumentvektoren der Klasse ci,um pizu erhalten )\\\\nSetze pi= pi+ d     \\', \\'11 Rocchio Textkategorisierungs-\\\\nAlgorithmus (Test) \\\\nGegeben ist das Testdokument x aus der Testmenge. \\\\nSei d der TF-IDF-gewichtete Termvektor von x\\\\nSei m= –2      (init. maximum cosSim )\\\\nFür i= 1 bis n:\\\\n(berechne Ähnlichkeit mit Prototypvektor )\\\\ns= cosSim( d, pi)\\\\nIf (s> m) \\\\nm:= s\\\\nr := ci(aktualisiere den ähnlichsten Klassen-Prototypen )\\\\nreturn Klasse r\\', \\'12 Illustration der Rocchio-Textkategorisierung \\', \\'13 Rocchio: Eigenschaften \\\\n•Garantiert konsistente Hypothese nicht. \\\\n•Bildet eine einfache Verallgemeinerung der Beispiele einer jeden Klasse (einen Prototyp ). \\\\n•Der Prototypvektor braucht nicht gemittelt oder anderweitig in der Länge normalisiert werden, da die Kosinusähnlichkeit gegenüber der Vektorlänge unempfindlich ist. \\\\n•Die Klassifizierung basiert auf der Ähnlichkeit zu den Klassen-Prototypen. \\', \\'14 Rocchio: Zeitkomplexität\\\\n• Hinweis: Die Zeit, um zwei dünn besetzte Vektoren zu \\\\naddieren, ist proportional zur kleinsten Anzahl der nicht-Null-Einträge beider Vektoren. \\\\n• Trainingszeit :  O(| D|( Ld+ | Vd|)) = O(| D| Ld) \\\\nwobei Lddie Durchschnittslänge eines Dokuments in Dist und Vddie \\\\ndurchschnittliche Vokabulargröße eines Dokuments in  D ist. \\\\n• Testzeit : O( Lt+ |C||V t|) \\\\nwobei Ltdie Durchschnittslänge eines Testdokuments ist und | Vt| die \\\\ndurchschnittliche Vokabulargröße eines Testdokument s. \\\\n–Wir berechnen und speichern die Längen der piVektoren während der \\\\nTrainingsphase. Dies ermöglicht die Berechnung von cosSim( d, pi) mit \\\\neinem Aufwand, der proportional zur Anzahl der nich t-Null-Einträge in d\\\\n(d.h. |Vt|) ist. \\', \\'15 Nächster-Nachbar-Lernalgorithmus \\\\n•Das Lernen besteht nur im Speichern der Trainings-beispiele aus D in einer geeigneten Repräsentation. \\\\n•Testfall x:\\\\n–Berechne die Ähnlichkeit zwischen xund allen Beispielen in D.\\\\n–Ordne  xdie Kategorie des ähnlichsten Beispiels in D zu. \\\\n•Berechnet nicht explizit eine Verallgemeinerung oder einen Prototypen einer Kategorie. \\\\n•Man nennt das NN-Verfahren auch: \\\\n–fallbasiert \\\\n–speicherbasiert \\\\n–faules Lernen („lazy learning“)\\', \\'16 k-Nächste-Nachbarn \\\\n•Nur das nächste Beispiel zu verwenden, um die Kategorisierung zu bestimmen, ist häufig die Ursache von Fehlern, da: \\\\n–einzelne Beispiele atypisch sein können. \\\\n–Rauschen (d.h. Fehler) bei der Kategorie eines einzelnen Trainingsbeispieles vorkommen kann. \\\\n•Eine robustere Alternative ist, die kähnlichsten \\\\nBeispiele zu finden und die Kategorie der Mehrheit dieser kBeispiele zurückzuliefern. \\\\n•Der Wert von kist typischerweise ungerade, um \\\\nein Unentschieden zu vermeiden; 3 und 5 werden am häufigsten verwendet. \\', \\'17 Ähnlichkeitsmaße\\\\n•Die Nächste-Nachbar-Methode hängt von einem Ähnlichkeits- (oder Abstands-)maß ab. \\\\n•Das einfachste stetige m-dimensionale \\\\nAbstandsmaß ist die euklidische Distanz .\\\\n•Das einfachste diskrete m-dimensionale binäre \\\\nAbstandsmaß ist der Hamming-Abstand (Anzahl \\\\nder Merkmalswerte, die sich unterscheiden). \\\\n•Für Text ist die Kosinusähnlichkeit von TF-IDF-gewichteten Vektoren typischerweise am effektivsten. \\', \\'18 Beispiel für 3-nächste-Nachbarn \\\\n(Euklidischer Abstand)\\\\n..\\\\n..\\\\n..\\\\n.....\\', \\'19 K-nächster-Nachbar für Text \\\\nTraining: Für jedes Trainingsbeispiel < x, c(x)> ∈D\\\\nBerechne den TF-IDF Vektor dxfür Dokument x\\\\nTestfall y:\\\\nBerechne TF-IDF Vektor dyfür Dokument y\\\\nFür jedes < x, c(x)> ∈D\\\\nsx= cosSim( dy, dx)\\\\nSortiere die Beispiele x∈Dabsteigend nach sx\\\\nFülle Menge Nmit den ersten k Beispielen aus D     \\\\n(d.h. den \\\\nähnlichsten Nachbarn )\\\\nGib als Klasse die Klasse der Mehrheit der Beispiele in N zurück. \\', \\'20 Beispiel: 3-nächste-Nachbarn für Text \\', \\'21 Rocchio-Anomalie \\\\n•Prototyp-Modelle haben Probleme mit polymorphen (disjunktiven) Kategorien.\\', \\'22 3-nächster-Nachbar: Vergleich \\\\n•Das nächste-Nachbar-Verfahren kann mit polymorphen Kategorien besser umgehen. \\', \\'23 Nächste-Nachbarn: Zeit-Komplexität\\\\n• Trainingszeit : O(| D| Ld) um TF-IDF \\\\nVektoren zu berechnen. \\\\n• Testzeit : O( Lt+ |D||V t|) um mit allen \\\\nTrainingsvektoren zu vergleichen. \\\\n–Man nehme an, dass die Längen der dxVektoren \\\\nwährend der Trainingsphase berechnet und gespeichert wird. Dies ermöglicht d. Berechnung von cosSim( d, dx) \\\\nmit einem Aufwand der proportional zur Anzahl der nicht-Null Einträge in d(d.h. |Vt|) ist. \\\\n•Die Testzeit kann für große Trainings-mengen sehr groß werden. \\', \\'24 Nächste-Nachbarn mit invertiertem Index \\\\n•Die Bestimmung der knächsten Nachbarn ist das gleiche \\\\nwie die Bestimmung der k besten Abfragen, wobei das \\\\nTestdokument als Anfrage an eine Datenbank von Trainingsdokumenten verwendet wird. \\\\n•Verwende standard-KSM-invertierte-Indexmethoden, um die knächsten Nachbarn zu finden. \\\\n• Testzeit : O( B|V t|)\\\\nwobei Bdie durchschnittliche Anzahl der Trainingsdokumente  ist, in \\\\nwelchen ein Test-Dokumentwort vorkommt. \\\\n•Folglich ist die Komplexität der gesamten Klassifizierung O( Lt+ B|V t|) \\\\n–Typischerweise gilt  B << | D|\\', \\'25 Bayessche-Methoden \\\\n•Lern- und Klassifizierungsmethoden basierend auf der Wahrscheinlichkeitstheorie. \\\\n•Bayes-Theorem spielt eine zentrale Rolle beim probabilistischen Lernen und Klassifizieren. \\\\n•Ausgehend von der (prior) Wahrscheinlichkeit  der Kategorien (wenn keine Information über ein Objekt vorhanden ist) ... \\\\n•... berechnet die Methode eine Kategorisierung basierend auf der (posterior) Wahrscheinlichkeitsverteilung, wenn man annimmt, dass eine Beschreibung eines Objektes gegeben ist. \\\\n•Zentrale Annahme ist die Unabhängigkeit der Merkmale für eine gegebene Kategorie. \\', \\'26 Naïve Bayes für Text \\\\n•Texte werden als Menge von Wörtern ( Bag of \\\\nWords ) repräsentiert. \\\\n•Für das Modell wird angenommen, dass das „Bag of Words“ eines Dokumentes aus einer gegebenen Kategorie cidurch wiederholtes Ziehen mit \\\\nZurücklegen aus dem Vokabular V= {w1, w2,…,wm} \\\\nerzeugt wurde. \\\\n•Die Wahrscheinlichkeitsverteilung für Vist \\\\nP( wj| ci). \\\\n•Man versucht nun, dieses generative Modell beim Lernen aus den Daten zu schätzen. \\', \\'27 Naïve Bayes a posteriori Wahrscheinlichkeiten \\\\n•Klassifizierungsergebnisse von naïve Bayes (die Klasse mit der maximalen a posteriori Wahrscheinlichkeit) sind üblicherweise ziemlich akkurat. \\\\n•Jedoch trifft dies, aufgrund des Nichtzutreffens der bedingten Unabhängigkeitsannahme, nicht auf die tatsächlichen geschätzten a posteriori Wahrscheinlichkeiten zu. \\', \\'28 Evaluierung von Kategorisierungsmodellen \\\\n•In der Praxis werden die Modelle auf Daten angewand t, bei denen die \\\\nKategorien nicht bekannt sind. \\\\n•Für Testzwecke werden Daten benötigt, bei denen die se Zuordnung \\\\ngegeben ist. \\\\n•Die Evaluierung muss auf Testdaten erfolgen, die un abhängig von den \\\\nTrainingsdaten (üblicherweise eine unabhängige Meng e von \\\\nBeispielen) sind. \\\\n•Klassifizierungsgenauigkeit : c/nwobei ndie Gesamtzahl der Testfälle \\\\nist und cdie Anzahl von Testfällen, die korrekt vom System \\\\nklassifiziert wurden. \\\\n•Die Ergebnisse können wegen Stichprobenfehlern aufg rund \\\\nverschiedener Trainings- und Testmengen  variieren. \\\\n•Mittle die Ergebnisse über eine Vielzahl von Traini ngs- und Testläufen \\\\n(Teile der Gesamtdaten), um die Klassifizierungsgen auigkeit möglichst \\\\ngut zu bestimmen. \\', \\'29 N-fache Kreuz-Validierung \\\\n•Idealerweise sind Test- und Trainingsmengen bei jedem Versuch unabhängig voneinander. \\\\n–Aber dies würde zu viele gekennzeichnete Daten erfo rdern. \\\\n•Alternative: \\\\n–Teile Daten in Ngleichgroße unabhängige Teile. \\\\n–Führe NVersuche durch, wobei jedes Mal ein anderer Teil de r Daten \\\\nzum Testen verwendet wird und das Training mit den verbleibenden \\\\nN−1 Segmenten erfolgt. \\\\n•Auf diese Weise sind zumindest die Testmengen unabhängig. \\\\n•Das Gesamtergebnis ergibt sich als die durchschnittliche Klassifizierungsgenauigkeit über die NVersuche. \\\\n•Typischerweise wählt man  N= 10. \\', \\'30 Lernkurven \\\\n•In der Praxis sind gekennzeichnete Daten üblicherweise selten und teuer. \\\\n•Man würde gern wissen, wie die Leistung der Methoden mit der Anzahl der Trainingsbeispiele variiert. \\\\n•Lernkurven tragen die Klassifizierungs-\\\\ngenauigkeit unabhängiger Testdaten ( y-\\\\nAchse) über die Anzahl der Trainings-beispiele ( x-Achse) auf. \\', \\'31 N-fache Lernkurven \\\\n•Lernkurven sind nur dann aussagekräftig, wenn diese über eine Vielzahl von Versuchen gemittelt werden. \\\\n•Verwende N-fache Kreuzvalidierung, um N volle \\\\nTrainings- und Testmengen zu erzeugen. \\\\n•Trainiere bei jedem Versuch mit zunehmenden Teilen der gesamten Trainingsmenge, um durch das Messen  der Genauigkeit auf den Testdaten jeden Punkt auf der gewünschten Lernkurve zu ermitteln. \\', \\'32 Beispiel einer Lernkurve \\\\n(Yahoo Science Data) \\\\n\\', \\'1Text-Clustern \\', \\'2Clustern \\\\n•Teile nicht kategorisierte Beispiele in disjunkte Untermengen, so genannte \\\\nCluster, ein, so dass: \\\\n–Beispiele innerhalb eines Clusters sich sehr ähnlich \\\\n–Beispiele in verschiedenen Clustern möglichst unterschiedlich sind. \\\\n•Entdeckt neue Kategorien in einer unüberwachten \\\\nWeise: es werden keine Namen für die Kategorien vorab zur Verfügung gestellt (im Gegensatz zur Kategorisierung) \\', \\'3.Einführung Clustern \\\\n....\\\\n..\\\\n....\\\\n.........\\\\n..\\\\n....\\\\n......\\', \\'4Einführung Clustern \\\\ncase sex glasses moustache smile hat \\\\n1 m y n y n2 f n n y n3 m y n n n4 m n n n n5 m n n y? n6 m n y n y7 m y n y n8 m n n y n9 m y y y n\\\\n10 f n n n n11 m n y n n12 f n n n n\\', \\'5\\\\nEinführung Clustern \\', \\'6Typen von Clustering-Verfahren •Hierarchische Verfahren –Parameter: Distanz- oder Ähnlichkeitsfunktion für Punkte und für Cluster –bestimmen eine Hierarchie von Clustern, indem die jeweils ähnlichsten Cluster verschmolzen werden •Partitionierende Verfahren –Parameter: Anzahl kder Cluster, Distanzfunktion –sucht ein „flaches“ Clustering in kCluster mit minimalen Kosten •Dichtebasierte Verfahren –Parameter: minimale Dichte in einem Cluster, Distanzfunktion –erweitert Punkte um ihre Nachbarn solange Dichte groß genug •Andere Clustering-Verfahren –Fuzzy Clustering –Soft Clustering (EM) –Graph-theoretische Verfahren –neuronale Netze Mehr Details zu Clusterverfahren gibt es in der KDD Vorlesung. BCDEABCDEA\\\\n\\', \\'7Hierarchisches Clustern \\\\n•Bilde eine baum-basierte hierarchische Taxonomie (Dendrogram ) aus einer Menge von Beispielen. \\\\n•Die rekursive Anwendung eines Standard-Cluster-Algorithmus führt auch zu hierarchischen Clustern (z.B. Bi-Sec-KMeans). animal \\\\nvertebrate \\\\nfish reptile amphib. mammal      worm insect crustacean invertebrate \\', \\'8Agglomeratives vs. divisives Clustern \\\\n•Agglomerative (bottom-up) Methoden \\\\nbeginnen mit je einem Beispiel als eigenem Cluster und verbinden diese iterativ, um größere Cluster zu bilden. \\\\n•Divisive (partitionierende, top-down ) \\\\ntrennen die Menge aller Beispiele in eine gegebene Anzahl von Clustern und wiederholen dies für jeden Cluster solange bis jeder Cluster nur noch ein Beispiel enthält. \\', \\'9Hierarchisches Agglomeratives Clustern \\\\n(HAC) \\\\n•Gegeben ist eine Ähnlichkeitsfunktion zur \\\\nBestimmung der Ähnlichkeit von zwei Objekten. \\\\n•Beginnt mit allen Objekten in einem separatem Cluster und verbindet dann mehrmals die beiden Cluster, die am ähnlichsten sind, bis nur noch ein Cluster da ist. \\\\n•Die Historie des Zusammenlegens bildet einen binären Baum oder eine Hierarchie. \\', \\'10 Cluster-Ähnlichkeit \\\\n•Ähnlichkeitsfunktion, die die Ähnlichkeit von zwei Objekten bestimmt: sim (x,y). \\\\n–z.B. Kosinus-Ähnlichkeit von Dokumentvektoren. \\\\n•Wie berechnet man die Ähnlichkeit von zwei Clustern, von denen jeder möglicherweise eine Vielzahl von Objekten enthält?  \\\\n–Single Link : Ähnlichkeit der zwei ähnlichsten \\\\nMitglieder. \\\\n–Complete Link : Ähnlichkeit der zwei am wenigsten \\\\nähnlichen Mitglieder. \\\\n–Average Link : Durchschnittsähnlichkeit zwischen allen \\\\nMitgliedern. \\', \\'11 Single Link am Beispiel \\', \\'12 Rechnerische Komplexität\\\\n•Bei der ersten Iteration müssen alle HAC-Methoden die Ähnlichkeit aller Paare von n\\\\nObjekten berechnen. Aufwand: O( n\\\\n2)\\\\n•Vor jedem der nachfolgenden n−2 \\\\nZusammenlegungsschritte muss der Abstand zwischen dem neu erzeugten Cluster und allen anderen noch existierenden Clustern berechnet werden. \\\\n•Um bei einem Gesamtaufwand von O( n2) zu \\\\nbleiben, muss die Berechnung der Ähnlichkeit mit jedem Cluster in konstanter Zeit erfolgen. \\', \\'13 Nicht-Hierarchisches Clustern \\\\n•Typischerweise muss man bei den meisten Verfahren die Anzahl der gewünschten Cluster k \\\\nangeben. \\\\n•Wähle willkürlich kObjekte als Saat \\\\n(Ausgangspunkt) der Clusterung (einen pro Cluster).  \\\\n•Bilde anfängliche Cluster, die auf dieser Saat basieren. \\\\n•Iteriere mehrfach und ordne Objekte Clustern neu zu, mit dem Ziel das Gesamtclusterergebnis zu verbessern. \\\\n•Stoppe, wenn das Clustern konvergiert oder nach einer festen Anzahl von Iterationen. \\', \\'14 K-Means \\\\n•Gegeben: Objekte werden durch reellwertige Vektoren repräsentiert. \\\\n•Die Cluster werden durch ihre Schwerpunkte \\\\n(Zentroide) repräsentiert – dies ist der Mittelwert der Punkte eines Clusters c:\\\\n•Die Neuzuordnung von Objekten zu Clustern basiert auf dem Abstand zu den aktuellen Cluster-Zentroiden.∑\\\\n∈=\\\\ncxxcrr r\\\\n||1(c) µ\\', \\'15 Abstandsmaße\\\\n•Euklidischer Abstand (L 2-Norm): \\\\n•L1-Norm (Manhattan-Distanz): \\\\n•Kosinus-Ähnlichkeit ( /barb2rightwird zu einem \\\\nAbstandsmaß durch Subtraktion von 1): 2\\\\n12 ) ( ),(im\\\\niiyx yxL − =∑\\\\n=rr\\\\n∑\\\\n=− =m\\\\nii iyx yxL\\\\n11 ),(rr\\\\ny xyxrrrr\\\\n⋅−•1\\', \\'16 K-Means-Beispiel \\\\n(K=2) \\\\nWähle eine Saat \\\\nOrdne Cluster neu zu \\\\nBerechne Schwerpunkte \\\\nx\\\\nxOrdne Cluster neu zu \\\\nxxx x Berechne Schwerpunkte \\\\nOrdne Cluster neu zu \\\\nkonvergiert!\\', \\'17 Zeit-Komplexität\\\\n•Der Aufwand zur Berechnung des Abstandes zwischen zwei Objekten sei O( m), wobei mdie Dimensionalität der Vektoren ist. \\\\n•Neuzuordnung von Clustern: O( kn ) Abstandsberechnungen, oder O( knm ). \\\\n•Zentroidberechnung: Jeder Objektvektor wird einmal zu seinem Schwerpunkt addiert: O( nm ). \\\\n•Die letzten zwei Schritte werden jeweils einmal pro Iteration durchge führt:  \\\\nO( Iknm ), Iist die Anzahl von Iterationen. \\\\n•Linear in allen relevanten Faktoren, wobei von einer  festen Anzahl von Iterationen ausgegangen wird, \\\\n•KMeans ist effizienter als HAC (O( n2) ). \\', \\'18 Text-Clustering \\\\n•HAC und K-Means werden zum Clustern von Texten wie folgt angewendet: \\\\n•Typischerweise nutzt man einen normalisierten , TF-IDF-\\\\ngewichteten Vektor und die Kosinus-Ähnlichkeit. \\\\n•Die Berechung wird für dünn besetzte Vektoren optimiert. \\\\n•Anwendungen: \\\\n–Bei einer typischen Suchanfrage werden Dokumente des gleichen Clusters passend zu der ursprünglichen Antwortmenge zurückgeliefert, um so den Recall zu erhöhen. \\\\n–Clustern der Suchergebnisse, um dem Anwender besser organisierte Ergebnisse anbieten zu können. \\\\n–Die automatische Erstellung von taxonomischen Hierarchien für eine Menge von Dokumenten mit dem Ziel, das Browsing zu erleichtern. (z.B. Yahoo & DMOZ). \\']',\n",
       " '[\\'Identiﬁcation of Key Information with Topic Analysis\\\\non Large Unstructured Text Data\\\\nBACHELOR THESIS\\\\nDepartment of Electrical Engineering and Computer Science\\\\nUniversity of Kassel\\\\nAuthor Name: Klara Maximiliane Gutekunst\\\\nAddress: Gartenstraße 23\\\\n34125 Kassel\\\\nMatriculation number: 35677772\\\\nE-Mail: klara.gutekunst@student.uni-kassel.de\\\\nDepartment: Chair Intelligent Embedded Systems\\\\nExamining board 1: Prof. Dr. Bernhard Sick\\\\nExamining board 2: Prof. Dr. Gerd Stumme\\\\nSupervisor: Dr. Christian Gruhl\\\\nDate: 21. November 2023\\', \\'\\', \\'Abstract iii\\\\nAbstract\\\\nThe goal of this thesis is to investigate the applicability of computational means to the\\\\nexploration of large unstructured text corpora. Finding relevant documents and intercon-\\\\nnections between documents becomes signiﬁcantly more diﬃcult due to the sheer amount\\\\nof documents available. Institutes, such as the German tax oﬃces, have access to leak\\\\ndata, for instance, the Panama Papers or the Bahamas leak , containing huge amounts of\\\\ndocuments and valuable information yet to be extracted. However, these institutes, com-\\\\npanies and individuals do not have suﬃcient resources to explore individual documents\\\\nin order to ﬁnd a speciﬁc one or to identify inherent key topics. Hence, computational\\\\nmeans, such as text mining or topic analysis, may help to overcome this obstacle. This\\\\nthesis proposes an approach to ﬁnding relevant documents which share common topics\\\\nfrom a large unstructured text corpus. The approach bundles diﬀerent methods, such as\\\\ntextual embeddings, transformation of images and clustering techniques. As a result of this\\\\nwork, a web interface that enables the comparison of the methods examined via queries\\\\nfor similar documents to a database is provided.\\', \\'\\', \\'Zusammenfassung v\\\\nZusammenfassung\\\\nDas Auﬃnden relevanter Dokumente und von Zusammenhängen zwischen Dokumenten\\\\nwird durch die enorme Menge an verfügbaren Dokumenten erheblich erschwert. Institutio-\\\\nnen, wie z.B. deutsche Finanzämter, haben Zugang zu Datenleaks, wie etwa den Panama\\\\nPapern oder dem Bahamas-Leak , die große Mengen an Dokumenten und wertvollen In-\\\\nformationen enthalten, die es zu extrahieren gilt. Diese Institute, Unternehmen und\\\\nEinzelpersonen verfügen jedoch nicht über ausreichende Ressourcen, um einzelne Doku-\\\\nmente zu durchsuchen, ein bestimmtes Dokument zu ﬁnden oder inhärente Themen zu\\\\nidentiﬁzieren. Daher können computergestützte Verfahren wie Text Mining oder Topic\\\\nanalysis sie dabei unterstützen. In dieser Arbeit wird ein Ansatz vorgestellt, der in\\\\neinem großen unstrukturierten Textkorpus relevante Dokumente mit gemeinsamen The-\\\\nmen ﬁndet. Dieser Ansatz bündelt verschiedene Methoden, wie z.B. textuelle Embed-\\\\ndings, Transformation von Bildern und Clustering-Techniken. Als Ergebnis der in dieser\\\\nArbeit untersuchten Methoden wird eine Weboberﬂäche bereitgestellt, die Abfragen nach\\\\nähnlichen Dokumenten zum Vergleich der verschiedenen Methoden ermöglicht.\\', \\'\\', \\'Contents vii\\\\nContents\\\\n1 Introduction 1\\\\n1.1 Motivation ..................................... 1\\\\n1.2 Research Questions ................................ 1\\\\n1.3 Own approach .................................. 2\\\\n1.4 Structure of the Thesis .............................. 3\\\\n2 Related work 5\\\\n3 Fundamentals 9\\\\n3.1 Preprocessing ................................... 9\\\\n3.1.1 Tokenization ............................... 9\\\\n3.1.2 Stop-Word-Removal ........................... 10\\\\n3.1.3 Stemming ................................. 10\\\\n3.1.4 Lemmatization .............................. 10\\\\n3.2 Embeddings .................................... 11\\\\n3.2.1 Neural Networks ............................. 11\\\\n3.2.2 Term Frequency - Inverse Document Frequency ............ 12\\\\n3.2.3 Document to Vector ........................... 13\\\\n3.2.4 Universal Sentence Encoder ....................... 14\\\\n3.2.5 InferSent ................................. 15\\\\n3.2.6 Sentence-BERT .............................. 16\\\\n3.3 Similarity measurement ............................. 17\\\\n3.3.1 Euclidian distance ............................ 18\\\\n3.3.2 Cosine Similarity ............................. 18\\\\n3.4 Topic analysis ................................... 19\\\\n3.4.1 Topic to Vector .............................. 19\\\\n3.4.2 Word clouds ................................ 20\\\\n3.5 Compression of data ............................... 20\\\\n3.5.1 Autoencoder ............................... 20\\\\n3.5.2 Eigenfaces ................................. 21\\\\n3.6 Clustering ..................................... 24\\\\n3.6.1 DBSCAN ................................. 25\\\\n3.6.2 OPTICS .................................. 26\\\\n3.7 Software frameworks ............................... 28\\\\n3.7.1 Elasticsearch database .......................... 28\\\\n3.7.2 Flask .................................... 29\\', \\'Contents viii\\\\n3.7.3 Angular .................................. 30\\\\n4 Own approach 31\\\\n4.1 Oﬄine Processing ................................. 31\\\\n4.1.1 Database ................................. 31\\\\n4.1.2 Eigendocs ................................. 34\\\\n4.1.3 Embeddings ................................ 36\\\\n4.1.4 Clustering using OPTICS ........................ 42\\\\n4.1.5 Topic analysis ............................... 43\\\\n4.1.6 Slurm ................................... 45\\\\n4.2 Web interface ................................... 47\\\\n4.2.1 Backend .................................. 47\\\\n4.2.2 Frontend .................................. 47\\\\n4.3 Trade-oﬀ between memory and query time ................... 50\\\\n5 Evaluation 51\\\\n5.1 Database ..................................... 51\\\\n5.2 Eigendocs ..................................... 52\\\\n5.3 Embeddings .................................... 53\\\\n5.4 Clustering using OPTICS ............................ 58\\\\n5.5 Comparison of models .............................. 61\\\\n5.6 Comparison with baseline topic analysis approach ............... 67\\\\n6 Conclusion 69\\\\n6.1 Discussion ..................................... 69\\\\n6.2 Contribution ................................... 71\\\\n7 Outlook 73\\\\nBibliography xi\\', \\'List of abbreviations ix\\\\nList of abbreviations\\\\nACID Atomicity, Consistency, Isolation, Durability\\\\nAE Autoencoder\\\\nBERT Bidirectional Encoder Representations from Transformers\\\\nBiLSTM Bi-directional Long Short-Term Memory\\\\nBoW Bag of Words\\\\nCBOW Continuous-Bag-of-Words\\\\nCSS Cascading Style Sheet\\\\nCSV Comma Separated Values\\\\nDoc2Vec Document to Vector\\\\nDAN Deep Averaging Network\\\\nDBSCAN Density-Based Spatial Clustering of Applications with Noise\\\\nDNN Deep Neural Network\\\\nGB Gigabyte\\\\nGloVe Global Vectors\\\\nHDBSCAN Hierarchical DBSCAN\\\\nHNSW Hierarchical Navigable Small World\\\\nIDF Inverse Document Frequency\\\\nIES Intelligent Embedded Systems\\\\nIR Information Retrieval\\\\nJSON JavaScript Object Notation\\\\nKL Karhonen-Loéve\\\\nkNN k-Nearest Neighbour\\\\nLDA Latent Dirichlet Allocation\\\\nLSTM Long Short-Term Memory\\\\nML Machine Learning\\\\nNLP Natural Language Processing\\\\nNN Neural Network\\\\nNoSQL Not only SQL\\\\nOPTICS Ordering Points To Identify Clustering Structure\\\\nPCA Principal Component Analysis\\\\nPKL Pickle\\\\nPV-DBOW Distributed Bag of Words\\\\nPVDM Paragraph Vector Distributed Memory\\\\nRMSE Root Mean Square Error\\\\nRNN Recurrent Neural Network\\\\nRSME Root Mean Square Error\\', \\'List of abbreviations x\\\\nSBERT Sentence-BERT\\\\nSNLI Stanford Natural Language Inference\\\\nSQL Structured Query Language\\\\nSVD Singular Value Decomposition\\\\nSVM Support Vector Machine\\\\nTop2Vec Topic to Vector\\\\nTF-IDF Term Frequency - Inverse Document Frequency\\\\nTF Term Frequency\\\\nUI User Interface\\\\nUMAP Uniform Manifold Approximation and Projection\\\\nURL Uniform Resource Locator\\\\nUSE Universal Sentence Encoder\\\\nVSM Vector Space Model\\\\nWord2Vec Word to Vector\\', \\'1 Introduction 1\\\\n1I n t r o d u c t i o n\\\\nThis thesis addresses the task of analysing large unstructured text corpora. Their explo-\\\\nration is challenging due to the heterogeneous nature of the data, i.e. diﬀerent formats and\\\\nlayouts, and the amount of information. It is not possible to ﬁnd a group of semantically\\\\nsimilar documents by traversing corpora manually. Therefore, this thesis explores diﬀerent\\\\napproaches to support the exploration of large unstructured text corpora by computational\\\\nmeans with the goal of grouping documents according to their similarity.\\\\nThe dataset inspected in this thesis is the so-called Bahamas leak . It is a collection of\\\\nroughly 38 Gigabyte ( GB) of ﬁscal documents, which were leaked in 2016 [ 49]. The doc-\\\\numents are unstructured, i.e. they are of diﬀerent types, content and layout. They are\\\\nrelevant in the context of tax fraud since they contain information about oﬀshore compa-\\\\nnies and their owners. Tax oﬃces examine this dataset to identify tax evasion. However,\\\\nit has proven to be challenging to identify relevant documents and their interconnections\\\\ndue to the amount of documents contained in the leak.\\\\n1.1 Motivation\\\\nOn a broader scope, this thesis aims to provide computational means to facilitate the\\\\nwork with large unstructured text data for humans. The goal is to actively use Machine\\\\nLearning ( ML) techniques to analyze a large text corpus and to reduce the amount of\\\\nmanual human work.\\\\nIn the context of tax fraud, large unstructured text data, such as the Bahamas leak is\\\\nexamined by tax oﬃces. However, tax oﬃces do not have suﬃcient resources to examine\\\\nall documents to ﬁnd an object of interest, for instance, an invoice. Hence, MLtechniques\\\\nought to facilitate the work of investigators by reducing manual labor. These methods\\\\nshould propose visually, i.e. of the same document type, or semantically, i.e. from the same\\\\ncompany, similar documents to the investigator.\\\\n1.2 Research Questions\\\\nIn order to support the exploration of large unstructured text data, this thesis aims to\\\\nprovide computational means to facilitate the work with large corpora. In this work,\\\\ndiﬀerent methods to derive semantic and visual information from unstructured text data\\', \\'1 Introduction 2\\\\nare applied. These techniques ought to be compared and evaluated. In the following, the\\\\nresearch questions addressed are deﬁned:\\\\nRQ1. Is it possible to use a visual representation to ﬁnd similar documents in\\\\nthe corpus?\\\\nAssuming that it is valuable to explore documents of similar type, for instance,\\\\ninvoices, simultaneously, the system should be able to ﬁnd similar documents with\\\\nrespect to their visual appearance. It remains to be seen whether encodings of the\\\\nvisual appearance of a document are suﬃcient to ﬁnd similar documents.\\\\nRQ2. Do diﬀerent embedding methods produce similar results?\\\\nThe task at hand deﬁnes a result as a set of response documents similar to a query\\\\ndocument. Hence, one has to compare response sets of diﬀerent methods. The sim-\\\\nilarity between to response documents can be evaluated with respect to the content\\\\nor the visual appearance of the documents.\\\\nRQ3. How are the results of the system presented to experts?\\\\nThis question aims to ﬁnd a suitable way to present the results of the system to the\\\\nuser in an intuitive manner.\\\\nRQ4. How can the performance of the system be evaluated?\\\\nSince the dataset is not labeled, the performance of the system cannot be evaluated\\\\nwith respect to a ground truth. Hence, other means of evaluation have to be found.\\\\nThese techniques could include time measurements or qualitative analysis of the\\\\nquery responses.\\\\n1.3 Own approach\\\\nThis thesis proposes an approach to group documents based on their appearance or seman-\\\\ntic similarity, which is deﬁned via diﬀerent embedding strategies, i.e. methods to derive\\\\nembeddings from texts. Embeddings are numerical representations of words, sentences or\\\\ntexts. They enable the comparison of heterogeneous data via cosine similarity, i.e. the\\\\nangle between embedding vectors, whereas visual information is clustered using diﬀerent\\\\napproaches including OPTICS (cf.Subsection 3.6.2 ) beforehand. The resulting groups of\\\\ndocuments are visualized using word clouds.\\\\nThis work’s goals include the implementation of a User Interface ( UI) for the techniques\\\\nexamined. However, this UIis not supposed to be an operational application for end users\\\\nfrom the tax oﬃce but serves the purpose of displaying the techniques examined. It should\\\\nassist the natural human approach to exploration: A human ﬁnds a document of interest,\\\\nfor instance, by keyword search, and thus, wants to ﬁnd similar documents. The tool\\\\nshould support keyword search, a detailed inspection of a document of interest and the\\\\nexploration of similar documents.\\', \\'1 Introduction 3\\\\n1.4 Structure of the Thesis\\\\nThe thesis is structured as follows.\\\\nChapter 1\\\\nFirstly, the problem of working with large unstructured text corpora is introduced.\\\\nSecondly, the dataset used in this thesis is described. Moreover, the goal of this thesis,\\\\nas well as the target audience of the problem investigated is stated. Afterwards, the\\\\nmotivation and research questions are presented. The chapter concludes with an\\\\noutlook on the techniques used and an overview of the thesis.\\\\nChapter 2\\\\nThis chapter covers related work where similar approaches are presented. Moreover,\\\\nthe chapter introduces the literature that serves as a basis for this thesis.\\\\nChapter 3\\\\nThe theoretical foundations of the techniques applied in this thesis are outlined in\\\\nChapter 3 . The techniques can be divided into preprocessing (cf. Section 3.1 ),\\\\nsemantic embeddings (cf. Section 3.2 ), similarity measurements (cf. Section 3.3 ),\\\\ntopic analysis (cf. Section 3.4 ), compression of data (cf. Section 3.5 ), clustering\\\\nalgorithms (cf. Section 3.6 ) and software frameworks (cf. Section 3.7 ).\\\\nChapter 4\\\\nThis chapter describes the implementation of the methods. The implementation is\\\\nbased on the theoretical foundations presented in Chapter 3 .O n a m o r e g r a n u l a r\\\\nlevel, this chapter covers the oﬄine preprocessing (cf. Section 4.1 ), the implementa-\\\\ntion of the UI(cf.Section 4.2 ) and the trade-oﬀ between memory and query time in\\\\nSection 4.3 .\\\\nChapter 5\\\\nThe evaluation of the methods is presented in this chapter. It gives a reason why\\\\ncertain parameter choices were made with respect to established parameter estima-\\\\ntion approaches. Moreover, it compares the diﬀerent methods with regard to their\\\\nquery responses and the bundle of methods constructed in the course of this thesis\\\\nto an existing baseline topic analysis approach.\\\\nChapter 6\\\\nThis chapter concludes this thesis. The insights acquired by exploring diﬀerent tech-\\\\nniques with the goal of the exploration of large unstructured text data are presented\\\\nand the research questions are revised in Section 6.1 .I n Section 6.2 the scientiﬁc\\\\ncontributions are highlighted.\\\\nChapter 7\\\\nThe last chapter gives an outlook on future work. It also includes a discussion of the\\\\nlimitations of this thesis.\\', \\'\\', \\'2 Related work 5\\\\n2R e l a t e d w o r k\\\\nThis chapter examines and summarises diﬀerent literature about topic analysis of text\\\\ncorpora and related ﬁelds. It presents a selection of textual embedding methods, visual\\\\ninformation encodings, dimensionality reduction methods, similar data corpora, similarity\\\\nmeasures and clustering methods. Moreover, a few topic analysis libraries are presented.\\\\nThe domain of Information Retrieval ( IR) works on large datasets. Hence, multiple sci-\\\\nentiﬁc papers working with large corpora, such as [ 44], have been published. Research on\\\\n(large) ﬁscal datasets includes MLtasks such as anomaly detection to identify credit card\\\\nfraud [ 75,48,47,39,60,27].\\\\nMLtechniques usually require numerical data as input. In order to utilize MLtechniques,\\\\ntextual data is often represented as real-valued vectors. Depending on the approach, vectors\\\\neither represent single words, sentences or whole documents. The models used in this\\\\nwork are brieﬂy introduced in the following. More detailed information can be found in\\\\nSection 3.2 .\\\\nThe TF-IDF model is a widely used model for text representation. Even though Zhang\\\\net al. discuss TF-IDF ’s drawbacks [ 77] the model is incorporated in this work due to its\\\\nsimplicity.\\\\nMikolov et al. discuss the well-established Word2Vec models CBOW and Skip-gram. The\\\\nauthors found that these Word2Vec models produce high-quality word embeddings on large\\\\ndatasets [ 44]. These Word2Vec models form the basis of so-called Doc2Vec models which\\\\nembed whole documents. The PVDM model extends the CBOW model to work on a set\\\\nof documents or paragraphs instead of words [ 78] and is used in this work.\\\\nA more complex model is the SBERT model [ 58]. This model is an extension of the BERT\\\\nmodel which set state-of-the-art results in many Natural Language Processing ( NLP)t a s k s .\\\\nReimers and Gurevych show that BERT is not suitable for certain similarity measures, such\\\\nas cosine similarity. Moreover, they argue that SBERT overcomes BERT ’s shortcomings.\\\\nSince the SBERT model is able to produce document embeddings, it is used in this work.\\\\nThe InferSent model is a sentence embedding model [ 14].Conneau et al. state that it\\\\noutperforms models trained in an unsupervised fashion. They train it on a labeled dataset\\\\nand optimize the model’s architecture. Since the model is pretrained and open-source, it\\\\nis used in this work.\\\\nAnother embedding model of interest is the USE model [ 9].Cer et al. propose two model\\\\narchitectures which respectively are either superior with regard to accuracy or resource\\', \\'2 Related work 6\\\\nconsumption. They claim that their model surpasses word-level embedding transfer learn-\\\\ning on several NLP tasks. Due to the fact that the pretrained models are open-source, the\\\\none that consumes fewer resources is used in this work.\\\\nThis thesis aims to encode visual information as low-dimensional real-valued vectors. Since\\\\nthe domain of face recognition deals with the task of deriving meaningful information from\\\\nhigh dimensional data, the Eigenfaces approach is adapted to document images in this\\\\nwork. The task of ﬁnding similar images of faces is transferred to ﬁnding similar document\\\\nimages. Eigenfaces projects face images into a lower-dimensional feature space which best\\\\nencodes the variation among the faces [ 67]. Since 1991 this technique has been covered in\\\\na lot of papers [ 67,76,71,65,16,64].\\\\nAnowar et al. propose a survey on diﬀerent dimensionality reduction techniques including\\\\nPCA ,LDA andSVD [6]. They conceptually categorize and compare the techniques. The\\\\nauthors conduct experiments on diﬀerent datasets to compare the techniques’ performance\\\\non classiﬁcation tasks. They ﬁnd that the classiﬁcation accuracy obtained on the reduced\\\\nversion of the datasets is superior to the accuracy achieved on the original datasets. Their\\\\nwork serves as a theoretical foundation for Section 3.5 .\\\\nAnother dimensionality reduction technique is an AE[46,40]. The papers provide a\\\\ntheoretical foundation for Subsection 3.5.1 .A n AElearns a meaningful low-dimensional\\\\nrepresentation of the input. This representation is used as a compressed version of certain\\\\nembeddings in this work.\\\\nTo determine the similarity between two objects, one has to deﬁne a metric. Prevalent\\\\nmetrics in the domain of comparing objects in a Vector Space Model ( VSM ) include (soft)\\\\ncosine similarity outlined by Sidorov et al. , as well as by Charlet and Damnati [62,11],\\\\nthe Manhattan and the Euclidean norm [ 37].Sidorov et al. propose the calculation of the\\\\nsoft similarity. Charlet and Damnati state that the soft cosine similarity is superior to the\\\\ncosine similarity since it takes into account the relations between words. When comparing\\\\ndiﬀerent norms in the context of IRfrom images Khosla et al. ﬁnd that the Manhattan\\\\nnorm has a better precision than the Euclidean norm. The similarity metric used in this\\\\nwork is the cosine similarity.\\\\nThe visual information of the document images shall be used to cluster them. Ankerst\\\\net al. introduce the clustering algorithm OPTICS which seems to be suitable for the task\\\\nat hand since it does not return an explicit clustering but a clustering structure [ 5]. More-\\\\nover, Ankerst et al. state that OPTICS is a method for database mining. Other researchers,\\\\nfor instance, Kanagala and Krishnaiah , compare related clustering algorithms including\\\\nK-Means, DBSCAN andOPTICS . They state that OPTICS overcomes DBSCAN ’s dif-\\\\nﬁculties and K-Means limitations [ 35].Patwary et al. ,Deng et al. and Agrawal et al.\\\\npropose OPTICS extensions for spatially and temporally evolving data or a parallel ver-\\\\nsion [ 54,17,2].\\', \\'2 Related work 7\\\\nThe methods explored in this thesis ought to be bundled into a tool. Some researchers have\\\\nalready developed complete topic analysis libraries whose functionalities can be compared\\\\nto the tool developed in this thesis. They merge a selection of the techniques stated above\\\\ninto a well-reasoned composite. BERTopic is a library that merges SBERT embeddings\\\\nwith UMAP dimension reduction, HDBSCAN clustering and the application of TF-IDF\\\\non the clusters [ 26].\\\\nOther well-established topic analysis approaches consist of LDA .Wang and Qian propose a\\\\ntechnique that ﬁrst applies LDA to reduce the data’s dimensionality and thereafter classiﬁes\\\\nthe result with a Support Vector Machine ( SVM )[72]. Similarly, Chen et al. use a kNN\\\\nalgorithm instead of a SVM on the textual subspace generated by LDA [12]. Another\\\\ntechnique proposed is LDA2VEC, which is subject to Churchill and Singh ’s work [ 13].\\\\nChaney and Blei ’s paper introduces an open-source library for topic model visualization,\\\\nexemplary showcased on a Wikipedia dataset [ 10].\\\\nAngelov andNiu and Dai claim that the Top2Vec model not only overcomes LDA ’s short-\\\\ncomings [ 4,52] but is also developed for topic analysis on a large collection of documents\\\\n[4]. The Top2Vec library serves as a baseline model for the application developed in this\\\\nwork.\\\\nIn contrast to the assumption of this thesis that the prevalent topics are static, in reality,\\\\ntopics may be dynamic or change over time. Not only Alghamdi and Alfalqi ,b u ta l s o\\\\nVayansky and Kumar have published surveys on topic analysis techniques, which take into\\\\naccount factors such as time [ 3,69].\\\\nSome of the techniques that were brieﬂy introduced in this chapter partake in the appli-\\\\ncation developed in this thesis. They are described in more detail in Chapter 3 .\\', \\'\\', \\'3 Fundamentals 9\\\\n3F u n d a m e n t a l s\\\\nThe following chapter outlines the theoretical principles of the methods used in this work.\\\\nFirst, the preprocessing of the data is described in Section 3.1 . Then, a variety of ways\\\\nto generate numerical representations of textual data is outlined in Section 3.2 . After-\\\\nwards, the diﬀerent similarity measurements are introduced in Section 3.3 . A selection of\\\\nconventional topic analysis approaches is outlined in Section 3.4 . Subsequently, two data\\\\ncompression techniques are presented in Section 3.5 . Then, Section 3.6 presents multiple\\\\nclustering methods. Finally, the libraries used to implement the web application and the\\\\ndatabase are introduced in Section 3.7 .\\\\n3.1 Preprocessing\\\\nSimilar to other MLdomains, NLP requires preprocessing of the data. Usually, textual data\\\\ncontains irrelevant information and noise. Examples of noise include so-called stop words,\\\\nsuch as “the” or “and”. However, irrelevant information can be task-speciﬁc. In some cases,\\\\nnumerical data may be regarded to be irrelevant and should be omitted. Preprocessing\\\\nimproves the performance and the results [ 56]. The next sections describe the non-trivial\\\\npreprocessing steps applied in this work.\\\\n3.1.1 Tokenization\\\\nFigure 3.1 :Tokenization visualized using an example text.\\\\nTokenization is the process of splitting a text into smaller pieces, so-called tokens . Tokens\\\\ncan be words and punctuation marks [ 8]. The deﬁnition of a token depends on the applica-\\\\ntion. For instance, certain tokenization implementations may identify tokens as subsequent\\\\nseries of non-whitespace characters omitting all numbers and punctuation marks [ 63].\\', \\'3 Fundamentals 10\\\\n3.1.2 Stop-Word-Removal\\\\nFigure 3.2 :Stop-Word-Removal visualized using an example text.\\\\nOmitting words that are not relevant to the context of the text is called stop-word-removal .\\\\nStop words not only depend on the domain but also the language [ 63].\\\\n3.1.3 Stemming\\\\nIn order to avoid language inﬂections, i.e. treating words with similar meanings diﬀerently,\\\\nstemming is applied [ 56]. According to Bird et al. ,stemming is the process of stripping oﬀ\\\\nany aﬃxes, i.e. preﬁxes and suﬃxes [ 63], from a word and returning the stem. Diﬀerent\\\\ntypes of stemmers are better suited for certain applications than others. Hence, the choice\\\\nof the stemmer depends on the application.\\\\n3.1.4 Lemmatization\\\\nFigure 3.3 :Lemmatization visualized using an example text.\\\\nStemming and lemmatization are used to reduce the vocabulary size [ 56]. Opposed to\\\\nstemming, lemmatization returns only stems that are considered valid words [ 8]. Some\\\\nimplementations of lemmatizers validate stems with regard to a set of valid words, i.e.\\\\nlemma s, stored in a dictionary. Lemmatizers are usually slower than stemmers [ 8].\\', \\'3 Fundamentals 11\\\\nThe WordNetLemmatizer from the nltk package1requires a vocabulary. According to\\\\nRadu et al. , it is frequently used for lemmatization of English texts [ 56].\\\\n3.2 Embeddings\\\\nUsually, MLtechniques require textual inputs to be converted to embeddings [ 43]. Em-\\\\nbeddings are numerical representations of words, sentences or texts. They can be used to\\\\npresent the textual data as real-valued vectors in a VSM . A simple example of a VSM in\\\\ntheNLP context is shown in Figure 3.4 .AVSM is aN-dimensional space [ 62].VSM sa r e\\\\ncommonly used due to their conceptual simplicity and because spatial proximity correlates\\\\nwith semantic proximity [ 77,9,58,4]. Representations in a VSM can improve the perfor-\\\\nmance in NLP tasks [ 45]. The following section outlines the fundamentals of a selection of\\\\nembeddings.\\\\nFigure 3.4 :As i m p l e VSM . The words are represented as vectors in a two-dimensional\\\\nspace. Since wine is semantically more similar to drink than to food,t h e\\\\nvectors are closer together.\\\\n3.2.1 Neural Networks\\\\nA Neural Network ( NN)i sa MLmodel which consists of multiple layers of nodes. A node,\\\\nor so-called neuron, takes an input vector and produces an output vector. The output is\\\\nderived from the calculation of a weighted sum of the inputs and an activation function\\\\n[32]. The architecture of a NNis shown in Figure 3.5 . The ﬁrst and last layers are called\\\\ninput and output layers, respectively. The layers between the input and output layers are\\\\ncalled hidden layers. If a NNhas more than one hidden layer, it is called a Deep Neural\\\\nNetwork ( DNN ) and working with DNN s is considered deep learning. To propagate the\\\\ninput through the network the layers are connected. In a feed-forward NN, the information\\\\nﬂows from the input layer to the output layer [ 31].\\\\nNNs are trained using the backpropagation algorithm which reduces the error between the\\\\npredicted and the actual output iteratively. While data is propagated in a forward direction\\\\n1https://www.nltk.org/_modules/nltk/stem/wordnet.html (last accessed: 12/11/2023)\\', \\'3 Fundamentals 12\\\\nFigure 3.5 :Architecture of a NN. The input layer is the ﬁrst layer of the network. It\\\\nreceives the input data x. The output layer is the last layer of the network and\\\\nreturns y. Between the input and output layers, there are one or more hidden\\\\nlayers.\\\\nthrough the network, the error is propagated in a backward direction. The weights of the\\\\nlayers are adjusted according to the error [ 32].\\\\n3.2.2 Term Frequency - Inverse Document Frequency\\\\nTerm Frequency - Inverse Document Frequency ( TF-IDF ) provides a numerical representa-\\\\ntion of a word in a document. Let a corpus of documents be denoted D={d1,d2,. . . ,d M},\\\\nMbeing the total number of documents in the corpus. Let a sequence of terms wj2Vbe\\\\ndenoted a document di={w1,w2,. . .},Vbeing the vocabulary, i.e. set of distinct words\\\\n[56].\\\\nThe TF-IDF model considers the frequency fwj,dof a word wjin a document dand the\\\\nfrequency of a word in the whole corpus. The frequency fwj,dis deﬁned in Equation 3.1 ,\\\\nw0\\\\njbeing the number of occurrences of wjind.\\\\nfwj,d=w0\\\\njP\\\\nk2dw0\\\\nk(3.1)\\\\nTFIDF (wj,d ,D )=TF(wj,d)·IDF(wj,D) (3.2)\\\\nTF(wj,d)=fwj,d (3.3)\\\\nIDF(wj,D) = log2M\\\\nMj(3.4)\\', \\'3 Fundamentals 13\\\\nTF-IDF is calculated using Equation 3.2 from [ 56]. Each entry of a TF-IDF embedding\\\\nvector represents the TF-IDF value of a word in a document. Hence, the embedding\\\\nvector is of the same length as the vocabulary of the corpus. The Term Frequency ( TF)\\\\nis determined utilizing Equation 3.3 , whereas the Inverse Document Frequency ( IDF)i s\\\\ncomputed by Equation 3.4 ,Mjbeing the number of documents the term wjappears in.\\\\nIDF measures the importance of a term wjin the corpus of documents Dunder the\\\\nassumption that a term’s importance to the data corpus is inversely proportional to its\\\\noccurrence frequency [ 77]. In other words: Terms which appear in many documents are\\\\nnot as important and thus, weighted less than document-speciﬁc terms. The calculation\\\\nofTFandIDF is visualized exemplary in Figure 3.6 .\\\\nFigure 3.6 :Exemplary calculation of TFand IDF for a document corpus D:TFonly\\\\nconsiders the documents of interest while IDF incorporates the importance of\\\\nthe word with respect to D.\\\\nTF-IDF has several drawbacks [ 56,77]:\\\\n•TF-IDF does not consider semantic similarities between words.\\\\n•TF-IDF does not take into account the order of words in a document.\\\\n•TF-IDF often produces high dimensional representations which have to be postpro-\\\\ncessed to reduce their dimensionality, e.g., by using Principal Component Analy-\\\\nsis (PCA ).\\\\n3.2.3 Document to Vector\\\\nAnother term used for Document to Vector ( Doc2Vec )i s Paragraph Vector [56,43].\\\\nDoc2Vec addresses TF-IDF ’s drawbacks by encoding texts as N\\\\x00dimensional vectors learnt\\\\nusing the words’ context [ 56].N2Ncan be chosen arbitrarily. It preserves semantic sim-\\\\nilarities between words and encodes linguistic regularities and patterns [ 45]. The model\\\\nhandles inputs of diﬀerent lengths, i.e. inputs can be sentences, paragraphs or documents.\\\\nDoc2Vec is an adaption of the Word to Vector ( Word2Vec ) model, which maps words\\\\ninto a VSM [56]. Both approaches assume that words appearing in similar contexts are\\', \\'3 Fundamentals 14\\\\n(a)CBOW architecture cf. [ 44].\\\\n(b)PVDM architecture cf. [ 43].\\\\nFigure 3.7 :Both approaches predict the centre word w(t) using the context. PVDM is\\\\nan adaption of CBOW to work on a set of documents or paragraphs instead of\\\\nwords.\\\\nsemantically similar. Hence, words which often appear in the same context produce similar\\\\nembeddings.\\\\nTheDoc2Vec embedding is obtained using a shallow NN, i.e. the NNhas only one hidden\\\\nlayer. The embeddings are created by the hidden layer. There are two Doc2Vec approaches\\\\nto designing the architecture of the NN:\\\\n•Paragraph Vector Distributed Memory ( PVDM ):\\\\nPredicts a word given a context [ 43,44].\\\\n•Distributed Bag of Words ( PV-DBOW ):\\\\nPredicts the context given a word [ 38,45,43].\\\\nThe PVDM algorithm considers words within a sliding window and their document the\\\\ncontext of a centre word [ 43]. The document vector is added to incorporate the docu-\\\\nment’s topic and thus, acts like a memory [ 43,4].PVDM encodes the context words into\\\\nvectors via the Word2Vec Continuous-Bag-of-Words ( CBOW ) model [ 55]. Each document\\\\nis mapped to a vector using an additional document-to-vector matrix. The vectors can be\\\\nconcatenated, averaged or summed up [ 43]. The resulting vector is the prediction of the\\\\ncentral word. The CBOW and the PVDM approach are displayed in Figure 3.7 .\\\\n3.2.4 Universal Sentence Encoder\\\\nCer et al. have published their Universal Sentence Encoder ( USE) model on TensorFlow\\\\nHub. They propose two architectures, one based on a Transformer and one based on a\\\\nDeep Averaging Network ( DAN )[9]. Both models’ input is a lowercase tokenized string.\\\\nTheir output is a 512-dimensional vector.\\\\nThe transformer model is more accurate and more complex than the DAN model [ 9].\\\\nThe transformer’s (self) attention is used to compute context-aware word embeddings,\\\\nwhich consider both the word order and their semantic identity. Since a sequence of\\', \\'3 Fundamentals 15\\\\nword embeddings of a sentence produces embeddings of diﬀerent dimensions, the approach\\\\npostprocesses the word embeddings. A sentence vector is obtained by computing the\\\\nelement-wise sum of the word embeddings and normalizing the result by dividing by the\\\\nsquare root of the sentence length.\\\\nThe DAN model receives real-valued embeddings of words and bi-grams as input. A\\\\nbi-gram is a tupel of two subsequent words in a text [ 8], for instance, (red, wine), (wine,\\\\ntastes), (tastes, good) . The embeddings can be obtained from the text strings using models\\\\nsuch as the Bag of Words ( BoW ) model [ 21]. They are averaged and subsequently passed\\\\nto a feedforward DNN [9]. The architecture of the DAN model is depicted in Figure 3.8 .\\\\nFigure 3.8 :Architecture of the DAN model used for USE based on the textual description\\\\nfrom [ 14]. The input words and bi-grams (w1,w2,. . . ,w N)are embedded. The\\\\nembeddings are averaged and subsequently passed to a feedforward DNN , which\\\\nproduces a 512-dimensional sentence embedding.\\\\nThe models are trained on both unsupervised training data, e.g., Wikipedia, and a su-\\\\npervised training dataset, i.e. Stanford Natural Language Inference ( SNLI )[9,58]. The\\\\nunsupervised training task is to predict the context given an input, i.e. Skip-Gram like\\\\ntasks. The supervised training task is classiﬁcation [ 9].\\\\n3.2.5 InferSent\\\\nInferSent is a sentence embedding method trained in a supervised manner on the SNLI\\\\ndataset [ 14,58]. The trained model is transferable to other tasks. Conneau et al. com-\\\\npare multiple architectures in their work. The Bi-directional Long Short-Term Mem-\\\\nory ( BiLSTM ) architecture with max pooling which was found to be the best option for\\\\nthe sentence encoder is depicted in Figure 3.9 [14].\\\\nA Long Short-Term Memory ( LSTM ) is a Recurrent Neural Network ( RNN ) that is capable\\\\nof learning long-term dependencies. RNN s have closed loops, i.e. feedback connections\\', \\'3 Fundamentals 16\\\\nbetween the nodes [ 59]. In other words, a LSTM is able to remember information as a\\\\nso-called state. Certain LSTM mechanisms control whether the current state is deleted,\\\\nwhether new data is saved and to what degree the current state contributes to the current\\\\ninput processed in the node. Hence, LSTM nodes are not only inﬂuenced by former outputs\\\\nbut also by their state. Since the LSTM computes diﬀerent numbers of hidden vectors ht\\\\ndepending on the length of a sentence, a max pooling layer is applied to the hidden vectors\\\\nwhich selects the maximum value for a patch of the hidden vectors.\\\\nFigure 3.9 :Architecture of the BiLSTM model with max pooling used for InferSent cf. [ 14].\\\\nThe input sentence (w1,w2,. . . ,w T)is read from both directions by a forward\\\\nand a backward LSTM producing\\\\x00!htand \\\\x00htrespectively. After concatenating\\\\x00!htand \\\\x00httoht, max pooling is applied. The output is a ﬁxed-sized embedding.\\\\nAccording to Reimers and Gurevych , InferSent consists of a single BiLSTM layer [ 58].\\\\nGiven a sentence (w1,w2,. . . ,w T)ofTwords, the BiLSTM architecture computes the hidden\\\\nrepresentations htfor each word wt. The hidden representation htis the concatenation of\\\\nthe forward and backward hidden vectors\\\\x00!htand \\\\x00ht.\\\\x00!htand \\\\x00htare produced by a forward\\\\nand backward LSTM respectively. Hence, the sentence is read from both directions and\\\\nthus, considers past and future context.\\\\n3.2.6 Sentence-BERT\\\\nSentence-BERT ( SBERT ) is an enhancement of Bidirectional Encoder Representations\\\\nfrom Transformers ( BERT ). The applicability of BERT is limited because it does not pro-\\\\nduce independent embeddings for single sentences [ 58]. Moreover, Reimers and Gurevych\\', \\'3 Fundamentals 17\\\\nfound that common similarity measurements, for instance, the ones discussed in Section 3.3 ,\\\\ndo not perform well on sentence embeddings produced by BERT .\\\\nBERT is a pre-trained transformer network which predicts a target value based on two input\\\\nsentences for sentence classiﬁcation or sentence-pair regression tasks [ 58]. The BERT base\\\\nmodel applies multi-head attention over 12 transformer layers, whereas the large model\\\\napplies multi-head attention over 24 transformer layers. The attention mechanism enables\\\\naccess to all hidden states as opposed to only the last hidden state [ 34]. It derives its output\\\\nvector as a dynamic weighted sum of the hidden states. The ﬁnal label is derived from a\\\\nregression function, which receives the output of the 12thor24thlayer, respectively.\\\\nFigure 3.10 :Architecture of SBERT cf. [58].BERT is extended by a pooling layer. The\\\\ninput is a sentence and the output is a ﬁxed-sized embedding.\\\\nSBERT provides ﬁxed-sized embeddings for single sentences [ 58]. It diﬀers from BERT in\\\\nterms of architecture, since it adds a pooling layer after the BERT model. Reimers and\\\\nGurevych compare diﬀerent pooling strategies, such as using the output of the CLS (i.e.\\\\nﬁrst) token, mean pooling and max pooling. The architecture of a single SBERT network\\\\nis depicted in Figure 3.10 . In order to work with multiple input sentences at the same time,\\\\nsiamese and triplet network architectures, i.e. multiple BERT networks with tied weights,\\\\nare constructed. To perform classiﬁcation or inference tasks, layers are added on top of\\\\ntheSBERT network. SBERT is trained on the SNLI dataset [ 58,28].\\\\nAccording to Reimers and Gurevych ,SBERT outperforms InferSent and USE on Seman-\\\\ntic Textual Similarity tasks and on SentEval, which is an evaluation toolkit for sentence\\\\nembeddings [ 58].\\\\n3.3 Similarity measurement\\\\nEmbeddings not only facilitate human interpretability of relationships between texts, but\\\\nthey also enable the use of metrics, i.e. similarity measures, to quantify the similarity\\\\nbetween texts [ 63,37].\\\\nThere are several similarity measures, such as the dot product quantifying the number of\\\\nshared tokens of two texts, the (soft) cosine similarity [ 62,11], which is the normalized dot\\\\nproduct and calculates the angle between two vectors, and many more [ 63,37,58]. The\\\\nfollowing section outlines a selection of similarity measures.\\', \\'3 Fundamentals 18\\\\n3.3.1 Euclidian distance\\\\nThe euclidian distance is a distance measure. In order to measure the distance between\\\\ntwo vectors in a N-dimensional space, the root of the sum of squared distances between the\\\\nrespective values of every dimension is calculated. The Euclidean (L2) norm between two\\\\nvectors a, bis deﬁned in Equation 3.5 [37]. The distance is zero if the vectors are identical,\\\\ni.e.a=b. The more aandbdiﬀer, the greater is the distance dE(a, b)between them.\\\\ndE(a, b)=vuutNX\\\\ni=1(ai\\\\x00bi)2 (3.5)\\\\n3.3.2 Cosine Similarity\\\\n(a)Similar vectors a, b.\\\\n(b)Dissimilar vectors a, b.\\\\nFigure 3.11 :Cosine Similarity between two vectors considers the angle between them.\\\\nThe similarity between two texts is measured by the cosine of the angle between their\\\\nrespective real-valued vectors. The cosine similarity is deﬁned in Equation 3.6 [62]. For\\\\npositive vectors, for instance, produced by TF-IDF , it is a value between 0and1.I f t h e\\\\nangle is close to zero degrees, the cosine similarity is close to 1and the vectors are similar.\\\\nIf the angle is close to 90degrees, the cosine similarity is close to 0and the vectors are\\\\ndissimilar. Both a similar and a dissimilar pair of vectors are depicted in Figure 3.11 .\\\\ncosine (a, b)=a·b\\\\nkak⇥kbk=PN\\\\ni=1aibiqPN\\\\ni=1a2\\\\niqPN\\\\ni=1b2\\\\ni(3.6)\\\\nThe formula from Equation 3.6 assumes that the vectors, which span the VSM are or-\\\\nthogonal and thus, completely independent. However, in practical applications, the index\\\\nterms which span the VSM are often semantically dependent.\\', \\'3 Fundamentals 19\\\\n3.4 Topic analysis\\\\nSince more and more textual data emerges, methods to analyze and extract information\\\\nfrom texts become more important. One of these methods is topic analysis. A topic can\\\\nbe deﬁned as a cluster of words that occur frequently or are semantically similar to each\\\\nother. A document can be represented by one or more topics [ 3].\\\\n3.4.1 Topic to Vector\\\\nThe approach Topic to Vector ( Top2Vec ) addresses several problems of state-of-the-art\\\\ntopic analysis approaches, such as Latent Dirichlet Allocation ( LDA )[4].Top2Vec does\\\\nnot require the user to specify the number of topics k, i.e. it does not discretize the topic\\\\nspace into ktopics, and it does not require stop word removal or lemmatization. It considers\\\\nthe semantic meaning of words. Top2Vec only associates one topic with a document.\\\\n(a)Skip-gram architecture cf. [ 52].\\\\n(b)CBOW architecture cf. [ 52].\\\\nFigure 3.12 :Both learning architectures of Top2Vec .w(t\\\\x002),w(t\\\\x001),w(t+ 1) ,w(t+ 2)\\\\nare the context words of the centre word w(t)of topic z(t).\\\\nTop2Vec is based on Word2Vec andDoc2Vec . The documents are embedded using the\\\\nDoc2Vec model PV-DBOW . The two learning architectures CBOW and Skip-gram from\\\\nWord2Vec are adapted to train the model as depicted in Figure 3.12 [52]. The Skip-Gram\\\\nlearning task is to predict the context a word came from [ 4,52].Top2Vec embeds words,\\\\ndocuments and topics in the same feature space. The similarity between embeddings can\\\\nbe measured using the cosine similarity function [ 52].\\\\nAngelov regards each point in the VSM as a topic, described by its nearest words. The\\\\nauthor states that topics are continuous and can be described by diﬀerent sets of words [ 4].\\\\nHence, topic analysis can be deﬁned as the task of ﬁnding sets of informative words that\\\\ndescribe the topic of a document. Documents in dense areas of the topic space are con-\\\\nsidered to be about the same topic. The density-based clustering algorithm Hierarchical\\\\nDBSCAN ( HDBSCAN ) is used to ﬁnd these dense areas. Since HDBSCAN has diﬃcul-\\\\nties ﬁnding dense clusters in high-dimensional data, the dimensionality reduction method\\\\nUniform Manifold Approximation and Projection ( UMAP ) is applied [ 4]. The steps of the\\\\ntopic analysis procedure Top2Vec are depicted in Figure 3.13 .\\', \\'3 Fundamentals 20\\\\nFigure 3.13 :Procedure of topic analysis using Top2Vec .\\\\nA topic vector is denoted as the centroid or average of the document vectors that belong\\\\nto a certain topic. The number of topics is derived from the number of dense areas. It\\\\nis possible to merge topics to hierarchically reduce the number of topics to any number\\\\nsmaller than the number of topics initially found.\\\\n3.4.2 Word clouds\\\\nA word cloud is a technique to visualize the most predominant words in a text [ 36]. The\\\\nsize of a word correlates to its frequency or importance in the text. However, a word does\\\\nnot have to be meaningful to appear large. A word cloud does not provide information\\\\nabout the meaning or context of words and thus, one has to be careful when interpreting\\\\nthe results.\\\\n3.5 Compression of data\\\\nAccording to Radu et al. , a decomposition of data preserves the inner structure in inherent\\\\nclusters. When data analysis techniques are applied to reasonably low-dimensional data,\\\\nthe results usually improve. Moreover, compressed data is less memory-consuming and\\\\noften less diﬃcult to interpret by humans since there are more methods to visualize low-\\\\ndimensional data. In the following, two approaches to reduce the dimensionality of data\\\\nare presented.\\\\n3.5.1 Autoencoder\\\\nThe idea of this approach is to ﬁnd a meaningful low-dimensional version of the input.\\\\nThe high-dimensional data is encoded into a low-dimensional representation using the\\\\nencoder of an undercomplete Autoencoder ( AE)[46]. Hence, the output of the latent\\\\nspace corresponds to the input’s embedding. The low-dimensional representation can be\\\\ndecoded into an approximation of the high-dimensional original using the decoder of the\\\\nAE.\\', \\'3 Fundamentals 21\\\\nFigure 3.14 :Structure of an AEcf. [46]. The six-dimensional input is encoded into a three-\\\\ndimensional representation. This encoding is decoded into a six-dimensional\\\\napproximation of the original input.\\\\nAn undercomplete AEis a feed-forward NN, which consists of an encoder and a decoder.\\\\nNNs are discussed in Subsection 3.2.1 . It learns eﬃcient (non-correlated) encodings of the\\\\ninput data [ 46]. It is undercomplete because the dimensionality of the hidden layer, or\\\\nso-called hidden space, is lower than the dimensionality of the input layer [ 31]. The input\\\\nand output layers have the same dimensionality.\\\\nThe network employs backpropagation to update the parameters of the network during\\\\ntraining. The AE’s goal is to approximate the identity function f✓(X)=X(trivial solution\\\\neliminated) for input Xand function parameters to be learned ✓[31].\\\\n3.5.2 Eigenfaces\\\\nAccording to Turk and Pentland , the idea of Eigenfaces is inspired by information theory.\\\\nOpposed to former approaches in the domain of face recognition which relied on the clas-\\\\nsiﬁcation of images based on a set of predeﬁned facial features, such as distance between\\\\neyes, Eigenfaces does not use predeﬁned features [ 67]. The goal of this approach is to rep-\\\\nresent images using a smaller set of image features, i.e. compression to a lower-dimensional\\\\nfeature space, such that it is possible to distinguish between the images [ 67,71]. These\\\\nfeatures do not necessarily correspond to human facial features [ 67]. Similar pictures, i.e.\\\\nof the same person, should lie on a manifold in the lower-dimensional feature space [ 65].\\\\nThe decomposition of input images not only reduces the complexity but also facilitates\\\\nmodeling probability density of a face image [ 65].\\\\nThe greyscale input images are two-dimensional arrays of numbers: x={xi|i2S},S\\\\nbeing a square lattice [ 76,67]. The images are reshaped to an one-dimensional array\\\\nx=[x1,x2,. . . ,x n]T2Rn, where n=kSkandRnis the n-dimensional euclidean space\\\\n[76]. Some authors remove the background to omit values outside the face area [ 67].\\', \"3 Fundamentals 22\\\\nTurk and Pentland stress that the data should be normalized, i.e. centered, as computed\\\\ninEquation 3.7 .\\\\x00kis the diﬀerence of the k-th training image and the average image\\\\ncalculated using Equation 3.8 ,Nbeing the number of training images.\\\\n\\\\x00k=xk\\\\x00 (3.7)\\\\n =1\\\\nNNX\\\\nk=1xk (3.8)\\\\nx=nX\\\\ni=1ˆxiei (3.9)\\\\nThe next step is to ﬁnd an alternative lower-dimensional representation of the images,\\\\nwhich preserves most of the information of the original image. In mathematical terms, this\\\\ndecomposition can be expressed using the formula in Equation 3.9 ,ebeing an orthogonal\\\\nbasis [ 76]. If all basis vectors are used, the original image can be reconstructed using a\\\\nlinear combination of the basis vectors [ 67,16]. The number of basis vectors is limited by\\\\nthe minimum of the training set size N[67] and the number of pixels n[16]. In order to\\\\ncompress the input from a nto am-dimensional space, given m⌧n,o n l yt h eﬁ r s t mbasis\\\\nvectors are used. The parameter mand the basis eis chosen such that ˆxiis small for i\\\\x00m\\\\n[76]. The compressed version of the image is denoted x\\'ˆx=[ ˆx1,ˆx2,. . . ,ˆxm]T. In other\\\\nwords: The compressed image is a vector of the ﬁrst mweights of the linear combination\\\\nof weight and basis vectors used to transform the compressed image back to the original\\\\nspace. The weights denote the position of the projection of the face images in the feature\\\\nspace or so-called face space spanned by the ﬁrst mbasis vectors [ 67].\\\\nIn the context of Eigenfaces, one basis used for decomposition is the Karhonen-Loéve ( KL)\\\\nbasis, i.e. PCA [76,67]. According to Zhang et al. ,t h e KLrepresentation is optimal in\\\\nthe sense that it minimizes the Root Mean Square Error ( RMSE ) between the original\\\\nimage and the compressed image calculated using m<n orthogonal vectors. The KL\\\\nbasis consists of the eigenvectors of covariance matrix C=E⇥\\\\nxxT⇤\\\\nof the input images x\\\\n[76]. Since these eigenvectors can have facial features, they are called Eigenfaces . There\\\\nare two approaches in the literature to determine the number of Eigenfaces mused to\\\\ncompress the input images:\\\\n(a)The cumulative explained variance of the ﬁrst i\\\\uf8ffneigenvectors (sorted by eigenval-\\\\nues\\\\x00i) is calculated [ 76,16,64]. The eigenvalues \\\\x00ican be interpreted as the amount\\\\nof variance explained by the corresponding eigenvector ei, which is equivalent to in-\\\\nformation or entropy. The user can choose how much variance, i.e. information,\\\\nshould be preserved, by choosing msuch that the explained variance is greater than\\\\na chosen threshold. Sudiana et al. use a threshold of 90%. A plot displaying the\\\\ncumulative explained variance and a threshold of 90% is shown in Figure 3.15 (a).\", \"3 Fundamentals 23\\\\n(b)The number of Eigenfaces mis chosen using the reconstruction error-complexity\\\\ntrade-oﬀ. The reconstruction error, i.e. the RMSE of the original image xand the\\\\ninverse transformed image x0, is calculated in Equation 3.10 for diﬀerent values of\\\\nm. The “elbow” point marks the point where the reconstruction error decreases only\\\\nslightly for increasing mand thus, is an indicator for the optimal m.Av i s u a l i z a t i o n\\\\nof this approach is shown in Figure 3.15 (b).\\\\n(a)The cumulative explained variance of the\\\\nﬁrst i\\\\uf8ffneigenvectors (sorted by\\\\neigenvalues \\\\x00i).(b)The reconstruction error RMSE calculated\\\\nfor diﬀerent values of m. The\\\\nreconstruction error increases less rapidly\\\\nafter 10 to 20 components.\\\\nFigure 3.15 :Two approaches to determine the number of Eigenfaces mused to compress\\\\nthe input images.\\\\nRSME =sPN\\\\ni=1(xi\\\\x00x0\\\\ni)2\\\\nN(3.10)\\\\nC\\'1\\\\nNNX\\\\nk=1xkxT\\\\nk=1\\\\nNXXT(3.11)\\\\nei=1p\\\\x00iXvi (3.12)\\\\nIn order to reduce calculation complexity, Cis approximated. Zhang et al. propose the\\\\napproximation displayed in Equation 3.11 , with X=[x1,x2,. . . ,xN],xi2Rn[76].\\\\nFinding the eigenvectors of XXTis still computationally expensive, since XXTis a n\\\\nbynmatrix. According to Zhang et al. , the eigenvectors of XXTcan be calculated by\\\\nusing the eigenvectors of XTX. The eigenvectors ei2RnofXXTcan be derived from\\\\nthe eigenvectors vi2RNofXTXusing Equation 3.12 as discussed in more detail in\\\\n[76]. According to Anowar et al. , the problem is reduced to a NbyNmatrix, which\\\\nis computationally less expensive to solve assuming N⌧n. The eigenvectors can be\\\\ncalculated using Singular Value Decomposition ( SVD )[76].SVD is a method, which\\\\ndecomposes a matrix into the so-called left singular vector, the diagonal matrix and the\\\\nright singular vector [ 6].\", \\'3 Fundamentals 24\\\\nIn the literature, face images are classiﬁed by comparing their position in the face space\\\\nwith those of already known faces [ 67]. According to [ 67], this approach performs well\\\\non datasets with little variation in pose, lighting and facial expression. However, Zhang\\\\net al. state, that the performance deteriorates if the variations increase since the changes\\\\nintroduce a bias and thus, the distance function used to make classiﬁcations is no longer a\\\\nreliable measure.\\\\n3.6 Clustering\\\\nClustering is used in a variety of domains to group data into meaningful subclasses [ 54,\\\\n17,35]. According to Patwary et al. andRadu et al. , common domains include anomaly\\\\ndetection, noise ﬁltering, document clustering and image segmentation. The objective is to\\\\nﬁnd clusters, which have a low inter-class similarity and a high intra-class similarity [ 54].\\\\nThe similarity is measured by a distance function, which is dependent on the data type.\\\\nCommon distance functions are the Euclidean distance, the Manhattan distance and the\\\\nMinkowski distance [ 35].\\\\nThere are multiple clustering techniques, which can be divided into four categories [ 2]:\\\\n•Hierarchical clustering : Algorithms, that create spherical or convex-shaped clus-\\\\nters, possibly naturally occurring. A terminal condition has to be deﬁned beforehand.\\\\nExamples include CLINK, SLINK [ 17] and Ordering Points To Identify Clustering\\\\nStructure ( OPTICS )[54].\\\\n•Partitional based clustering :A l g o r i t h m s ,t h a tp a r t i t i o nt h ed a t ai n t o kclusters,\\\\nkis given apriori. Clusters are shaped in a spherical manner, are similar in size and\\\\nnot necessarily naturally occurring. KMeans is a popular example of a partitional-\\\\nbased clustering algorithm.\\\\n•Density based clustering : Density is deﬁned as the number of objects within a\\\\ncertain distance of each other [ 35]. The resulting clusters can be of arbitrary shape\\\\nand size. The algorithm usually chooses the optimal number of clusters given the\\\\ninput data. However, some algorithms are sensitive to input parameters, such as\\\\nradius, minimum number of points and threshold. Popular examples are Density-\\\\nBased Spatial Clustering of Applications with Noise ( DBSCAN )a n d OPTICS .\\\\n•Grid based clustering : Similar to density-based clustering, but according to\\\\nAgrawal et al. better than density-based clustering. Examples include ﬂexible grid-\\\\nbased clustering [ 17].\\\\nMultiple approaches listed below use the term \"-neighbourhood , which is deﬁned as the set\\\\nof all objects within a certain distance \"of a given object [ 54]. In other words: N\"(x)=\\\\n{y2X|dist(x, y)\\\\uf8ff\", y6=x},\"being the so-called generating distance.\\', \\'3 Fundamentals 25\\\\n3.6.1 DBSCAN\\\\nThe clusters identiﬁed by DBSCAN have a high density and are separated by low-density\\\\nregions [ 35]. In order to create clusters of minimum size and density, DBSCAN distin-\\\\nguishes between three types of objects [ 35]:\\\\n•Core objects : An object xwith at least minPts 2Nobjects in its \"-neighbourhood\\\\nN\"(x), i.e. |N\"(x)|\\\\x00minPts is true [ 54].\\\\n•Border objects : An object with less than minPts objects in its \"-neighbourhood,\\\\nwhich is in the \"-neighbourhood of a core object.\\\\n•Noise objects : An object, which is neither a core object nor a border object.\\\\nKanagala and Krishnaiah deﬁne y2Xasdirectly density reachable from x2X,i fyis in\\\\nthe\"-neighbourhood of core object x[35]. Moreover, a point y2Xisdensity reachable\\\\nfrom x2X, if there is a chain of objects x1,. . . ,x nwith x1=xandxn=y, which are\\\\ndirectly density reachable from each other as displayed in Figure 3.16 [35].\\\\nFigure 3.16 :Density reachability cf. [ 5]. The object y2Xis density reachable from\\\\nx2X, since it exists a chain of directly density reachable objects between x\\\\nandy.\\\\nThe objects x2Xandy2Xare said to be density connected , if there is an object o,\\\\nfrom which both xandyare density reachable [ 35]. Density connectivity is visualized in\\\\nFigure 3.17 .\\\\nFigure 3.17 :Density connectivity cf. [ 5]. The objects xandyare density connected since\\\\nthere is an object o, from which both xandyare density reachable.\\\\nThe DBSCAN algorithm starts by labeling all objects as core, border or noise points.\\\\nThen, it eliminates noise points and links all core points, which are within each other’s\\\\nneighbourhood [ 35]. Groups of connected core points form a cluster. In the end, every bor-\\\\nder point is assigned to a cluster. The non-core point cluster assigning is non-deterministic\\\\n[54]. This algorithm creates clusters as a maximal set of density-connected points [ 35].\\', \\'3 Fundamentals 26\\\\nAccording to Kanagala and Krishnaiah ,DBSCAN can identify outliers or noise. How-\\\\never, the algorithm is sensitive to the input parameters minPts and\"and has diﬃculties\\\\ndistinguishing closely located clusters [ 35]. Moreover, if one wants to obtain hierarchical\\\\nclustering, one has to run the algorithm multiple times with diﬀerent \", which is expen-\\\\nsive in terms of memory usage [ 54]. According to Radu et al. ,DBSCAN is aﬀected by\\\\nthe curse of dimensionality. Since DBSCAN relies on nearest neighbour queries and these\\\\nbecome less meaningful in high dimensions, i.e. distances become diﬃcult to interpret, the\\\\nquality and accuracy of the results decline with increasing dimensionality [ 56].Radu et al.\\\\nfound that their DBSCAN model assigns most objects noise when the dimensionality is\\\\nsuﬃciently large.\\\\n3.6.2 OPTICS\\\\nOPTICS does not return an explicit clustering, but rather a density-based clustering struc-\\\\nture of the data, which is equivalent to repetitive clustering for a broad range of parameters\\\\n[5].Ankerst et al. claim that real-world datasets cannot be described by a single global\\\\ndensity, since they often consist of diﬀerent local densities, as displayed in Figure 3.18 .\\\\nFigure 3.18 :Clusters with diﬀerent densities cf. [ 5]. Since C1andC2have diﬀerent densi-\\\\nties than AandB, a clustering algorithm using one global density parameter\\\\nwould detect the clusters A,BandC, rather than A,B,C1andC2.\\\\nOpposed to DBSCAN ,OPTICS is able to detect clusters of varying densities [ 17].OPTICS\\\\nproduces an order of the elements according to the distance to the already added elements\\\\n[17,54]: The ﬁrst element added to the order list is arbitrary. The order list is iteratively\\\\nexpanded by adding the element of the \"-neighbourhood to the order list, which has the\\\\nsmallest distance to any of the elements already in the order list. Hence, clusters with\\\\nhigher density, i.e. lower \", are added ﬁrst (prioritized) [ 35,5]. When there are no more\\\\nelements in the \"-neighbourhood to add, the process is repeated for the other clusters. The\\\\nnon-core point cluster assigning is non-deterministic [ 54].\\\\nRD(y)=(\\\\nNULL if | N\"(x)|< minPts\\\\nmax(core_dist(x),d i s t(x, y))otherwise(3.13)\\', \\'3 Fundamentals 27\\\\nOPTICS saves the reachability distance RD(y), as calculated in Equation 3.13 from [ 54],\\\\nwith core distance core_distbeing the minimal distance \"minsuch that |N\"min(x)|\\\\x00\\\\nminPts (i.e. the distance to the minPtsthpoint in N\") or NULL else, of each element y\\\\nto its predecessor xin the order list and thus, a representation of the density necessary\\\\nto keep two consecutive objects xandyin the same cluster [ 54]. If\"<R D (y), then yis\\\\nnot density reachable from any of its predecessors and thus, one can determine whether\\\\ntwo points are in the same cluster using the information saved by OPTICS [54,5]. If\\\\nthe core distance of an element is not NULL, i.e. it is a core object, and it is not density\\\\nreachable from its predecessors, it is the start of a new cluster [ 5]. Otherwise, the element\\\\nis a noise point. According to Patwary et al. , the algorithm builds a spanning tree, which\\\\nenables obtaining the clusters for a given \"by returning the connected components of the\\\\nspanning tree after omitting all edges with \"<R D (y)[54]. The relationship between \",\\\\ncluster density and nested density-based clusters is displayed in Figure 3.19 .\\\\nFigure 3.19 :The relationship between \", cluster density and nested density-based clusters\\\\ncf. [5]. For a constant minPts , clusters with higher density such as C1,C2\\\\nandC3, i.e. a low \"2value, are completely contained in lower density clusters\\\\nsuch as Cgiven \"1>\"2. This idea forms the basis of OPTICS of expanding\\\\nclusters iteratively and thus, enables the detection of clusters for a broad range\\\\nof neighbourhood radii 0\\\\uf8ff\"i\\\\uf8ff\".\\\\nThis procedure enables the extraction of clusters for arbitrary 0\\\\uf8ff\"i\\\\uf8ff\"[35,5]. According\\\\ntoPatwary et al. ’s work, even though the clustering algorithm is expensive, the extraction\\\\nonly needs linear time. Ankerst et al. claim that the algorithm yields good results if\\\\nthe input parameters minPts and\"are “large enough” and thus, the algorithm is rather\\\\ninsensitive to the input parameters.\\\\nThe smaller \"is chosen, the more objects will be identiﬁed as noise and thus, the algorithm\\\\nwill not identify clusters with low density, since some objects only become core objects for a\\\\nlarger \"[5]. According to Ankerst et al. ,t h eo p t i m a lv a l u ef o r \"creates one cluster for most\\\\nof the objects with respect to a constant minPts , since information about all density-based\\\\nclusters for \"i<\"is preserved. Ankerst et al. present a heuristic for choosing \"based on\\\\nthe expected k-nearest neighbour distance [ 5].\\\\nHigh values for minPts smoothen the reachability curve, even though the overall shape\\\\nstays roughly the same [ 5]. According to Ankerst et al. , the optimal value for minPts is\\\\nbetween 10 and 20.\\', \\'3 Fundamentals 28\\\\n3.7 Software frameworks\\\\nThe embeddings obtained by the methods described in Section 3.2 are stored in a Elas-\\\\nticsearch database. It is described in Subsection 3.7.1 . The diﬀerent methods explored in\\\\nthis work ought to be presented in a web application. This application should be used to\\\\ncompare the methods and to visualize the results. Subsection 3.7.2 andSubsection 3.7.3\\\\nrespectively describe Flask and Angular, which are the frameworks and components used\\\\nto implement the web interface.\\\\n3.7.1 Elasticsearch database\\\\nElasticsearch is a widely used non-relational database, which was designed to store and\\\\nperform full-text search on a large corpus of unstructured data [ 70]. This open-source\\\\ndistributed document-driven database system is built in Java and is based on the Apache\\\\nLucene (Java) library for high-speed full-text search [ 70,74]. According to Zamﬁr et al. ,\\\\nElasticsearch provides Wikipedia’s full-text search and suggestions as well as Github’s\\\\ncode search and Stack Overﬂow’s geolocation queries and related questions. It enables\\\\nnear real-time search by short refreshing periods which make performed operations on the\\\\ndata quickly available for search.\\\\nElasticsearch is a document store, which stores schemaless key-value pairs called documents\\\\n[22]. The documents are stored in logical units, so-called indices. As stated by Zamﬁr\\\\net al. andVoit et al. , the indices are structured similarly to Apache Lucene’s inverted\\\\nindex format. An index can be spread into multiple nodes. A node is a single running\\\\ninstance of Elasticsearch [ 74]. An index is divided into one or more shards, which can be\\\\nstored on diﬀerent servers and enable parallelization. Replicas are copies of shards, which\\\\ncreate redundancy and thus, ensure availability.\\\\nThe documents are saved in a JavaScript Object Notation ( JSON ) format [ 70]. A docu-\\\\nment’s ﬁelds and ﬁeld types are deﬁned by the user when initializing the database index.\\\\nBy default, every ﬁeld of a document is indexed and searchable [ 74].\\\\nBy specifying the unique _id of a document and the database index ,i ti sp o s s i b l et o\\\\nretrieve a speciﬁc document from the database using a GETendpoint of the HTTP API. The\\\\nparameters _source_excludes or_source_includes can be used to deﬁne the structure\\\\nof the response [ 19].\\\\nThe keyword used when performing a full-text search is match . To query for a speciﬁc\\\\nvalue, one has to specify the ﬁeld of interest and the query value.\\\\nElasticsearch preprocesses the query value before starting the search [ 19]. The default\\\\npreprocessing steps of the so-called default analyzer include tokenization and lowercasing.\\\\nOmitting stop words is disabled by default, but custom stop words can be provided by the\\', \\'3 Fundamentals 29\\\\nuser or the English stop word list can be used. It is possible to create custom tokenizers,\\\\nwhich split the query value into tokens of a certain maximum length.\\\\nAnother useful feature of Elasticsearch is the multi-term synonym expansion where the user\\\\nquery is expanded to include synonyms of the query terms [ 19]. The maximum number of\\\\nexpansion terms is set to 50 by default but can be conﬁgured by the user. By default, the\\\\nmulti-term synonym expansion option is enabled.\\\\nElasticsearch also provides the option to perform fuzzy matching instead of exact search.\\\\nBy enabling the fuzzy matching option, a Elasticsearch query consisting of, for instance,\\\\nBahama returns documents that contain the word Bahamas . By default, this option is not\\\\nenabled but can be enabled and conﬁgured individually by the user [ 19].\\\\nAnother search option of Elasticsearch is the k-Nearest Neighbour ( kNN) search on real-\\\\nvalued vectors. The return value of a kNN search is the knearest neighbours to the query\\\\nvector in terms of a certain distance function [ 42]. In order to perform kNN search on a ﬁeld\\\\nit has to be of type dense_vector , indexed and a similarity measure has to be deﬁned\\\\nwhen initializing the database [ 19]. The query value must have the same dimension as the\\\\nvectors stored in the database. A kNN search either returns the exact brute-force nearest\\\\nneighbours or an approximation of the nearest neighbours calculated by the Hierarchical\\\\nNavigable Small World ( HNSW )a l g o r i t h m[ 42,19].HNSW is a graph-based algorithm\\\\n[42].\\\\nBesides Elasticsearch, the elastic stack oﬀers other tools, for instance, Kibana, which pro-\\\\nvides a user interface to manage diﬀerent models. After saving a model in Kibana, it is\\\\npossible to create a text embedding ingest pipeline, which embeds new documents or rein-\\\\ndexes existing documents [ 20]. Elasticsearch’s kNN implementation not only allows literal\\\\nmatching on search terms but also semantic search incorporating Kibana’s text embedding\\\\ningest pipeline on search terms [ 19].\\\\n3.7.2 Flask\\\\nFlask is open source and written in Python by Armin Ronancher in 2004 [ 7,50]. According\\\\ntoCopperwaite and Leifer and Muﬁd et al. ,F l a s ki so n eo ft h em o s tp o p u l a rP y t h o n\\\\nweb frameworks. It provides powerful libraries for core functionality such as routing,\\\\ntemplating, and HTTP request parsing [ 15]. It can be extended with additional plugins\\\\nwithout aﬀecting the internal structure of the existing system [ 7].\\\\nFlask uses the Jinja Template Engine for template ﬁles including HTML pages, whereas\\\\nstatic ﬁles such as Cascading Style Sheet ( CSS) ﬁles are handled using the Werkzeug WSGI\\\\ntoolkit [ 7]. According to Aslam et al. , Jinja is modeled after the Django template system.\\\\nWerkzeug implements, for instance, requests and response objects [ 50].\\\\nAll requests received from clients are passed to an instance of the Flask application [ 25].\\\\nHence, the ﬁrst step is to create an instance of the Flask class as shown in Listing 3.1.\\', \"3 Fundamentals 30\\\\n1app =Flask( __name__ )\\\\nListing 3.1 :Initialization of a Flask application instance.\\\\nClients send requests to the web server, which passes them to the Flask application in-\\\\nstance. The queries are then routed to the corresponding functions. Routing is the process\\\\nof mapping Uniform Resource Locator ( URL ) paths to functions [ 25]. To deﬁne a route,\\\\ntheroute decorator is used as displayed in Listing 3.2.\\\\n1@api .route( \\'/documents/<id> \\',e n d p o i n t =\\'document \\')\\\\n2class Document (Resource):\\\\n3 def get(self ,id):\\\\n4 client =Elasticsearch(CLIENT_ADDR)\\\\n5 return query_database .get_doc_meta_data(client, doc_id =id)\\\\nListing 3.2 :Exemplary deﬁnition of a function to display routing with Flask. The route\\\\ndecorator is used to deﬁne the URL path.\\\\nURL s can contain dynamic components, which are enclosed in <>angle brackets. The\\\\nvalues of these components are passed to the function as arguments [ 25]. By default,\\\\ndynamic components are of type string . However, other types including intandfloat\\\\nare supported.\\\\nAn endpoint is a class with certain methods, which can be accessed using HTTP requests.\\\\nEvery endpoint can have multiple decorators, including GET,POST ,PUTandDELETE [22].\\\\nThe GETmethod is used to retrieve data from the server, whereas the other methods are\\\\nused to either insert, update or delete data.\\\\n3.7.3 Angular\\\\nAngular is a framework for building web applications. It uses Node.js and TypeScript.\\\\nUsually, the source code is structured into diﬀerent modules, including components and\\\\nservices. Components are used to deﬁne the appearance of the application, while services\\\\ncontain the logic of the application and communicate with the backend.\\\\nAngular applications are created using the ng new <name> command line interface [ 61].\\\\nThis command creates a skeleton, which can be customized to meet the needs of the\\\\napplication.\", \\'4 Own approach 31\\\\n4O w n a p p r o a c h\\\\nIn this thesis, a tool is developed that oﬀers text queries, detailed document inspection and\\\\nqueries for semantically or visually similar documents to the user. This chapter describes\\\\nhow the theoretical basics from Chapter 3 interplay and how they are used to construct\\\\nthis tool. Section 4.1 outlines the steps carried out before the application is operative,\\\\nSection 4.2 covers the resulting application and Section 4.3 discusses the dilemma faced\\\\nwhen balancing memory usage and query time. Speciﬁc parameter choices are explained\\\\ninChapter 5 .\\\\n4.1 Oﬄine Processing\\\\nThe tasks carried out before the application is operative are outlined in this section. They\\\\nare considered to be oﬄine preprocessing steps. A process works in an oﬄine fashion if the\\\\nprocess requires access to the whole data at once [ 29]. This section outlines implementation\\\\ndetails of the way the data is derived, the database storing the data and the baseline topic\\\\nanalysis approach compared to this work’s application.\\\\n4.1.1 Database\\\\nFirst, the content of the Elasticsearch database is described, then, the initialization, inser-\\\\ntion and updating process of ﬁlling the database are explained and ﬁnally, the process of\\\\nquerying is outlined.\\\\nContent of the database\\\\nIn this work, the database is ﬁlled once with data from the Bahamas leak. The data is a\\\\nlarge unstructured corpus of PDF ﬁles. Since leak data does not change over time it is not\\\\nnecessary to update the database. After the initialization of the database, it is used for\\\\nqueries. Therefore, the workﬂow of processing the text corpus is carried out completely\\\\noﬄine and in advance.\\\\nThe index Bahamas stores diﬀerent embeddings of the information derived from the text\\\\nlayer and metadata of the documents. As depicted in Figure 4.1 , not only textual informa-\\\\ntion is stored in the database, but also information about the appearance of the ﬁrst page\\\\nof the PDF. The structure of the index is presented in Table 4.1 . The visual information\\\\nis stored in the ﬁelds pca_image ,pca_optics_cluster andargmax_pca_cluster .\\', \\'4O w na p p r o a c h 3 2\\\\nTable 4.1 :Fields of the Elasticsearch database index Bahamas .\\\\nField name Field description\\\\n_id Unique identiﬁer of document i. The identiﬁer is gen-\\\\nerated by the sha256 hash algorithm from hashlib using\\\\nthe PDF ﬁle as input.\\\\ndoc2vec 55 dimensional Doc2Vec embedding of i.\\\\nsim_docs_tﬁdf TF-IDF embedding enhanced by an all-zero ﬂag of i. The\\\\nall-zero ﬂag is one if the TF-IDF embedding consists of\\\\nonly zeros, zero else. If the embedding’s dimensionality\\\\nis greater than 2048, the encoder of a trained AEis used\\\\nto compress the embedding.\\\\ngoogle_univ_sent_encoding 512 dimensional USE embedding of i.\\\\nhuggingface_sent_transformer 384 dimensional SBERT embedding of i.\\\\ninferSent_AE InferSent embedding of i. Since the pretrained InferSent\\\\nmodel embedding’s dimension is 4096, the encoder of a\\\\ntrained AEhas to reduce the dimension to 2048.\\\\npca_image 13-dimensional PCA version of ﬁrst page image of i.\\\\npca_optics_cluster Cluster of iidentiﬁed by OPTICS onPCA version of\\\\nimage.\\\\nargmax_pca_cluster Number of maximum PCA component as cluster of i.\\\\ntext Text of i.\\\\npath Path to i.\\\\nFigure 4.1 :PDFs to Database. First, the data is preprocessed: The ﬁrst page of a PDF\\\\nﬁle is converted to an image and the complete text is extracted. The images\\\\nare stored in the database as well as the text and diﬀerent embeddings of the\\\\ntext. Some values, such as the image or the InferSent embedding, have to be\\\\ncompressed to become a vector of at most 2048 dimensions.\\', \"4 Own approach 33\\\\nInitialization, insertion and updating\\\\nTo facilitate working with and running the code, the initialization of the database is split\\\\ninto multiple steps. As depicted in Figure 4.2 , ﬁrst the database is initialized by deﬁning\\\\nthe index name and the mappings, i.e. the ﬁeld names, types and sizes. This step is carried\\\\nout using the method create .\\\\nFigure 4.2 :Procedure of initialization and ﬁlling of the database.\\\\nAfterwards, the documents are created using the method create . The initial creation of\\\\na document only deﬁnes the ﬁelds id,text andpath .\\\\nThe embeddings are added to the documents in a third step. To increase the eﬃciency\\\\nof this step, data parallelism, i.e. parallelizing the execution of a method across multiple\\\\ninput values, is applied. In this work, a set of paths to documents is split among multiple\\\\nprocesses. First, the absolute paths of all documents are saved in a list. This list is\\\\npartitioned into num_cpus many lists sub_lists of similar size. Each process works on\\\\na sublist. The Pool object from the multiprocessing module is used for data parallelism.\\\\nThe steps carried out are displayed in Listing 4.1. The embeddings are subsequentially\\\\ninserted into the database for each sublist.\\\\n1with Pool(processes =num_cpus) aspool:\\\\n2 for model_name inmodel_names:\\\\n3 proc_wrap =wrapper(model_name =model_name, baseDir =src_path)\\\\n4 pool .map(proc_wrap, sub_lists)\\\\nListing 4.1 :Usage of Pool for data parallelism. The paths to the documents are partitioned\\\\ninto sublists which are simultaneously inserted into the database. Since the\\\\nPool object does not work with a lambda function, a class wrapper is created\\\\nwhich provides the same functionality.\\\\nThe document embeddings are added to the database using the method update as displayed\\\\nin Listing 4.2.\\\\n1client .update(index =\\'bahamas \\',id=id,b o d y ={\\'doc\\':\\\\n2 {MODELS2EMB[model_name]: embedding}})\\\\nListing 4.2 :Update of a database entry to insert a speciﬁc embedding.\\\\nQueries\\\\nThe default analyzer is used for the full-text search since for instance conﬁguring a maxi-\\\\nmum token length did not seem necessary or likely to improve the results.\", \"4O w na p p r o a c h 3 4\\\\n1results =elastic_search_client .search(\\\\n2 index =\\'bahamas \\',\\\\n3 size =count,\\\\n4 from_ =(page *count),\\\\n5 query ={\\'match \\':{\\\\n6 \\'text \\':{\\'query \\':text,\\\\n7 \\'fuzziness \\':\\'AUTO \\',}\\\\n8 },\\\\n9 }, source_includes =SRC_INCLUDES)\\\\nListing 4.3 :Exemplary query to an Elasticsearch database index. The parameters size\\\\nandfrom_ deﬁne the number of results to return and the start index of the\\\\nresults. To enable fuzzy search a value for fuzziness has to be set.\\\\nMoreover, the fuzzy matching option is set to AUTO , which means in terms of keyword or\\\\ntext ﬁelds that the allowed Levenshtein Edit Distance, i.e. number of characters changed\\\\nto create an exact match between two terms, to be considered a match, is correlated to\\\\nthe length of the term [ 19]. By default, terms of length up to two characters must match\\\\nexactly, terms of length three to ﬁve characters must have an edit distance of one and terms\\\\nof length six or more characters must have an edit distance of two [ 19]. An exemplary query,\\\\nwhich uses fuzzy search is given in Listing 4.3.\\\\nAccording to Malkov and Yashunin ,o n eo f kNN search’s use cases is semantic document\\\\nretrieval, which makes it a good ﬁt for this task. In this work, the approximate nearest\\\\nneighbours search HNSW is used since it is faster and the results are good enough for the\\\\npurpose of this work. The similarity measure used in this work is the cosine similarity. The\\\\nother similarity measures provided by Elasticsearch are the l2_norm or so-called Euclidian\\\\ndistance and the dot_product which is the non-auto-normalized version of the cosine\\\\noption. Since cosine is not deﬁned on vectors with zero magnitude, embeddings that can\\\\nreturn all zero vector representations, such as TF-IDF , are enhanced with an all-zero ﬂag\\\\nbefore inserting them into the database.\\\\nIn this work, the only tool from the elastic stack used is Elasticsearch. Without Kibana,\\\\nthe used models are saved on disk as Pickle ( PKL ) ﬁles. Consequently, instead of using\\\\nthekNN query structure for semantic search on embeddings provided by Elasticsearch and\\\\nKibana, the normal kNN search on a ﬁeld that contains an embedding is used.\\\\n4.1.2 Eigendocs\\\\nIn this work, the Eigenfaces approach from Subsection 3.5.2 is used to compress the images\\\\nof the ﬁrst page of the documents. The idea is that documents not only hold textual\\\\ninformation but also visual information, such as layout, company logo or signature. By\\\\nmapping those images on a subspace, they ought to be grouped by visual similarity. The\", \\'4 Own approach 35\\\\nFigure 4.3 :From PDFs to Eigendocs. Firstly, the ﬁrst page of a document is converted\\\\nto an image. Then, the image is preprocessed: It is placed on a white canvas,\\\\nto ensure all images have the same dimensions. Moreover, it is converted to\\\\ngreyscale and normalized to values between zero and one. Afterwards, the two-\\\\ndimensional image is reshaped into a one-dimensional array. Lastly, the image\\\\nis compressed using Eigendocs.\\\\nprocedure of the Eigenfaces adaption Eigendocs is displayed in Figure 4.3 . Diﬀerent stages\\\\nof this approach are displayed in Figure 4.4 .\\\\nFigure 4.4 :10 randomly selected documents from the test set. The number of images in\\\\nthe test set is 561, while the PCA model is ﬁtted to 1680 training images.\\\\nThe original images are displayed in the ﬁrst row. The second row shows the\\\\nreconstruction from their compressed version in the fourth row. The third row\\\\nshows the reconstruction error, i.e. the diﬀerence between the reconstructed and\\\\nthe original image. The last row presents the greyscale values of the compressed\\\\n13-dimensional image as a line.\\\\nThe documents are ﬁrst read from a directory. Subsequently, their ﬁrst page is converted\\\\nto an image and saved. The maximum height and width among all images in a corpus\\\\nof 1000 randomly sampled images are calculated. The selection of 1000 images is used to\\\\nreduce the run time of the script. The maximum height and width are used to create a\\\\nwhite canvas for each image which forms the background. Every image is placed in the\\\\nupper left corner of the canvas. Hence, assuming the selection of documents used to ﬁt\\\\nthePCA model is representative, scaling is not necessary and thus, the portion of white\\\\npixels on the right and bottom side encodes the sizes of the original image. Therefore, the\\\\nrelative size of images in the corpus is incorporated in the resulting representation of the\\', \\'4O w na p p r o a c h 3 6\\\\ninput images. However, some images of the test set are bigger than the maximum values\\\\nof the selected images and as a consequence are scaled.\\\\n1def rgb2gray (img):\\\\n2 return 0.299 *img[:,:, 0]+0.587 *img[:,:, 1]+0.114 *img[:,:, 2]\\\\n3#m o r ec o d e\\\\n4C=np.ones((max_w,max_h))\\\\n5C[:doc .shape[ 0],:doc .shape[ 1]]=rgb2gray(doc)\\\\n6documents .append(C .ravel())\\\\nListing 4.4 :Preprocessing of the input images from Dr. Christian Gruhl. Conversion of\\\\nRGB pixel values to greyscale according to [ 41]. The background is a white\\\\ncanvas. The images are converted to one-dimensional greyscale values.\\\\nAfterwards, the images are converted to greyscale using line 5of Listing 4.4. Before\\\\nreturning the image, the two-dimensional image vectors are converted to one-dimensional\\\\nones as displayed in line 6of Listing 4.4. The decomposition is transformed using PCA as\\\\ndisplayed in Listing 4.5. The implementation of PCA from sklearn intrinsically normalizes\\\\nthe data as described in Subsection 3.5.2 .\\\\n1pca =decomposition .PCA(n_components =n_components, whiten =True ,\\\\n2 svd_solver =\"randomized\" )\\\\nListing 4.5 :Initialization of the PCA instace used to compress the images. Since the\\\\nEigenfaces approach uses SVD , the adaption Eigendocs has to be implemented\\\\nlikewise applying a svd_solver .\\\\n4.1.3 Embeddings\\\\nFirstly, the implementation of the AEused to compress high-dimensional embeddings is\\\\npresented. Then, the models used to encode the textual data are outlined below with\\\\nregard to implementation details.\\\\nAutoencoder\\\\nIn this work, an AEis used to reduce the dimensionality of the InferSent and the TF-IDF\\\\nembeddings. Since the InferSent model is pretrained, it is not possible to change the\\\\ndimensionality of the embedding without a considerably big eﬀort, i.e. retraining the model\\\\non a suﬃciently large data corpus and reconﬁguring the model’s parameters. Therefore,\\\\nit is not feasible to change the dimensionality of the InferSent embedding, but rather add\\\\na supplementary layer after the model to produce the ﬁnal embedding. Similarly, the\\\\nTF-IDF embedding dimension correlates with the vocabulary size and thus, the size of the\\\\ndata corpus. Further reducing the vocabulary size would decrease the TF-IDF model’s\\', \"4 Own approach 37\\\\nquality. Hence, the idea is to use the encoder of an AEto reduce the dimensionality of the\\\\nInferSent and the TF-IDF embedding.\\\\nFigure 4.5 :Architecture of the AE.\\\\nThe implementation was provided by the blog post from [ 33]. It uses the library keras1.\\\\nThe architecture is adapted to fulﬁl the needs of the speciﬁc context. It is presented in\\\\nFigure 4.5 .\\\\nTF-IDF\\\\nThe TF-IDF model has to be initialized and trained on the data corpus to build a\\\\ndata-speciﬁc vocabulary. An exemplary implementation is given in Listing 4.6. The\\\\nTfidfVectorizer is provided by the scikit-learn package. The input parameter de-\\\\nﬁnes the input type, i.e. content means that the input is a list of strings or bytes, whereas\\\\nfile assumes the input has a read method and filename denotes a list of ﬁlenames as\\\\ninput [ 66]. An embedding is obtained using the command from Listing 4.7.\\\\n1tfidf_model =TfidfVectorizer( input =\\'content \\',\\\\n2 preprocessor =TfidfTextPreprocessor() .transform, min_df =3,\\\\n3 max_df =int(len(docs) *0.07 ))\\\\n4tfidf_model .fit(documents)\\\\nListing 4.6 :Initialization of the TF-IDF model. Firstly, an instance of the\\\\nTfidfVectorizer class is created. Secondly, the fit method is called to ﬁt\\\\nthe model on the documents.\\\\n1tfidf_model .transform(text) .todense()\\\\nListing 4.7 :Encoding a text using the TF-IDF model.\\\\nThe preprocessor parameter deﬁnes the preprocessing, i.e. string transformation, stage.\\\\nIt is possible to override the default with a custom preprocessing function. The parameters\\\\n1https://keras.io/ (last accessed: 19/11/2023)\", \\'4O w na p p r o a c h 3 8\\\\nmin_df andmax_df deﬁne the minimum and maximum document frequency of a word in\\\\nthe corpus to be considered relevant. The default values are 1, i.e. a term has to appear\\\\nat least once, and 1.0, i.e. a term appears at most in all documents, respectively [ 66].\\\\nBy default, the scikit-learn implementation uses the norm=’l2’ conﬁguration, i.e. the\\\\nEuclidean norm. The implementation of TF-IDF inscikit-learn is diﬀerent from the\\\\noriginal TF-IDF deﬁnition. The diﬀerence is the calculation of the IDFpart, which is given\\\\ninEquation 4.1 from [ 66]. The one is added to Mijdue to the parameter smooth_id=True\\\\nby default to prevent zero divisions and to avoid logarithmic divergences due to a zero\\\\nargument [ 55]. After calculating the TF-IDF values, they are normalized by the Euclidean\\\\nnorm given in Equation 4.2 .\\\\nidf(wij) = log1+M\\\\n1+Mij+1 (4.1)\\\\nvnorm=v\\\\nkvk2=vq\\\\nv2\\\\n1+v2\\\\n2+...+v2\\\\nM(4.2)\\\\nFigure 4.6 :TF-IDF pipeline. Firstly, the text extracted from the documents is prepro-\\\\ncessed using a custom preprocessor. Then, the TF-IDF values are obtained from\\\\ntheTfidfVectorizer . Afterwards, the all-zero ﬂag is added to the TF-IDF\\\\nweights. If the resulting dimensionality is bigger than 2048, the encoder of an\\\\nAEis used to reduce the dimensionality. The results are stored in the database.\\\\nThe pipeline in Figure 4.6 visualizes the steps carried out in this work to derive the\\\\nTF-IDF embedding of a text and store it in the database. The text of the PDFs is ex-\\\\ntracted and preprocessed using a custom preprocessor. Thereafter, it is embedded using the\\\\nTfidfVectorizer . The TF-IDF weights are the embedding. Before storing the TF-IDF\\\\nweights in the database, they are enhanced with an all-zero ﬂag. The all-zero ﬂag ensures\\\\nthat no all-zero vectors are stored in the database by extending those that have a zero\\\\nmagnitude with a “1” entry and “0” otherwise. All-zero TF-IDF weights indicate that a\\\\ndocument does not have any terms with the vocabulary in common. Since the vocabulary\\\\nis kept relatively small with respect to the number of diﬀerent words in the data corpus\\\\nto reduce the dimensionality of the embeddings, it is not unlikely that a document does\\\\nnot contain any of the vocabulary terms. The all-zero ﬂag is necessary because the cosine\\', \\'4 Own approach 39\\\\nsimilarity used to query for similar documents in the database cannot handle vectors of\\\\nzero magnitude. This alteration does not change the cosine similarity between non-zero\\\\nmagnitude vectors, since the additional zero adds no supplementary information to the\\\\ncalculation of the cosine similarity. The vectors whose all-zero ﬂag is one have a cosine\\\\nsimilarity of one when being compared to each other.\\\\nFigure 4.7 :Preprocessing visualized using an example text. The stop word removal im-\\\\nplicitly tokenizes the text.\\', \\'4O w na p p r o a c h 4 0\\\\nThe preprocessing steps of the custom preprocessor are visualized in Figure 4.7 .F i r s t l y , t h e\\\\naccents are stripped from the text. Then, all new line symbols are replaced with a whites-\\\\npace. Afterwards, the text is converted to lowercase. Then the numbers are discretized,\\\\ni.e. all numbers between 0 and 99999 are replaced with the string SMALLNUMBER , numbers\\\\nbigger than 99999 are replaced with the string BIGNUMBER and ﬂoats are replaced with the\\\\nstring FLOAT . The next step is to remove all punctuation symbols. To ensure empty tokens\\\\ngenerated by prior preprocessing steps are omitted, all sequences of multiple subsequent\\\\nwhitespaces are discarded. After that, the symbols for numbers are enclosed with pointed\\\\nbrackets, e.g. <SMALLNUMBER> . Then, the text is tokenized, i.e. split at whitespaces, and\\\\nstop words are omitted. The stop word list is provided by the nltk package and consists\\\\nof common English stop words. Afterwards, the tokens are lemmatized. The lemmatizer\\\\nused is the WordNetLemmatizer from the nltk package. The WordNetLemmatizer uses the\\\\nEnglish lexical database WordNet to return valid stems [ 53]. In the end, the tokens are\\\\njoined to a string and returned.\\\\nSince the dimensionality of the TF-IDF embeddings is big for a large text corpus, an AEis\\\\nused to reduce the dimensionality of the embedding if its dimensionality exceeds 2048.\\\\nDoc2Vec\\\\nThe library gensim provides the Doc2Vec model used in this work. The model is initialized\\\\nwith input data of type tagged documents , which are documents with (numerical) tags.\\\\nIn this work, the default parameters are used. The default algorithm is PVDM [24]. The\\\\nparameters vector_size andwindow deﬁne the dimensionality of the embeddings and the\\\\nsize of the window, i.e. the maximum distance between the current and the predicted word,\\\\nrespectively. The default value for vector_size is 100, whereas the default window size is 8\\\\n[24,23]. The min_count parameter deﬁnes a threshold below which words will be ignored.\\\\nIts default value is 5. The workers parameter denotes the number of threads to be used\\\\nfor training. The default value is 1 [ 24]. The epochs parameter speciﬁes the number of\\\\niterations over the corpus. The default value is 10. By default, the hierarchical softmax\\\\nalgorithm, i.e. hs=1 , is used for training [ 78]. Many Doc2Vec default values are adopted\\\\nfrom Word2Vec since the gensim Doc2Vec implementation inherits from the Word2Vec\\\\nimplementation.\\\\nInferSent\\\\nThe InferSent model is implemented using PyTorch [ 58]. The parameters used to initial-\\\\nize the model are presented in Listing 4.8. The parameter version in line 6indicates\\\\nwhether the model is trained with GloVe or fastText for the value 1 or 2 respectively.\\\\nSince the model is precomputed, it is not possible to change certain parameters, such\\\\nas the word embedding dimension word_emb_dim or the dimension of the output vectors\\\\nenc_lstm_dim .\\', \"4 Own approach 41\\\\n1\\'bsize \\':64,\\\\n2\\'word_emb_dim \\':300,\\\\n3\\'enc_lstm_dim \\':2048 ,\\\\n4\\'pool_type \\':\\'max\\',\\\\n5\\'dpout_model \\':0.0,\\\\n6\\'version \\':1\\\\nListing 4.8 :Parameters of the InferSent model.\\\\nThe steps necessary to create a working instance of the InferSent model are presented in\\\\nListing 4.9. After the InferSent model is initialized in line 1,t h e state_dict of the model\\\\nis loaded in line 2. This dictionary consists of learnable parameters, i.e. weights and bias,\\\\nof the model. The state_dict is obtained from the PKL ﬁle of InferSent as stated in [ 18].\\\\nThe path to the word embeddings is set in line 3.F i n a l l y ,i nl i n e 4, the vocabulary of the\\\\nmodel is built. More precisely, only those embeddings needed are kept while the rest is\\\\ndiscarded.\\\\n1infersent =InferSent(params_model)\\\\n2infersent .load_state_dict(torch .load(model_path))\\\\n3infersent .set_w2v_path(w2v_path)\\\\n4infersent .build_vocab(docs, tokenize =True )\\\\nListing 4.9 :Initializing the InferSent model.\\\\nIn this work, a custom set of vector representations of words is used. The custom word\\\\nembeddings are computed by a Word2Vec model trained on 2048 randomly selected doc-\\\\numents from the Bahamas dataset which reduces the run time of the script. The only\\\\nparameter which diﬀers from the default settings of Word2Vec is the vector_size which\\\\nis set to 300. After the Word2Vec model is trained, the word embeddings are saved in a\\\\nﬁle whose ﬁle path is the value of w2v_path in line 3of Listing 4.8.\\\\nIn this work, an AEis used to reduce the dimensionality of the InferSent embedding.\\\\nUSE\\\\nTheUSE model implemented with TensorFlow [ 58]. In this work, the fourth version of the\\\\nmodel is used. The implementation from Tfhub uses the DAN architecture [ 68]. The USE\\\\nﬁle has a size of about 1 GB. It is not necessary to preprocess the data for the model.\\\\nSBERT\\\\nThe SBERT model is implemented with PyTorch [ 58]. An instance of the model is ob-\\\\ntained as shown in Listing 4.10. The model contains a BERT transformer, which has a\", \"4O w na p p r o a c h 4 2\\\\nmax_seq_length of128. It does not convert inputs to lowercase by default [ 57]. The\\\\noutput of the BERT transformer is passed to a pooling layer, which is initialized with the\\\\npooling_mode parameter. The default is mean_pooling , which calculates the mean of the\\\\noutput vectors of the transformer. The other options include cls_token_pooling , which\\\\nreturns the output of the ﬁrst token and max_pooling , which returns the maximum value\\\\nof the output vectors. The word embedding dimension is 384 by default [ 57].\\\\n1SentenceTransformer( \\'paraphrase-MiniLM-L6-v2 \\')\\\\nListing 4.10 :Initialization of the SBERT model.\\\\n4.1.4 Clustering using OPTICS\\\\nFigure 4.8 :The ﬁrst page of each document is converted to an image. The image is pre-\\\\nprocessed, i.e. conversion to greyscale and resizing.\\\\nSimilar to the approach from Ankerst et al. ,OPTICS is used to cluster the images of the\\\\nﬁrst page of documents in this work. The procedure is displayed in Figure 4.8 . There are\\\\ntwo diﬀerent preprocessing approaches:\\\\n1.The images are ﬁrst preprocessed to 32x32 normalized greyscale pixels (cf. [ 5]) as\\\\nvisualized in Figure 4.10 and afterwards compressed to 13-dimensional vectors using\\\\nPCA .\\\\n2.The technique Eigendocs from Subsection 3.5.2 is used to compress the images to\\\\n13-dimensional normalized greyscale images as displayed in Figure 4.4 .\\\\n1optics_model =OPTICS(cluster_method =\\'dbscan \\',m i n _ s a m p l e s =2,m a x _ e p s =10,\\\\n2 eps=1.5)\\\\nListing 4.11 :Initialization of the OPTICS model. The minimum number of samples\\\\nmin_samples in a cluster corresponds to minPts .\\\\nThe conﬁgurations used when initializing an OPTICS model greatly inﬂuence the clusters\\\\nreturned. The parameter max_eps is inﬁnity by default but can be speciﬁed by the user\\\\nto reduce complexity and runtime. According to literature, max_eps should be big enough\\\\nto include almost all points in a single cluster. The way the reachability plot is used to\\\\nextract clusters is dependent on the cluster_method . One can choose either dbscan orxi\", \\'4 Own approach 43\\\\n(a)The reachability plot of the documents\\\\npreprocessed according to item 1 (cf.\\\\n[5]).(b)The reachability plot of the documents\\\\npreprocessed according to item 2 (i.e.\\\\nEigendocs).\\\\nFigure 4.9 :The plot was created using the OPTICS algorithm from the Python library\\\\nscikit-learn. The underlying dataset consists of 2241 documents from the Ba-\\\\nhamas leak. It shows the reachability distance of each document to its prede-\\\\ncessor in the order list.\\\\nas a clustering method. The parameters min_samples andepsinﬂuence the cluster sizes\\\\nand number of clusters found for a given clustering approach. The value of eps deﬁnes\\\\nthe distance between two points to still be considered neighbours and can be chosen by\\\\nconsulting the reachability plot which is displayed in Figure 4.9 . The code to initialize an\\\\nexemplary OPTICS model is displayed in Listing 4.11.\\\\n4.1.5 Topic analysis\\\\nTwo topic analysis approaches are outlined below. The ﬁrst one serves as the baseline\\\\nmodel of this thesis and the second one is used multiple times throughout the application\\\\nto visualize results obtained by queries.\\\\nTop2Vec\\\\nAngelov ’sTop2Vec model is provided in the Python library Top2Vec [4]. In his work,\\\\nUMAP ’s hyperparameters are set to 15 nearest neighbours, cosine similarity as the distance\\\\nmetric and 5 as the embedding dimension. The word and document embeddings are\\\\ngenerated by the Doc2Vec version PV-DBOW . The window and vector size are 15 and\\\\n300 respectively and a hierarchical softmax is used.\\\\nIn this work, a class is implemented, which uses the Top2Vec library. When initiating\\\\nan instance of this class, the Top2Vec model is trained on the given document corpus as\\\\ndisplayed in Listing 4.12. The class provides methods to query for the number of topics\\\\nas well as the most similar topics and documents to an input keyword. The most similar\\\\ntopics can be visualized using word clouds. The core functionalities are implemented by\\', \\'4O w na p p r o a c h 4 4\\\\nFigure 4.10 :Preprocessing of 100 documents to 32x32 normalized greyscale pixels.\\', \"4 Own approach 45\\\\ntheTop2Vec library, but the class is used to modify the return values to be compatible\\\\nwith the UI.\\\\n1Top2Vec(documents =self .documents, speed =\\'fast-learn \\',w o r k e r s =8)\\\\nListing 4.12 :Initialization of the Top2Vec model.\\\\nWord clouds\\\\nThe implementation of word clouds in this thesis is based on the Python library wordcloud\\\\nbyMüller [51]. This implementation removes English stop words from the text by default.\\\\nThe input text is split into tokens using a regex. By default, plurals are removed if their\\\\nsingular version is present and their frequency is added to their singular version. By default,\\\\nnumbers are not included as tokens.\\\\nIn order to ensure that the words presented are interpretable, the input text is preprocessed\\\\nas displayed in Listing 4.13. The WordNetLemmatizer from the nltk package is used to\\\\nensure the stemmed words exist. A word cloud is initialized as shown in Listing 4.14.\\\\n1lemmatizer =WordNetLemmatizer()\\\\n2tokens =[lemmatizer .lemmatize(token) for token intokens]\\\\nListing 4.13 :Custom preprocessing of word cloud input.\\\\n1wordcloud =WordCloud(width =800,h e i g h t =500,r a n d o m _ s t a t e =21,\\\\n2 contour_width =3,m a x _ f o n t _ s i z e =110,b a c k g r o u n d _ c o l o r =\\'white \\',\\\\n3 max_words =5000 ).generate( \\',\\'.join(tokens))\\\\nListing 4.14 :Initialization of a word cloud.\\\\n4.1.6 Slurm\\\\nSince the data corpus is too big to be processed locally on a Apple M2 Pro MNW83D/A\\\\nwith 16 GBRAM and 12 cores, the Chair Intelligent Embedded Systems ( IES) has oﬀered\\\\nto provide computational means to solve this problem. The scripts can be processed by\\\\nmultiple nodes which are managed by Slurm. Slurm is an open-source management tool\\\\nfor Linux clusters [ 1]. It allocates resources, i.e. compute nodes, and provides the means\\\\nto start, execute and monitor jobs [ 1,73].\\\\nThe so-called Slurm daemons control nodes, partitions, jobs and job steps [ 1]. A partition\\\\nis a group of nodes and a job is the allocation of resources, i.e. compute nodes, to a user for\\\\na limited period of time. A basic visualization of the architecture is given in Figure 4.11 .\", \\'4O w na p p r o a c h 4 6\\\\nFigure 4.11 :Slurm architecture. The management node has a slurmctld daemon, while\\\\nevery compute node has a slurmd daemon. The nodes communicate. The\\\\nuser can use certain commands, for instance srun andsqueue , anywhere on\\\\nthe cluster.\\\\nTable 4.2 :A selection of sbatch scripts used in this work.\\\\nName of sbatch script Description\\\\nae_conﬁg.sh Comparison of diﬀerent AEarchitectures in terms of the met-\\\\nrics cosine similarity and Root Mean Square Error ( RSME ).\\\\nallocate_res.sh Allocates resources to enable a SSH tunnel connection from\\\\na local VSCode instance to the server of the IES. When en-\\\\nabled, the database content can be displayed with the Elas-\\\\nticsearch plugin.\\\\ncreate_database.sh Initializes the database by specifying ﬁelds.\\\\ncreate_documents.sh Inserts the document’s metadata information, i.e. path and\\\\ntext.\\\\nelasticContainer.sh Starts the Elasticsearch container using the headless\\\\npodman-compose up command.\\\\ninit_database.sh Initializes database, subsequentially inserts documents meta-\\\\ndata, embeddings and clusters.\\\\ninsert_clusters.sh Inserts PCA weights, OPTICS and argmax clusters.\\\\ninsert_embeddings.sh Subsequentially inserts embeddings of documents.\\\\nown_w2v_model.sh Creates and saves custom Word2Vec model.\\\\nrun_pdf2png.sh Converts and saves the PNG version of the ﬁrst page of the\\\\nPDFs.\\\\nA job is started by a sbatch script. This script deﬁnes the partition ,t h e job-name ,\\\\nthe number of nodes ,t h e cpus-per-task ,t h em e m o r y mem allocated, the time limit\\\\nand the path to store error andoutput logs. It is possible to work on multiple CPUs\\\\nsimultaneously to divide the workload of a task. In this work, multiple sbatch scripts\\\\nare used to carry out a variety of tasks. A summary of the tasks and scripts is given in\\\\nTable 4.2 .\\', \\'4 Own approach 47\\\\n4.2 Web interface\\\\nA basic web interface is provided to facilitate the comparison of the models explored in\\\\nthis thesis. However, the focus of this work is on the methods and not on the application.\\\\nThe tool consists of a backend and a frontend which are described in Subsection 4.2.1 and\\\\nSubsection 4.2.2 .\\\\n4.2.1 Backend\\\\nThe framework used for the backend is Flask. There are multiple endpoints, which are\\\\nused to retrieve data from the server:\\\\n•Documents: Returns a list of documents, which best match the query. The infor-\\\\nmation returned for each document is the respective id,path ,a n d text . The query\\\\ncan be of type match_all , which returns all documents in the database, or a fuzzy\\\\nfull-text query if text is speciﬁed, or a kNN query on a certain ﬁeld of the database\\\\nif both knn_type andknn_source are given. Moreover, the number count and start\\\\nindex page of the results returned can be speciﬁed. By default, the ﬁrst 10 documents\\\\nare returned.\\\\n•Document: Returns the metadata, i.e. text and path, of a document with the speciﬁed\\\\nid. The URL to access this endpoint is /documents/<id> .\\\\n•PDF: Returns the PDF ﬁle. This endpoint is used to display the PDF in the detail\\\\ncomponent of the frontend. The URL to access this endpoint is /documents/<id>/pdf .\\\\n•WordCloud: Returns the bytes of a word cloud image. Depending on additional\\\\nparameters, the word cloud is either generated from one document or a group of\\\\nsimilar documents. If the knn_type is speciﬁed, a query for the count most similar\\\\ndocuments is performed. By default, count is 10. The URL to access this endpoint\\\\nis/documents/<id>/wordcloud .\\\\n•Term Frequency: Returns the term frequency calculated for the speciﬁed document.\\\\nTheURL to access this endpoint is /documents/<id>/term_frequency .\\\\n•TopicWordCloud: If term is speciﬁed this endpoint returns a word cloud of the\\\\nterms that describe the topics most similar to the query term. The parameter count\\\\nspeciﬁes the number of topics to be returned. Its default value is 3. The topics are\\\\ngenerated by Top2Vec . The URL to access this endpoint is /topics/wordcloud .\\\\n•Topics: Returns the topics generated by Top2Vec . The topics are described by the\\\\nwords closest to the topic vectors. The URL to access this endpoint is /topics .\\\\n4.2.2 Frontend\\\\nThe framework used for the frontend is Angular. There are three main components, which\\\\nare used to display the data:\\', \\'4O w na p p r o a c h 4 8\\\\n•Home: The home component is used to display the results of a text query. It consists\\\\nof a search bar, which is used to enter the query term, and a list of results. If no text\\\\nquery is entered the ﬁrst documents of the database, i.e. the result of a match_all\\\\nquery, are displayed. The search component is shown in Figure 4.12 .\\\\n•Detail: The detail component is used to display the details of a document. The\\\\ndocument name and ID are located on the left side of the screen. Beneath the\\\\ndocument name and ID, a button which opens the term frequency image on a new\\\\npage is located. Moreover, the word cloud of the document is displayed. The word\\\\ncloud is generated from the text of the document. On the right side of the screen,\\\\nthere is a PDF viewer which displays the pages of the document. Beneath the PDF\\\\nviewer, the ﬁle names and a word cloud of the most similar documents are displayed\\\\nafter a query for them is initiated by the user. The detail component is shown in\\\\nFigure 4.13 .\\\\n•Topic: The topic component is used to display the topics of the documents. The\\\\ntopics are lists of words generated by top2vec . The user can query for the most\\\\nsimilar topics to a term. The results are displayed as a word cloud. The upper limit\\\\nof the number of topics can be deﬁned by the user. The topic component is shown\\\\ninFigure 4.14 .\\\\nFigure 4.12 :Home component of the frontend. The search bar is used to enter the text\\\\nquery. The results of the query are displayed below the search bar.\\', \\'4 Own approach 49\\\\nFigure 4.13 :Detail component of the frontend. The chosen document is displayed, as\\\\nwell as its most similar documents in the database. The word clouds of the\\\\ndocument and the most similar documents are displayed.\\', \\'4O w na p p r o a c h 5 0\\\\nFigure 4.14 :Topic component of the frontend. The topics identiﬁed by Top2Vec are listed.\\\\nBelow them, the user can query for the most similar topics to a term. The\\\\nresults are displayed as a word cloud.\\\\n4.3 Trade-oﬀ between memory and query time\\\\nAt the beginning of this thesis, it was unclear to what degree the tool, i.e. the database\\\\nﬁelds and query results should be precomputed. A tool which is trained once oﬄine is\\\\nbeneﬁcial due to the amount and the nature of the data. The Bahamas leak is static and\\\\nthus, the database does not need to be updated with new documents.\\\\nIn the course of ﬁlling the database with information, one had to face obstacles not only\\\\nregarding excessive memory usage but also long run times of methods. Early on it became\\\\nevident that one either had to reduce accuracy and details in order to achieve less memory\\\\nor one had to settle for minutes to hours of calculations and bigger costs in terms of memory\\\\nconsumption.\\\\nBeforehand, it was not clear which information, i.e. ﬁelds in the database, seemed worth the\\\\ntime and memory. For instance, initially, the image of the ﬁrst PDF page of each document\\\\nwas saved alongside the other ﬁelds within the database. After scaling the amount of data\\\\nstored in the database to about 2900 documents, this approach caused severe issues in\\\\nterms of memory usage. Hence, this ﬁeld is omitted.\\', \\'5E v a l u a t i o n 5 1\\\\n5E v a l u a t i o n\\\\nSince the dataset has no ground truth, the procedure used to pick the parameter values\\\\nis not comparable to ground truth-based approaches. Hence, the evaluation is informal\\\\nand the methods applied have arisen from regular consultation with experts from the tax\\\\noﬃce. Run times of diﬀerent conﬁgurations are measured and compared. Parameters are\\\\nchosen with respect to model-speciﬁc procedures, such as reachability plots for OPTICS .\\\\nThe models are compared to each other and the tool constructed from the composition of\\\\nthe models is compared to the baseline topic analysis model Top2Vec .\\\\n5.1 Database\\\\nThere is a variety of parameter values to choose from when working with databases. The\\\\nchoice of the similarity metric is discussed ﬁrst. Secondly, the reasons for choosing Elas-\\\\nticsearch as a database are presented.\\\\nSimilarity measurements\\\\nAccording to Reimers and Gurevych , the similarity measurements discussed in Section 3.3\\\\nobtained roughly the same results in their experiments [ 58].\\\\nAs the similarity between vectors is usually calculated using some form of cosine similarity,\\\\nrather than Euclidean distance in literature, cosine similarity is preferable over Euclidean\\\\ndistance. Since the models may produce embeddings which are not normalized, the cosine\\\\nsimilarity is used instead of the dot product.\\\\nElasticsearch\\\\nAccording to Grinberg , Structured Query Language ( SQL) databases are a good choice\\\\nfor eﬃciently storing structured data. This is because their paradigm ACID , i.e. Atom-\\\\nicity, Consistency, Isolation, Durability, provides high reliability. Not only SQL ( NoSQL )\\\\ndatabases, on the other hand, are more ﬂexible and can be used to store unstructured\\\\ndata [ 25]. They do not require a predeﬁned schema and can therefore accept documents\\\\nof arbitrary structure [ 22]. Usually, NoSQL databases do not oﬀer services such as JOIN s.\\\\nNoSQL databases are said to outperform out-of-the-box SQL databases. Since the dataset\\\\nconsists of unstructured documents and the task at hand does not require performing any\\\\nJOIN s, aNoSQL database is favourable. Elasticsearch is chosen since it is well known to\\', \\'5E v a l u a t i o n 5 2\\\\nprovide near real-time search and to operate on big data. Subsequently, it is a good ﬁt for\\\\nthe underlying dataset.\\\\nSince Elasticsearch stores vectors of at most 2048 dimensions, the TF-IDF and InferSent\\\\nembeddings are problematic. Besides imposing limits to the dimensionality of the em-\\\\nbeddings, Elasticsearch oﬀers a variety of convenient functionalities, such as the built-in\\\\nkNN search. Therefore, in this work, Elasticsearch is used regardless of the dimensionality\\\\nconstraints imposed by the database. Hence, the techniques are adjusted to the database\\\\nand not vice versa.\\\\nThe time necessary to ﬁll the Elasticsearch database has been evaluated and improved\\\\nthroughout this work. The current time measurements are shown in Figure 5.1 . The\\\\ntimes correspond to calculation of 2048 embeddings. It is possible to measure this com-\\\\npuation time individually since the task of ﬁlling the database is modulized. Modulizing\\\\nis beneﬁcial since it is possible to update the embeddings without having to recreate the\\\\ndatabase. Moreover, it facilitates debugging and comparing the models used to create the\\\\nembeddings.\\\\nFigure 5.1 :Time per module of creating the Bahamas database using a random selection\\\\nof 2048 documents. The x-axis is logarithmic. The reference time is measured\\\\nusing cProfiler on a Apple M2 Pro MNW83D/A with 16 GBRAM and 12\\\\ncores.\\\\n5.2 Eigendocs\\\\nIn order to determine the optimal number of components used for Eigendocs the cumulative\\\\nexplained variance and the reconstruction error are plotted as displayed in Figure 3.15\\\\nfrom Subsection 3.5.2 . The ﬁrst plot indicates that 90% of the variance is explained by 441\\\\ncomponents. Usually, that would have been the number of dimensions of the subspace onto\\', \\'5E v a l u a t i o n 5 3\\\\nwhich the documents would have been projected. However, when working with clustering\\\\nalgorithms like OPTICS , the number of dimensions should be reduced even further to\\\\nachieve valid clusters. Therefore the reconstruction error with respect to diﬀerent numbers\\\\nof components is taken into consideration.\\\\n1sqr_dif =(X_test -X_test_pca_inverse) **2\\\\n2reconstr_err .append(np .sqrt(np .mean(sqr_dif)) /\\\\n3 (np.sum(np .abs( 1-X_test)) /X_test .shape[ 0]))\\\\nListing 5.1 :Adaption of the RSME : Firstly, the squared diﬀerences between the original\\\\nand the reconstructed images are calculated. Since the values are normalized,\\\\na 1 corresponds to a white pixel. Then, the absolute values of all non-white\\\\npixels of the test set are summed up. The average number of non-white pixels\\\\nis calculated by dividing the sum by the number of images in the test set. This\\\\napproach considers pixels of value p2[0,1]as(p·100) % white and thus, they\\\\nare incorporated in the sum.\\\\nUsually, a RSME is minimized to determine the optimal parameter conﬁgurations. In this\\\\ncase, the reconstruction error shall be interpreted. To facilitate the interpretability of the\\\\nreconstruction error, its calculation is adapted to incorporate the content of the images.\\\\nAt ﬁrst sight, the majority of image pixels are white, i.e. do not convey any information.\\\\nTherefore, the reconstruction error is divided by the average number of non-white pixels.\\\\nHence, the reconstruction error of an image is weighted by the amount of information it\\\\nconveys. The calculation is given in Listing 5.1. The result is displayed in Figure 3.15 .\\\\nSince the reconstruction error increases less rapidly after 10 to 20 components, the number\\\\nof components is set to 13, which has been an “elbow” point in a similar trial using a not\\\\nrandomly selected dataset of 195 images.\\\\nSome impressions of the Eigendocs algorithm are displayed in Figure 4.4 .A s s u m i n g t h a t\\\\nthe selection of documents is representative, the preprocessing of the documents using\\\\nEigendocs should have encoded information about the dimensionality of the images. How-\\\\never, this assumption is not valid since bigger images exist. Therefore, the idea of incor-\\\\nporating information about the image’s dimensions is not entirely implemented.\\\\n5.3 Embeddings\\\\nAs discussed in Subsection 4.1.3 , there is a range of possible parameter values to choose\\\\nfrom when implementing embedding models. The section below states which ﬁndings have\\\\nled to the parameter values applied in this work.\\', \\'5E v a l u a t i o n 5 4\\\\nTF-IDF\\\\nThe main obstacle to overcome is the high dimensionality of the TF-IDF embeddings.\\\\nHence, the goal of the parameter selection is to ﬁnd a way to reduce the dimensionality of\\\\nthe vocabulary to 2048 which is the maximum dense vector dimensionality of Elasticsearch.\\\\nHowever, the quality of the embeddings should not decline too much.\\\\nThe choice of the preprocessor is investigated with regard to the goal of minimizing the\\\\nvocabulary size. Both the default and a custom preprocessor are tested on a data corpus\\\\nof 2048 randomly selected documents concerning the vocabulary (size). While the default\\\\npreprocessor had a vocabulary size of 5893, the custom preprocessor had a size of 5585.\\\\nThe relative diﬀerences between vocabulary sizes seem to be inversely proportional to the\\\\ndataset size since the trend is already visible for two diﬀerent data corpus sizes in Table 5.1 .\\\\nThe custom preprocessor is chosen because it had a smaller vocabulary size. The diﬀerences\\\\nbetween both vocabularies are visualized in Figure 5.2 .\\\\nTable 5.1 :Comparison of vocabulary sizes resulting from the default and the custom\\\\nTF-IDF preprocessor on diﬀerent data corpus sizes.\\\\nﬁrst trial second trial\\\\ndocument corpus size M 195 2048\\\\ncustom preprocessor vocabulary size A 1521 5585\\\\ndefault preprocessor vocabulary size B 1641 5893\\\\n(B-A)/M 120/195 =\\\\n0,6153846154308/2048 =\\\\n0,150390625\\\\n(a)The terms only present in the vocabulary\\\\nproduced by the default preprocessor.\\\\n(b)The terms only present in the vocabulary\\\\nobtained from the custom preprocessor.\\\\nFigure 5.2 :The word clouds visualize which words are unique to both vocabularies on a\\\\nrandom selection of 2048 documents.\\\\nAs stated in Section 5.1 ,t h e TF-IDF embeddings can be problematic with regard to the\\\\ndimensionality limitations imposed by Elasticsearch. The parameters min_df andmax_df\\\\nare set to values which keep the vocabulary size small and thus, the dimensionality of the\\\\nembeddings is reasonably small. Furthermore, this work employs dimensionality reduction\\', \\'5E v a l u a t i o n 5 5\\\\ntechniques to reduce the dimensionality of the embeddings if the embeddings have a higher\\\\ndimensionality than 2048.\\\\nDoc2Vec\\\\nSince no labeled data is available, the evaluation of the Doc2Vec embeddings is limited.\\\\nTherefore, the Doc2Vec embeddings are evaluated by comparing them to other embeddings.\\\\nThe Doc2Vec model is not tuned in terms of hyperparameter selection, but the default\\\\nsettings are used since there is no way to evaluate the resulting embeddings.\\\\nInferSent\\\\nThe max pooling type is used for the InferSent model, since Conneau et al. found by\\\\nconducting experiments using diﬀerent pooling techniques that it is the best option [ 14].\\\\nInitially, in this work, the Global Vectors ( GloVe ) word embeddings were used for the\\\\nInferSent model. However, since the ﬁle of precomputed GloVe word embeddings has a\\\\nsize of 5.65 GBand thus, slows down the model, ultimately another word embedding is\\\\nused. The time necessary to compute and insert 195 documents for speciﬁc embeddings\\\\nis displayed in Figure 5.3 . The custom word embedding used in this work is a Word2Vec\\\\nmodel trained on a selection of 2048 randomly selected documents from the Bahamas\\\\ndataset.\\\\nPennington et al. state that GloVe outperforms Word2Vec on the same corpus, vocabulary\\\\nand window size in terms of quality and speed [ 55]. Hence, the quality of the results\\\\nobtained in this work may have suﬀered from using a custom Word2Vec instead of GloVe .\\\\nHowever, since the computation time of the project is a crucial factor, the custom Word2Vec\\\\nis used.\\\\nFigure 5.3 :Reference time necessary to calculate and insert 195 InferSent embeddings for\\\\ndiﬀerent precomputed word embeddings on a Apple M2 Pro MNW83D/A with\\\\n16GBRAM and 12 cores. Using a custom Word2Vec model is around 5.5\\\\ntimes faster than GloVe .\\', \\'5E v a l u a t i o n 5 6\\\\nUSE\\\\nSince there are no parameters to customize the evaluation of the USE embeddings is limited.\\\\nTherefore, the USE embeddings are evaluated by comparing them to other embeddings.\\\\nAE\\\\nIn order to determine which architecture for the hidden or so-called latent space of the AE\\\\nis the best option, diﬀerent architectures are tested and compared in terms of RSME and\\\\ncosine similarity. The RSME is calculated as given in Listing 5.2. The cosine similarity is\\\\ncalculated as given in Listing 5.3. Due to the fact that cosine similarity values are bound\\\\nby 0 and 1, they are easier to rank than metrics that can yield any real number. However,\\\\ncosine similarity is usually applied to calculate the angle between two vectors and thus,\\\\none has to be cautious when interpreting the results. For instance, the vectors (0,1)Tand\\\\n(0,2)Thave a cosine similarity of 1, even though they are not the same vectors. Since\\\\nanAEis supposed to reconstruct the input rather than return a dependent or related\\\\nvector, this metric should be combined with a traditional metric. The dataset used for the\\\\nevaluation is a selection of 195 documents from the Bahamas dataset.\\\\n1rsme =np.linalg .norm(inverse_embedding -embeddings)\\\\n2 /np.sqrt(embeddings .shape[ 0])\\\\nListing 5.2 :Computation of the RSME between the original and the reconstructed embed-\\\\nding.\\\\n1cos_sim =statistics .mean([np .dot(inverse_emb, embedding)\\\\n2 /(np.linalg .norm(inverse_emb) *np.linalg .norm(embedding))\\\\n3 for inverse_emb, embedding inzip(inverse_embedding, embeddings)])\\\\nListing 5.3 :Computation of the cosine similarity between the original and the recon-\\\\nstructed embedding.\\\\nThe scores of diﬀerent architectures are shown in Figure 5.4. The x-axis displays the\\\\nnumber of neurons in each layer for the respective experiments. The input space is 4096-\\\\ndimensional since that is the dimensionality of InferSent embeddings. The output of the\\\\nencoder is 2048-dimensional which is the maximum dimensionality supported by Elastic-\\\\nsearch for dense vectors. While most of the architectures produce similar results, one\\\\narchitecture stands out. Combining 2500-, 3000- and 3500-dimensional layers in the hid-\\\\nden space produces the worst RSME results. The smallest RSME and the biggest cosine\\\\nsimilarity are achieved by adding a 3500-dimensional layer in the hidden space. However,\\\\nthe results of the best architecture do not diﬀer greatly from the others.\\', \\'5E v a l u a t i o n 5 7\\\\nFigure 5.4 :The eﬀect of diﬀerent AEarchitectures on the reconstruction error. The error\\\\nis measured in terms of RSME (blue bars) and cosine similarity (yellow bars)\\\\nbetween the original and the reconstructed image. The smallest RSME and\\\\nthe biggest cosine similarity belong to the architecture best suited to this task\\\\nand are coloured green.\\', \\'5E v a l u a t i o n 5 8\\\\n5.4 Clustering using OPTICS\\\\n(a)Preprocessing according to item 1 (cf.\\\\n[5]).(b)Preprocessing according to item 2 (i.e.\\\\nEigendocs).\\\\nFigure 5.5 :The clusters are extracted from the respective reachability plots in Figure 4.9\\\\nbyOPTICS . The points in the three-dimensional space correspond to the\\\\nweights of the ﬁrst three principal components. The blue points are noise\\\\npoints, whereas any other colour denotes a cluster.\\\\nThe algorithm OPTICS is applied to data, which is preprocessed according to item 1\\\\n(cf. [ 5]) and item 2 (i.e. Eigendocs) from Subsection 4.1.4 . The clusters from Figure 5.5\\\\nare extracted from the respective reachability plots in Figure 4.9 . The three-dimensional\\\\nplots visualize the ﬁrst three dimensions of the data and thus, the weights of the ﬁrst\\\\nthree principal components assigned by the Eigendocs algorithm. By visual inspection and\\\\ncomparison of both plots, it can be seen that the projection by the Eigendocs approach of\\\\nitem 2 scatters the objects further in the three-dimensional space. One could argue that\\\\nthis is due to the fact, that the input data encodes not only the visual appearance in terms\\\\nof page layout but also the size of the document. Possibly, the objects are grouped by\\\\ndocument size.\\\\nTo analyze the results of the clustering, the content of the clusters is examined. Since\\\\nthe documents are not labeled, the content of the clusters is analyzed by visual inspection\\\\nand displayed in Figure 5.6 . The yellow images belong to the group identiﬁed as noise.\\\\nThe images preprocessed according to item 1 are partitioned into more clusters than the\\\\nEigendocs approach. Most of the certiﬁcates are classiﬁed as noise for both approaches in\\\\nthe trials carried out.\\\\nThe approach from item 1 (cf. [ 5]) omits information about the images’ original size. This\\\\ninformation is encoded in the Eigendocs approach. Hence, the preprocessing approach\\\\nchosen to create the OPTICS input for the Elasticsearch database index is Eigendocs.\\\\nAccording to Deng et al. ,OPTICS was developed to improve DBSCAN ﬂaws. With respect\\\\nto the evolution of these clustering methods, i.e. DBSCAN being OPTICS basis, DBSCAN\\', \\'5E v a l u a t i o n 5 9\\\\n(a)Preprocessing according to item 1 (cf.\\\\n[5]).\\\\n(b)Preprocessing according to item 2 (i.e.\\\\nEigendocs).\\\\nFigure 5.6 :In this visualization, at most 10 random elements of a cluster are displayed.\\\\nThe yellow images belong to the group denoted as noise. Most certiﬁcates are\\\\nclassiﬁed as noise.\\', \\'5E v a l u a t i o n 6 0\\\\nis chosen for the clustering method in Listing 4.11. In order to reduce calculation com-\\\\nplexity the maximum \"is 10. The distance between two points to still be considered\\\\nneighbours is deﬁned after a visual inspection of the reachability plot. Considering the\\\\nintrinsic structure of the Eigendocs data, it is set to 1.5to return meaningful clusters.\\', \\'5E v a l u a t i o n 6 1\\\\n5.5 Comparison of models\\\\nThis evaluation does not aim to ﬁnd the best model but to compare the similarity of the\\\\nmodels’ query results. It is a qualitative evaluation of a selection of documents from the\\\\nBahamas leak. This selection is a 2048 document corpus that is randomly chosen without\\\\nreplacement. A query deﬁnes a ﬁeld, i.e. embedding model, and a query document. The\\\\nquery response consists of the documents that are considered most similar to the query\\\\ndocument in terms of cosine similarity.\\\\nThe diﬀerences between the models are illustrated by visualizing the ﬁrst ﬁve response\\\\ndocuments of a sample query. The text of the query document is encoded using the\\\\nrespective model. A kNN query is used to obtain the results from the local database\\\\ncontaining 2048 documents. The query results are displayed in Figure 5.7 . The query\\\\ndocument, i.e. the image surrounded by a border, is omitted from the query response. The\\\\ndocuments in the query response are listed according to descending similarity to the query\\\\ndocument. All models except Doc2Vec andUSE returned only documents of CREDIT\\\\nSUISSE . Apart from this diﬀerence, the query responses of the models are very similar.\\\\nTo further investigate the diﬀerences between the models, the query responses of the models\\\\nare compared qualitatively on query documents that are considered unusual in terms of\\\\ntheir appearance. The document in Figure 5.8 is a table consisting of little text compared\\\\nto other samples from the data corpus. The document in Figure 5.9 is mostly handwritten,\\\\nwhich is unusual since most other samples are computer-generated.\\\\nThe models produced good query responses on a query document consisting of little text as\\\\nshown in Figure 5.8 . Even though at ﬁrst glimpse, the response documents seem unrelated\\\\nto the query document, they share multiple words, such as director . Similar response\\\\ndocuments do not have to be of similar visual appearance since the text embeddings only\\\\nconsider information from the text layer.\\\\nThere are query documents such as the one in Figure 5.9 that reveal the dissimilarity\\\\nbetween certain models. Most models’ response documents are similar to Figure 5.9(b) .\\\\nThese response documents depict the same type of document, i.e. handwritten receipts for\\\\nan annual fee. The TF-IDF model, however, returns diﬀerent response documents. The\\\\nresponse documents from Figure 5.9(a) are not handwritten and cover diﬀerent content,\\\\ni.e. requesting payment and three documents concerning an address change. TF-IDF ’s\\\\nresults could thus be considered to be of poor quality.\\\\nSince not only textual information but also visual information is encoded in the database,\\\\nthe next step is to compare the query responses of approaches that consider visual similar-\\\\nity. The query responses are clustered using OPTICS orargmax of the PCA compression.\\\\nThe ﬁrst exemplary query document in Figure 5.10 is an image of a usual document.\\\\nBoth clustering approaches yield similar results. More speciﬁcally, the responses share two\\\\ndocuments. Moreover, the last document in the response of both approaches diﬀers most\\', \\'5E v a l u a t i o n 6 2\\\\n(a)Doc2Vec\\\\n(b)SBERT\\\\n(c)InferSent\\\\n(d)TF-IDF\\\\n(e)USE\\\\nFigure 5.7 :Exemplary query response for diﬀerent embeddings on the same query docu-\\\\nment.\\\\nFigure 5.8 :InferSent query responses on a query document consisting of little text.\\', \\'5E v a l u a t i o n 6 3\\\\n(a)TF-IDF\\\\n(b)SBERT\\\\nFigure 5.9 :Qualitative comparison of query responses. The majority of the query docu-\\\\nment consists of handwritten text. The results of the TF-IDF model are not\\\\nsimilar to the query document. The other models, including SBERT , produce\\\\nresults that are more similar to the query document.\\\\nfrom the group. Most documents have a similar visual appearance, i.e. they have a stamp.\\\\nNone of the result documents originate from the same company as the query document.\\\\nThe second exemplary query document in Figure 5.11 is a certiﬁcate. The OPTICS clus-\\\\ntering approach yields a response that is more similar to the query document than the\\\\nargmax of the PCA compression because all its responses are from the same document\\\\ntype as the query document. Hence, the OPTICS clustering approach is considered to be\\\\nsuperior to the argmax approach.\\\\n(a)OPTICS\\\\n(b)argmax ofPCA compression\\\\nFigure 5.10 :Qualitative comparison of query responses. The response documents are clus-\\\\ntered using OPTICS orargmax of the PCA compression. They are not com-\\\\npared in terms of textual but visual similarity.\\\\nAnother approach to compare the response documents is to visualize the intersection of\\\\nthe query results. The Venn diagram is chosen since it displays intersections of all items of\\', \\'5E v a l u a t i o n 6 4\\\\n(a)OPTICS\\\\n(b)argmax ofPCA compression\\\\nFigure 5.11 :Qualitative comparison of query responses. The response documents are clus-\\\\ntered using OPTICS orargmax of the PCA compression. They are not com-\\\\npared in terms of textual but visual similarity. The query document is a\\\\ncertiﬁcate.\\\\nthe power set of the models. To build a Venn diagram, the number of documents that are\\\\nshared between the query results of several models is computed. First, all query responses\\\\nof a model irrespective of the query document are saved in a set. The cardinalities of\\\\nthe intersections of multiple sets are displayed in Figure 5.12 . Since ﬁve models encode\\\\ntextual information, the Venn diagram consists of ﬁve circles. One should be cautious\\\\nwhen interpreting the layout of the Venn diagram since the cardinality of an intersection\\\\nof documents does not correlate with its area in this visualization.\\\\nThe Venn diagrams in Figure 5.12 display the total number of shared response documents\\\\nfor 10 queries and 3, 5 or 10 response documents respectively. The models produce rather\\\\ndissimilar query results. Any combination of more than two models among the green\\\\n(Doc2Vec ), orange ( SBERT ), red ( TF-IDF )a n db l u e( USE) models seem to produce rather\\\\ndissimilar results as the number in the respective areas is close to zero across all Venn\\\\ndiagrams.\\\\nSince the Venn diagrams compare the responses of the models irrespective of the query\\\\ndocument, another approach is to compare the query results of the models for each query\\\\ndocument individually. This approach ﬁrst constructs a matrix of the number of shared\\\\nquery results between all model pairs summed up over all query documents. The matrix\\\\nconsists of ﬁve rows and ﬁve columns, where each row and column represents a model.\\\\nThe cell values are the number of shared query results between the models of the row\\\\nand column. It is possible to normalize the matrix to obtain the portion of shared query\\\\nresults. If two models produce the same query results, the cell value is either the total\\\\nnumber of query results or 1 if the matrix is normalized. Since the matrix is symmetric,\\\\nonly the upper triangular matrix is computed and the other half is mirrored. The matrix\\\\nis visualized using a heatmap as displayed in Figure 5.13 . The code snippet in Listing 5.4\\\\nshows the calculation of the similarity matrix.\\', \\'5E v a l u a t i o n 6 5\\\\n(a)3r e s p o n s ed o c u m e n t sp e rq u e r y .(b)5r e s p o n s ed o c u m e n t sp e rq u e r y .\\\\n(c)10 response documents per query.\\\\nFigure 5.12 :10 documents are randomly sampled from a 2048 document corpus. For each\\\\nsampled document and model, a kNN query is conducted. The respective\\\\nresponse documents excluding the query document are saved. The cardinality\\\\nof the intersection of all response documents irrespective of query document\\\\nfor diﬀerent models is visualized in terms of Venn diagrams.\\\\n1sim_matr =np.matrix(np .zeros(( len(model_names), len(model_names))))\\\\n2for idindf.index:\\\\n3 for i, model inenumerate (model_names):\\\\n4 for jinrange (i, len(model_names)):\\\\n5 sim_matr[i, j] +=np.sum([df .loc[ id,\\\\n6 model_names[j]] .count(item) for item indf.loc[ id,m o d e l ] ] )\\\\n7 sim_matr[j, i] =sim_matr[i, j]\\\\n8ifnormalize:\\\\n9 sim_matr /=np.array( len(df.index) *len(df.iloc[ 0,0]))\\\\nListing 5.4 :Calculation of the similarity matrix used to produce the heatmap.\\', \\'5E v a l u a t i o n 6 6\\\\nFigure 5.13 :Heatmap visualizing portion of shared query results on 2000 queries with 10\\\\nresponses each on a 2048 document corpus.\\\\nThe heatmap in Figure 5.13 shows that the models yield dissimilar query responses. USE\\\\nand InferSent produce the most similar responses, which indicates the maximum value of\\\\nshared query results. However, the maximum is less than 0.5and thus, rather dissimilar.\\\\nAny two-element combination of Doc2Vec ,TF-IDF andSBERT produces the most dis-\\\\nsimilar query results, namely only around 25% of shared response documents per query.\\\\nSince the normalization does not consider the distribution of the cardinalities of the inter-\\\\nsections of the query results among the models, another approach is to calculate the mean\\\\nand standard deviation of the cardinalities of the intersections of the query results. There-\\\\nfore, 30 trials are conducted. Each trial consists of 10 randomly sampled query documents\\\\nwith 10 responses each (excluding the query document in the response). The normalized\\\\nsimilarity matrix for each model pair concerning one trial is calculated using Listing 5.4.\\\\nThe cardinalities are stored in a dictionary of lists indexed by the respective model pairs.\\\\nFinally, the mean and standard deviation are computed for each model pair and stored in a\\\\nComma Separated Values ( CSV) ﬁle. The results are displayed in Table 5.2 . The standard\\\\ndeviation does not exceed 0.1for any model pair and is similar among all combinations.\\\\nAs observed before, any two-element combination of Doc2Vec ,TF-IDF andSBERT has\\\\nthe lowest ( 18\\\\x0020%) mean portion of shared query results.\\', \\'5E v a l u a t i o n 6 7\\\\nTable 5.2 :Mean and standard deviation of the average portion of shared response docu-\\\\nments of diﬀerent models on a 2048 document corpus. One trial produced ﬁve\\\\nreal values, i.e. one portion per model. A portion is obtained from 10 randomly\\\\nsampled query documents with 10 responses each (excluding the query docu-\\\\nment in the response). The sample selection is based on the same dataset for\\\\neach trial and thus, query documents can be selected for multiple trials. There\\\\nwere 30 trials.\\\\nmodel 1 model 2 mean std\\\\nDoc2Vec USE 0.26 0.09\\\\nDoc2Vec InferSent 0.26 0.07\\\\nDoc2Vec TF-IDF 0.2 0.08\\\\nDoc2Vec SBERT 0.18 0.06\\\\nUSE InferSent 0.33 0.08\\\\nUSE TF-IDF 0.25 0.08\\\\nUSE SBERT 0.25 0.06\\\\nInferSent TF-IDF 0.26 0.08\\\\nInferSent SBERT 0.24 0.06\\\\nTF-IDF SBERT 0.18 0.05\\\\n5.6 Comparison with baseline topic analysis approach\\\\nThe baseline topic analysis Top2Vec oﬀers a variety of built-in functionalities to the user.\\\\nIt is possible to retrieve human interpretable inherent topics of a set of documents, as well\\\\nas the topics most similar to certain search terms and word clouds of these results. Hence,\\\\nthis library meets the needs articulated by this work.\\\\nOpposed to Top2Vec , this thesis proposes a composite of diﬀerent approaches to encoding\\\\nvisual and semantic information and query for them using a database. To be more speciﬁc,\\\\nthis thesis not only relies on one semantic embedding model but oﬀers several techniques\\\\nand an approach to incorporate visual information.\\\\nMoreover, the tool implemented in this thesis can display the term frequency of the doc-\\\\nument chosen in the detail component. The Top2Vec library does not oﬀer a comparable\\\\nservice.\\\\nHowever, it is not possible to query for topics of the corpus which best describe a search\\\\nterm. Alternatively, one can perform a fuzzy text search on the documents. The user can\\\\ninspect the PDF of a document upon clicking on its name in the list of documents. The\\\\ndetail view enables the investigation of similar documents in terms of diﬀerent embedding\\\\napproaches.\\\\nDue to Top2Vec ’s architecture, documents and words are mapped into the same VSM .\\\\nHence, the topic vector deﬁnition and representation by its closest words are more mean-\\\\ningful than the approach of the thesis. In this thesis, a topic is represented by frequent\\\\nwords in the set of documents that are not necessarily meaningful.\\', \\'\\', \\'6 Conclusion 69\\\\n6C o n c l u s i o n\\\\nTo conclude this thesis, the research questions are revised. The insights acquired by ex-\\\\nploring diﬀerent techniques with the goal of the exploration of large unstructured text data\\\\nare discussed in Section 6.1 . Then, Section 6.2 points out the scientiﬁc contributions.\\\\n6.1 Discussion\\\\nRQ1 seeks to discover whether visual embedding methods are suitable for the task of\\\\nﬁnding similar documents in large unstructured text corpora.\\\\nIn this work, ﬁrstly, the images are preprocessed using the Eigendocs approach as discussed\\\\ninSubsection 4.1.4 andSection 5.4 . Then, the numerical data obtained from preprocessing\\\\nthe images is then reduced using PCA . Determining the number of components to be used\\\\nforPCA in a meaningful scientiﬁc way was problematic (cf. Subsection 3.5.2 &Section 5.2 ):\\\\nThe cumulative explained variance did not indicate a small number of components to use.\\\\nThe RSME calculation was found to be unsuitable for the dataset since multiple random\\\\nselections of documents from the dataset did not indicate a clear “elbow” point. The\\\\ncompressed images are clustered using the OPTICS algorithm and the argmax approach\\\\n(cf.Section 5.5 ).\\\\nIn general, the query responses from Section 5.5 which are based on visual information\\\\nconsist of visually similar documents. Hence, in terms of qualitative evaluation, the visual\\\\nrepresentation is considered to be valuable means to ﬁnd visually similar documents in\\\\nlarge unstructured corpora. Consequently, the answer to RQ1 is positive but acknowledges\\\\nthe lack of scientiﬁc justiﬁcation for the proof of concept’s conﬁguration.\\\\nRQ2 aims to ﬁnd out whether diﬀerent embedding methods produce similar results.\\\\nWhen comparing diﬀerent semantic embedding methods in Section 5.5 , slight diﬀerences\\\\nbetween the models become evident. TF-IDF ,Doc2Vec andSBERT are most dissimilar\\\\nto each other. The TF-IDF approach performs rather poorly on unusual query documents\\\\nsuch as handwritten ones.\\\\nThe distinct response documents and their order are diﬀerent for the same query document\\\\nregarding diﬀerent semantic embeddings. Diﬀerent semantic embeddings can yield response\\\\ndocuments containing the same company name. While the semantic responses’ contents\\\\nare considered similar to the query document and each other, the visual responses are more\\\\ndissimilar from the query document and each other.\\', \\'6 Conclusion 70\\\\nTo answer RQ2, the content and visual appearance of the response documents of diﬀerent\\\\nembedding techniques is similar, but the actual documents in the response sets diﬀer.\\\\nRQ3 poses the question of how the results of the system are presented to experts. The\\\\nidea of the representation is to enable users to explore the data corpus. They should be\\\\nable to query for terms, to ﬁnd similar documents in the text corpus and to derive the\\\\ninherent topics of the documents. The proof of concept is the implementation of a web\\\\ninterface which oﬀers these services. The web interface is introduced in Section 4.2 .\\\\nRQ4 addresses the evaluation of the performance of the system. In this work, the sys-\\\\ntem is evaluated with respect to multiple parameters. Section 5.1 discusses the time to\\\\ncompute the diﬀerent embeddings in Figure 5.1 .I n Section 5.3 , some embedding mod-\\\\nels are evaluated with respect to their time consumption for diﬀerent conﬁgurations (cf.\\\\nFigure 5.3 ).\\\\nAnother way to evaluate the performance of the system is to compare the results of diﬀerent\\\\nmodels via the intersection of their response sets (cf. Section 5.5 ). These intersections can\\\\nbe visualized with Venn diagrams ( Figure 5.12 ) and heatmaps ( Figure 5.13 ). To ensure\\\\nthe results are not random, the statistical properties of the response sets are calculated\\\\nand presented in Table 5.2 .\\\\nAdditional ﬁndings which were obtained in the course of working on this thesis are\\\\npresented in the following.\\\\nThe InferSent and the TF-IDF model produce embeddings of large dimensionalities. Since\\\\nchanging the dimensionality would require retraining the models or risking the loss of\\\\nquality, the dimensionality is not changed in this work.\\\\nThe database Elasticsearch was chosen because it oﬀers built-in functionalities, such as\\\\nkNN and fuzzy text search. It is well-suited for ﬂexible data since it is possible to insert\\\\nincomplete documents into the database. However, the maximum dimensionality of the\\\\nembeddings is limited to 2048. If the task at hand requires higher dimensional embed-\\\\ndings, such as the ones produced by the InferSent model, another database may be more\\\\nsuitable.\\', \\'6 Conclusion 71\\\\n6.2 Contribution\\\\nIn the context of this thesis, several MLtechniques to derive semantic and visual informa-\\\\ntion from unstructured data are explored.\\\\nIn order to ﬁnd relevant documents in a text corpus, the corpus is made searchable. This\\\\nis achieved by constructing a pipeline that preprocesses the documents and stores them\\\\nin a database in an oﬄine fashion. The local Elasticsearch database stores 2048 randomly\\\\nchosen documents from the whole dataset, whereas the database on the IES server stores\\\\naround 497504 incomplete documents. Owing to Elasticsearch’s implementation, (fuzzy)\\\\ntext queries and kNN search queries can be conducted with minimal latency (cf. Subsec-\\\\ntion 3.7.1 ).\\\\nConcerning RQ1, the visual embedding method Eigendocs is implemented. It is an adap-\\\\ntion of the prevalent Eigenfaces approach to the task of ﬁnding similar documents. The idea\\\\nof projecting items into a lower dimensional space is kept. The preprocessing is extended\\\\nby placing the document images onto a white canvas as described in Subsection 4.1.2 .\\\\nThe semantic embedding methods TF-IDF ,Doc2Vec , InferSent, USE and SBERT are\\\\nexplored. The conﬁgurations of the models are altered to reduce their runtime. Since\\\\nthe dimensionalities of TF-IDF and InferSent embeddings are too big to be stored in an\\\\nElasticsearch database, the dimensionality of the embeddings is reduced by the encoder of\\\\nanAE(cf.Section 4.1.3 ).\\\\nIn the matter of RQ3, a web interface is implemented, which provides the possibility to\\\\nconduct text queries and allows to examine a document of interest in more detail (cf.\\\\nSection 4.2 ): The detail component not only contains a PDF viewer, word clouds of the\\\\nmost frequent words in the document or query response, but also an option to display the\\\\nﬁle names of the response documents for diﬀerent embeddings. The tool implemented in\\\\nthis work is not designed for a productive environment since the focus is on the comparison\\\\nof diﬀerent models rather than usability.\\\\nSince the dataset is not labeled, the evaluation of the results is not trivial. Therefore, with\\\\nregard to RQ2 andRQ4 multiple evaluation methods are implemented (cf. Section 5.5 ).\\\\nThe ﬁrst method is a Venn diagram that depicts the intersection of the query responses\\\\nof the power set of diﬀerent embedding models. The second method is a heatmap that\\\\nillustrates the average portion of shared response documents between diﬀerent embedding\\\\nmodels. Moreover, the mean and standard deviation of the portion of shared response\\\\ndocuments are calculated to further investigate the distribution of the results obtained\\\\nabove. Furthermore, the time consumption of the computation of embeddings for diﬀerent\\\\nmodels is evaluated (cf. Section 5.1 &Section 5.3 ). Lastly, in Subsection 3.4.1 ,t h et o o li s\\\\ncompared to a baseline topic analysis approach called Top2Vec .\\', \\'\\', \\'7O u t l o o k 73\\\\n7O u t l o o k\\\\nWhen investigating both semantic and visual embedding methods, diﬀerences between the\\\\nmodels became evident. Overall, the textual embedding methods produced more meaning-\\\\nful responses than the visual embedding methods. However, this is not surprising since the\\\\ntextual embedding methods prioritize documents containing equal or semantically similar\\\\nterms and thus, return documents of similar content or originating from the same company\\\\nas the query document. Visual embedding methods, on the other hand, return visually\\\\nsimilar documents.\\\\nIt is complicated to compare the responses of semantic and visual embedding methods since\\\\nthey operate on fundamentally diﬀerent data. A more thorough evaluation could include\\\\na survey. A selection of the results of this work is incorporated into the ﬁrst approach\\\\nto constructing a survey. 13 people with diﬀerent academic backgrounds have partaken\\\\nin the survey. A sample question and an illustrative result from the survey are displayed\\\\ninFigure 7.1 . However, constructing a survey is complicated since semantic similarities\\\\nshould be evaluated on a textual level, i.e. content, which is diﬃcult for non-experts and\\\\nnot natural since humans are prone to assess similarities by visual inspection. Moreover,\\\\nidentiﬁcation of the target audience is diﬃcult since the target audience of the tool could\\\\nbe expanded to be more general than the tax oﬃce.\\\\n(a)Aq u e s t i o no ft h es u r v e y .\\\\n (b)Selection of results of survey.\\\\nFigure 7.1 :A ﬁrst survey approach from [ 30].\\', \\'7O u t l o o k 74\\\\nSimilar to Pennington et al. ’s work, in this thesis, for many models used, any unspeci-\\\\nﬁed parameters are set to their default values, assuming that they are close to optimal\\\\nacknowledging that this simpliﬁcation should be revised in a more thorough analysis.\\\\nThe TF-IDF approach performs rather poorly on unusual query documents. There are\\\\nmultiple factors that could have contributed to this result. Firstly, the vocabulary is dras-\\\\ntically reduced to satisfy the database’s constraints concerning dense vector dimensionality.\\\\nThus, TF-IDF may either be unsuitable for the task of ﬁnding similar documents when\\\\nthe vocabulary size is restricted or further research is required to ﬁnd more suitable means\\\\nto compress the embedding before inserting it into the database. Secondly, the evaluation\\\\nof the diﬀerent preprocessors of TF-IDF is carried out on small datasets consisting of 195\\\\nand 2048 documents. This dataset may not be representative of the whole corpus.\\\\nIn this work, the precomputed GloVe embeddings are replaced by a custom Word2Vec\\\\nmodel. However, Pennington et al. state that GloVe outperforms Word2Vec on the same\\\\ncorpus, vocabulary and window size in terms of quality [ 55]. Hence, the quality of InferSent\\\\nmight have deteriorated due to the replacement of GloVe byWord2Vec .\\\\nWhen preprocessing the document images in the Eigendocs approach, the images are placed\\\\non a white canvas assuming its dimensions are bigger or equal to all other documents in the\\\\ncorpus. Since this assumption was not true, the images selected to ﬁnd the dimensionalities\\\\nof the canvas were not representative. Future work should include a more thorough analysis\\\\nof the maximal image sizes in the corpus.\\\\nThe parameter selection for PCA is not representative of the whole dataset, due to the\\\\nfact that the dataset used for calculating the reconstruction error is too small. Moreover,\\\\nthe resulting plot is not optimal for conducting the “elbow method”, since no signiﬁcant\\\\nchange in the slope is evident.\\\\nDiﬀerent AEarchitectures are experimentally evaluated on a selection of 195 documents.\\\\nSince the dataset is too small and not drawn randomly from the whole data corpus the re-\\\\nsults are not representative. Thus, future work should include a more elaborate evaluation\\\\nof diﬀerent AEarchitectures on a bigger document corpus.\\\\nThe comparison of the diﬀerent embedding methods in terms of query response similarity\\\\nwas carried out on the data which was stored in the database. For future work, the\\\\ncomparison should be carried out on a separate dataset to evaluate the performance of the\\\\nmodels on unseen data.\\\\nThe evaluation of the similarity between query results of diﬀerent models so far has not\\\\nconsidered the individual weights for respective query responses because it was diﬃcult\\\\nto ﬁnd means to interpret and visualize semantic meaningful weight relationships. Hence,\\\\nfuture work could include the weights of the query responses in the evaluation.\\\\nMoreover, the similarity of the query documents is not considered in the evaluation. To\\\\nfurther improve the evaluation, the number of occurrences of query documents in the\\', \\'7O u t l o o k 75\\\\nresponse documents of other queries could be examined. Another approach to evaluation\\\\ncould be to assess the quality of the images which were returned by multiple models.\\\\nPossibly, one could create a hypothesis about whether better responses correlate with the\\\\nnumber of models that returned them.\\\\nThe elastic stack oﬀers a wide range of tools, for instance, Kibana that can be used to\\\\nmanage models and to create ingest pipelines to embed new documents. If models are\\\\nmanaged by Kibana, the models no longer have to be managed by the user and thus, the\\\\nsystem would most likely be more user-friendly and less prone to errors.\\\\nAnother issue is the fact that the database contains neither all embeddings nor all doc-\\\\numents. The Bahamas leak contains 38 GBof data. Even though multiprocessing using\\\\nPool is used to split the workload across up to 100 processes, the embedding process is not\\\\nﬁnished after several days. Hence, more advanced coding techniques have to be applied to\\\\nspeed up the embedding process.\\\\nThe domain of ﬁnancial fraud and tax evasion is very interesting. Thus, future work\\\\ncould include the development of a working system for the tax oﬃce based on the system\\\\nimplemented in this thesis. The techniques explored in this work could be used to ﬁnd\\\\nsimilar documents to a query document and thus, facilitate initial exploration of a large\\\\ndata corpus.\\', \\'Bibliography xi\\\\nBibliography\\\\n[1]Slurm: Quick start user guide. URL https://slurm.schedmd.com/quickstart.html .\\\\n[Accessed 16.09.2023].\\\\n[2]K.P. Agrawal, Sanjay Garg, Shashikant Sharma, and Pinkal Patel. Development and\\\\nvalidation of optics based spatio-temporal clustering technique. Information Sciences ,\\\\n369:388–401, 2016.\\\\n[3]Rubayyi Alghamdi and Khalid Alfalqi. A survey of topic modeling in text mining.\\\\nInternational Journal of Advanced Computer Science and Applications ,6 : 1 4 7 – 1 5 3 ,\\\\n2015.\\\\n[4]Dimo Angelov. Top2vec: Distributed representations of topics. arXiv:2008.09470 ,\\\\n2020.\\\\n[5]Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. Optics:\\\\nOrdering points to identify the clustering structure. SIGMOD Rec. ,2 8 : 4 9 – 6 0 ,1 9 9 9 .\\\\n[6]Farzana Anowar, Samira Sadaoui, and Bassant Selim. Conceptual and empirical\\\\ncomparison of dimensionality reduction algorithms (pca, kpca, lda, mds, svd, lle,\\\\nisomap, le, ica, t-sne). Computer Science Review ,4 0 ( 1 0 0 3 7 0 ) : 1 – 1 3 ,2 0 2 1 .\\\\n[7]Fankar Armash Aslam, Hawa Nabeel Mohammed, Jummal Musab Mohd. Munir, and\\\\nMurade Aaraf Gulamgaus. Eﬃcient way of web development using python and ﬂask.\\\\nInternational Journal of Advanced Research in Computer Science ,6 : 5 4 – 5 7 ,2 0 1 5 .\\\\n[8]Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with\\\\nPython . O’Reilly Media, Sebastopol, CA, USA, 1st edition, 2009.\\\\n[9]Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John,\\\\nNoah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung,\\\\nBrian Strope, and Ray Kurzweil. Universal sentence encoder. arXiv:1803.11175 , pages\\\\n1–7, 2018.\\\\n[10]Allison Chaney and David Blei. Visualizing topic models. International AAAI Con\\\\uffff\\\\nference on Web and Social Media ,6 : 4 1 9 – 4 2 2 ,2 0 2 1 .\\\\n[11]Delphine Charlet and Géraldine Damnati. Simbow at semeval-2017 task 3: Soft-cosine\\\\nsemantic similarity between questions for community question answering. Orange\\\\nLabs, pages 315–319, 2017.\\', \\'Bibliography xii\\\\n[12]Qiuxing Chen, Lixiu Yao, and Jie Yang. Short text classiﬁcation based on lda\\\\ntopic model. In International Conference on Audio, Language and Image Process\\\\uffff\\\\ning (ICALIP) , pages 749–753, 2016.\\\\n[13]Rob Churchill and Lisa Singh. The evolution of topic modeling. ACM Comput. Surv. ,\\\\n54:1–35, 2022.\\\\n[14]Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes.\\\\nSupervised learning of universal sentence representations from natural language infer-\\\\nence data. arXiv:1705.02364 ,2 0 1 8 .\\\\n[15]Matt Copperwaite and Charles Leifer. Learning Flask Framework .P a c k tP u b l i s h i n g ,\\\\n2015.\\\\n[16]Laura Dayton, Dante Rousseve, Neil Sehgal, and Sindura Sriram. Final project report:\\\\nMethods of facial recognition. CSCI ,2 0 2 0 .\\\\n[17]Z. Deng, Y. Hu, M. Zhu, and et al. A scalable and fast optics for clustering trajectory\\\\nbig data. Cluster Computing ,1 8 : 5 4 9 – – 5 6 2 ,2 0 1 4 .\\\\n[18]download-infersent. Infersent. URL https://github.com/facebookresearch/\\\\nInferSent . [Accessed 14.11.2023].\\\\n[19]Elasticsearch-guide. Elasticsearch guide. URL https://www.elastic.co/guide/en/\\\\nelasticsearch/reference/current/index.html . [Accessed 15.09.2023].\\\\n[20]Elasticsearch-kNN-embedding. How to deploy a text embedding model and use it\\\\nfor semantic search. URL https://www.elastic.co/guide/en/machine-learning/\\\\n8.10/ml-nlp-text-emb-vector-search-example.html . [Accessed 15.09.2023].\\\\n[21]Fabio. Deep averaging network.ipynb. URL https://github.com/f0bs/\\\\nMachine_Learning/blob/master/Deep%20Averaging%20Network.ipynb4 . [Accessed\\\\n04.10.2023].\\\\n[22]Daniel Gaspar and Jack Stouﬀer. Mastering Flask Web Development: Build Enter\\\\uffff\\\\nprise-Grade, Scalable Python Web Applications ,v o l u m e2 .P a c k tP u b l i s h i n g ,2 0 1 8 .\\\\n[23]gensim-doc2vec-init. gensim.models.doc2vec. URL https://tedboy.github.io/\\\\nnlps/generated/generated/gensim.models.Doc2Vec.__init__.html . [Accessed\\\\n01.10.2023].\\\\n[24]gensim-word2vec-init. Word2vec embeddings. URL https://radimrehurek.com/\\\\ngensim/models/word2vec.html#gensim.models.word2vec.Word2Vec . [Accessed\\\\n01.10.2023].\\\\n[25]Miguel Grinberg. Flask Web Development: Developing Web Applications with Python .\\\\nO’Reilly Media, Inc., 2018.\\', \\'Bibliography xiii\\\\n[26]Maarten Grootendorst. Bertopic: Neural topic modeling with a class-based tf-idf\\\\nprocedure. arXiv:2203.05794 ,2 0 2 2 .\\\\n[27]Machine Learning Group. Credit card fraud detection, 2017. URL https://www.\\\\nkaggle.com/datasets/mlg-ulb/creditcardfraud . [Accessed 18.11.2023].\\\\n[28]Stanford NLP Group. The stanford natural language inference (snli) corpus. URL\\\\nhttps://nlp.stanford.edu/projects/snli/ . [Accessed 18.12.2023].\\\\n[29]Christian M. Gruhl. Novelty Detection for Multivariate Data Streams with Probalistic\\\\nModels . Kassel university press, 2022.\\\\n[30]Klara M. Gutekunst. Empirische bewertung von suchanfragen. URL https://forms.\\\\ngle/EU8UUxnWc7hWBngj8 . [Accessed 20.11.2023].\\\\n[31]Klara M. Gutekunst. Identifying ﬁscal fraud with anomaly detection techniques. Tech-\\\\nnical report, University of Kassel, 2023. URL https://github.com/KlaraGtknst/\\\\nidentifying-fiscal-fraud .\\\\n[32]Tom Hanika. Artiﬁcial intelligence. Technical report, 2023. Lecture script.\\\\n[33]impl-src-ae. Image compression using autoencoders in keras. URL https://blog.\\\\npaperspace.com/autoencoder-image-compression-keras/ . [Accessed 06.11.2023].\\\\n[34]Dan Jurafsky and James H. Martin. Speech and Language Processing ,v o l u m e3 .2 0 2 3 .\\\\n[35]Hari Krishna Kanagala and V.V. Jaya Rama Krishnaiah. A comparative study of k-\\\\nmeans, dbscan and optics. In International Conference on Computer Communication\\\\nand Informatics (ICCCI) , pages 1–6, 2016.\\\\n[36]Pooja Kherwa and Poonam Bansal. Topic modeling: A comprehensive review. EAI\\\\nEndorsed Transactions on Scalable Information Systems ,7 : 1 – 1 6 ,2 0 1 9 .\\\\n[37]Gunjan Khosla, Navin Rajpal, and Jasvinder Singh. Evaluation of euclidean and man-\\\\nhanttan metrics in content based image retrieval system. In International Conference\\\\non Computing for Sustainable Global Development (INDIACom) , pages 12–18, 2015.\\\\n[38]Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. From word\\\\nembeddings to document distances. PMLR ,3 7 : 9 5 7 – 9 6 6 ,2 0 1 5 .\\\\n[39]Tzu-Hsuan Lin and Jehn-Ruey Jiang. Credit card fraud detection with autoencoder\\\\nand probabilistic random forest. Mathematics ,9 : 2 6 8 3 – 2 6 9 9 ,2 0 2 1 .\\\\n[40]Tzu-Hsuan Lin and Jehn-Ruey Jiang. Credit card fraud detection with autoencoder\\\\nand probabilistic random forest. Mathematics ,9 ( 2 1 ) ,2 0 2 1 .U R L https://www.mdpi.\\\\ncom/2227-7390/9/21/2683 .\\\\n[41]Fredrik Lundh, Jeﬀrey A. Clark, and Contributors. The image class. URL\\\\nhttps://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.\\\\nImage.convert . [Accessed 14.12.2023].\\', \\'Bibliography xiv\\\\n[42]Yu. A. Malkov and D. A. Yashunin. Eﬃcient and robust approximate nearest neighbor\\\\nsearch using hierarchical navigable small world graphs. arXiv:1603.09320 , pages 1–13,\\\\n2018.\\\\n[43]Tomas Mikolov and Quoc Le. Distributed representations of sentences and documents.\\\\nJMLR ,3 2 : 1 – 9 ,2 0 1 4 .\\\\n[44]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of\\\\nword representations in vector space. arXiv:1301.3781 , pages 1–12, 2013.\\\\n[45]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀrey Dean.\\\\nDistributed representations of words and phrases and their compositionality.\\\\narXiv:1310.4546 ,2 0 1 3 .\\\\n[46]Sumit Misra, Soumyadeep Thakur, Manosij Ghosh, and Sanjoy Kumar Saha. An\\\\nautoencoder based model for detecting fraudulent credit card transaction. Procedia\\\\nComputer Science , 167:254–262, 2020. International Conference on Computational\\\\nIntelligence and Data Science.\\\\n[47]Sumit Misra, Soumyadeep Thakur, Manosij Ghosh, and Sanjoy Kumar Saha. An\\\\nautoencoder based model for detecting fraudulent credit card transaction. Procedia\\\\nComputer Science , pages 254–262, 2020.\\\\n[48]Giulia Moschini, Régis Houssou, Jérôme Bovay, and Stephan Robert-Nicoud. Anomaly\\\\nand fraud detection in credit card transactions using the arima model. Engineering\\\\nProceedings ,5 : 5 6 – 6 7 ,2 0 2 1 .\\\\n[49]Mauritius Much, Frederik Obermaier, Bastian Obermayer, and Vanessa Wormer.\\\\nSo funktioniert das system bahamas. URL https://www.sueddeutsche.de/\\\\nwirtschaft/bahamas-leaks-so-funktioniert-das-system-bahamas-1.3172913 .\\\\n[Accessed 08.08.2023].\\\\n[50]Mohammad Robihul Muﬁd, Arif Basoﬁ, M. Udin Harun Al Rasyid, Indhi Farhandika\\\\nRochimansyah, and Abdul rokhim. Design an mvc model using python for ﬂask\\\\nframework development. In International Electronics Symposium (IES) , pages 214–\\\\n219, 2019.\\\\n[51]Andreas Müller. word cloud. URL https://github.com/amueller/word_cloud/\\\\ntree/main . [Accessed 05.10.2023].\\\\n[52]Li-Qiang Niu and Xin-Yu Dai. Topic2vec: Learning distributed representations of\\\\ntopics. International Conference on Asian Language Processing (IALP) , pages 193–\\\\n196, 2015.\\\\n[53]nltk-lemma-wordnet. nltk.corpus.reader.wordnet module. URL https://www.nltk.\\\\norg/api/nltk.corpus.reader.wordnet.html . [Accessed 27.10.2023].\\', \\'Bibliography xv\\\\n[54]Mostofa Ali Patwary, Diana Palsetia, Ankit Agrawal, Wei-keng Liao, Fredrik Manne,\\\\nand Alok Choudhary. Scalable parallel optics data clustering using graph algorith-\\\\nmic techniques. In High Performance Computing, Networking, Storage and Analysis .\\\\nACIM, 2013.\\\\n[55]Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global\\\\nvectors for word representation. EMNLP ,p a g e1 5 3 2 – 1 5 4 3 ,2 0 1 4 .\\\\n[56]Robert-George Radu, Iulia-Maria Rădulescu, Ciprian-Octavian Truică, Elena-Simona\\\\nApostol, and Mariana Mocanu. Clustering documents using the document to vector\\\\nmodel for dimensionality reduction. AQTR , pages 1–6, 2020.\\\\n[57]Nils Reimers and Iryna Gurevych. sentence-transformers/paraphrase-minilm-l6-v2.\\\\nURL https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-\\\\nv2#sentence-transformersparaphrase-minilm-l6-v2 . [Accessed 04.10.2023].\\\\n[58]Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese\\\\nbert-networks. arXiv:1908.10084 ,2 0 1 9 .\\\\n[59]Larry R.MEDSKER and L. C. JAIN. Recurrent neural networks. Design and Appli\\\\uffff\\\\ncations ,v o l u m e5 .2 0 0 1 .\\\\n[60]Yusuf Sahin and Ekrem Duman. Detecting credit card fraud by decision trees and\\\\nsupport vector machines. World Congress on Engineering ,2 1 8 8 : 4 4 2 – 4 4 7 ,2 0 1 2 .\\\\n[61]Shyam Seshadri. Angular Up and Running: Learning Angular, Step by Step . O’Reilly\\\\nMedia, Inc., 2018.\\\\n[62]Grigori Sidorov, Alexander Gelbukh, Helena Gómez-Adorno, and David Pinto. Soft\\\\nsimilarity and soft cosine measure: Similarity of features in vector space model. Com\\\\uffff\\\\nputación y Sistemas ,1 8 : 4 9 1 – 5 0 4 ,2 0 1 4 .\\\\n[63]Gerd Stumme and Robert Jäschke. Internet-suchmaschinen. Technical report, 2011.\\\\nLecture script.\\\\n[64]Dodi Sudiana, Mia Rizkinia, and Fahri Alamsyah. Performance evaluation of machine\\\\nlearning classiﬁers for face recognition. In International Conference on Quality in\\\\nResearch (QIR): International Symposium on Electrical and Computer Engineering ,\\\\npages 71–75, 2021.\\\\n[65]Yichuan Tang and Xuan Choo. Intrinsic divergence for facial recognition. Centre for\\\\nTheoretical Neuroscience , pages 1–17, 2008. Paper.\\\\n[66]tﬁdf-scikit-learn. Tf–idf term weighting. URL https://scikit-learn.org/\\\\nstable/modules/feature_extraction.html#text-feature-extraction . [Accessed\\\\n29.09.2023].\\\\n[67]M. A. Turk and A. P. Pentland. Face recognition using eigenfaces. Computer Society\\\\nConference on Computer Vision and Pattern Recognition , pages 586–591, 1991.\\', \\'Bibliography xvi\\\\n[68]UniversalSentEnc-dev. universal-sentence-encoder. URL https://tfhub.dev/\\\\ngoogle/universal-sentence-encoder/4 . [Accessed 04.10.2023].\\\\n[69]Ike Vayansky and Sathish A.P. Kumar. A review of topic modeling methods. Infor\\\\uffff\\\\nmation Systems ,9 4 ( 1 0 1 5 8 2 ) : 1 – 1 5 ,2 0 2 0 .\\\\n[70]A. Voit, A. Stankus, S. Magomedov, and I. Ivanova. Big data processing for full-text\\\\nsearch and visualization with elasticsearch. IJACSA ,8 : 1 – 8 ,2 0 1 7 .\\\\n[71]Chang Wang and Sridhar Mahadevan. Multiscale manifold learning. AAAI Conference\\\\non Artiﬁcial Intelligence ,2 7 : 9 1 2 – 9 1 8 ,2 0 1 3 .\\\\n[72]Ziqiang Wang and Xu Qian. Text categorization based on lda and svm. In Inter\\\\uffff\\\\nnational Conference on Computer Science and Software Engineering , pages 674–677,\\\\n2008.\\\\n[73]Andy B. Yoo, Morris A. Jette, and Mark Grondona. Slurm: Simple linux utility\\\\nfor resource management. In Job Scheduling Strategies for Parallel Processing , pages\\\\n44–60, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.\\\\n[74]V. Zamﬁr, M. Carabas, C. Carabas, and N. Tapus. Systems monitoring and big data\\\\nanalysis using the elasticsearch system. International Conference on Control Systems\\\\nand Computer Science , pages 188–193, 2019.\\\\n[75]Vladimir Zaslavsky and Anna Strizhak. Credit card fraud detection using self-\\\\norganizing maps. Information and Security ,1 8 : 4 8 – 6 3 ,2 0 0 6 .\\\\n[76]Jun Zhang, Yong Yan, and M. Lades. Face recognition: eigenface, elastic matching,\\\\nand neural nets. 85(9):1423–1435, 1997. doi: 10.1109/5.628712.\\\\n[77]Wen Zhang, Taketoshi Yoshida, and Xijin Tang. Tﬁdf, lsi and multi-word in infor-\\\\nmation retrieval and text categorization. IEEE International Conference on Systems,\\\\nMan and Cybernetics , pages 108–113, 2008.\\\\n[78]Radim Řehůřek. Doc2vec paragraph embeddings, 2022. URL https://\\\\nradimrehurek.com/gensim/models/doc2vec.html . [Accessed 01.10.2023].\\', \\'Eidesstattliche Erklärung xvii\\\\nEidesstattliche Erklärung\\\\nHiermit erkläre ich, Klara Maximiliane Gutekunst, dass ich die vorliegende Arbeit mit dem\\\\nTitel “Identiﬁcation of Key Information with Topic Analysis on Large Unstructured Text\\\\nData” selbstständig und nur mit den nach der Prüfungsordnung der Universität Kassel\\\\nzulässigen Hilfsmitteln angefertigt habe. Die verwendete Literatur ist im Literaturverze-\\\\nichnis angegeben. Wörtlich oder sinngemäß übernommene Inhalte habe ich als solche\\\\nkenntlich gemacht.\\\\nKassel, 21. November 2023\\\\nKlara Maximiliane Gutekunst\\']',\n",
       " '[\\'  \\\\n- THE COMMONWEALTH OF THE BAHAMAS \\\\nThe International Business Companies Act 2000; The Segregated Accounts Act, 2004 \\\\nCompany Limited by Shares \\\\n     RESTATED _ \\\\nmo MEMORANDUM OF ASSOCIATION if \\\\nOF CO \\\\nINNOVATIS INVESTMENTS FUND LIMITED (SAC) \\\\n1. The name of the Company is INNOVATIS INVESTMENTS FUND LIMITED \\\\n(SAC). \\\\n| 2. The registered office of the Company will be at 2\"™ Terrace West, Centreville, P.O. \\\\n| Box N-7755, Nassau, Bahamas. \\\\n3. The registered agent of the Company will be First Choice Services Ltd., an Terrace \\\\nWest, Centreville House, P.O. Box N-10567, Nassau, Bahamas. \\\\n4. The object or purpose for which the Company is established is to engage in any act or \\\\nCommonwealth of the Bahamas. \\\\n5. In the absence of appropriate authorisation the Company may not: \\\\n1 \\\\nCompanies Regulation Act 2000; \\\\n(b) carry on business as an insurance or a reinsurance company; or \\\\n| \\\\n(c) carry on the business of providing corporate or financial services as \\\\ndefined’ by the Financial and Corporate Service Providers Act 2000. \\\\n6. The liability of the members 1s limited. \\\\nshares. This Memorandum ‘is a governing instrument within the meaning of the SAC. \\\\nAct. \\\\n  1/3 a activity that is not prohibited under any law for the time being in force in The \\\\n(a) carry on banking or trust business as defined by the Banks and Trust \\\\n7. The Company is and shall remain registered as a segregated accounts company under \\\\nthe SAC Act and shall maintain a Segregated Account in respect of certain classes of\\\\n\\', \\'  \\\\n10. \\\\n11. \\\\n  12. \\\\n13. \\\\n14. The company’s base currency is USD and shall, for the time being, be authorised to \\\\nissue Shares in the currency of the USD. \\\\nThe Company will, for the time being, have an authorized share capital of 10,100 (ten \\\\nthousand one hundred USD), which is divided into one (1) Class.of Share as follows: \\\\nThe “Management Shares”: \\\\n10\\\\\\'000 (ten thousand) registered voting Shares of USD 0.01 (one USD cent) par value \\\\neach; \\\\nThe “Investors’ Shares” or “Shares”: \\\\n1\\\\\\'000\\\\\\'000 (one million) registered non-voting Shares of USD 0.01 (one United States \\\\nDollar cent) par value each (hereinafter the \"Class A Shares”); \\\\nThe Company has the power, by a resolution of the members or by a resolution of the \\\\ndirectors, to increase OF reduce the capital and to divide the shares in the \\\\naforementioned or any increased or reduced capital into several classes and to issue \\\\nthe shares of any class of classes at par or at a premium and with qualified or special \\\\nrights, privileges or conditions attached to them or subject to any restrictions or \\\\nlimitations. \\\\nThe designations, powers, preferences, rights, qualifications, limitations and \\\\nrestrictions of each class and series of shares that the Company is authorised to issue \\\\nshall be fixed by resolution of directors, but the directors shall not allocate different \\\\nrights as to voting, dividends, redemption or distributions on liquidation unless the \\\\nMemorandum of Association shall have been amended to create separate classes of \\\\nshares and all the aforesaid rights as to voting, dividends, redemption and \\\\ndistributions shall be iddntified in each separate class. \\\\nThe shares of the Company may be issued as registered or non-registered shares as \\\\nthe directors may from| time to time determine by resolution or as stipulated in the \\\\nTerm Sheet. { \\\\nShares in the Company}may be transferred subject to the prior or subsequent approval \\\\nof the Company as evidenced by a resolution of directors or by a resolution of \\\\nmembers. \\\\nThe Company may amend its Memorandum of Association by a resolution of \\\\nmembers or by resolution of directors. \\\\n2/3\\\\n\\', \\'  \\\\nAssociation. \\\\nMarco Montanari, Director \\\\n2” Terrace West, Centreville \\\\nNassau, Bahamas \\\\nWitness to the above signatures: \\\\n| \\\\n| \\\\n| ~ \\\\n, Alisa Richardson \\\\n2\" Terrace West, Centreville \\\\nNassau, Bahamas \\\\n    : 3/3          Olivier Chaponnier, Director \\\\n2\"! Terrace West, Centreville \\\\nNassau, Bahamas \\\\nNassau, May 27, 2009 \\\\nCamonwealth of the Bahamas \\\\ni General\\\\\\'s Office \\\\nIoay to roar bn acy te original Gepoaad in thts fie. \\\\nGeneral \\\\nere 203 MA \\\\n  \\\\n   \\\\n\\']',\n",
       " \"['  \\\\nREGISTER OF DIRECTORS AND OFFICERS OF VESTA INTERNATIONAL FUND LIMITED SAC \\\\nINCORPORATED UNDER THE INTERNATIONIAL BUSINESS COMPANIES ACT \\\\n  \\\\n  \\\\n  NAMES ADDRESS OCCUPATION OFFICE HELD DATE OF ELECTION Resignation \\\\nDate \\\\nGraham Garner Nassau, Bahamas Businessman DIRECTOR Dec 5 2014 \\\\nWendy Warren Nassau, Bahamas Businesswoman |DIRECTOR Dec 5 2014 \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n  \\\\n                \\\\n  MUNDO ADVISORS\\\\n']\",\n",
       " '[\\'  \\\\n  COMMONWEALTH OF THE BAHAMAS \\\\nTHE INTERNATIONAL BUSINESS COMPANIES ACT 2000 \\\\n(No. 45 of 2000) \\\\nARTICLES OF ASSOCIATION \\\\nOF \\\\nDIVERSIFIED STRATEGIES FUND LTD. \\\\nMarch 29\" 2007. \\\\nPRELIMINARY \\\\nIn these Articles, if not inconsistent with the context, the words and expressions standing in the first \\\\ncolumn of the following table shall bear the meanings set opposite them in the second column \\\\nthereof. \\\\nExpressions Meanings \\\\n1.1 Business Day any day except days which shall be in Brazil, New York or The \\\\nBahamas a legal holiday or a day on which banking institutions \\\\nare authorized or required by law or other government action to \\\\nclose or such other day as may be determined by a Resolution of \\\\nDirectors or stipulated in the Offering Memorandum . \\\\n12 Capital The sum of the aggregate par value of all outstanding Shares with \\\\npar value of the Company and Shares with par value held by the \\\\nCompany as treasury Shares plus \\\\n1.2.1 the aggregate of the amounts designated as capital of all \\\\noutstanding Shares without par value of the Company and Shares \\\\nwithout par value held by the Company as treasury Shares, and \\\\n1.2.2 the amounts as are from time to time transferred from surplus to \\\\ncapital by a resolution of Directors. \\\\n1.3 Class a Class of Shares of the Company, each Class representing \\\\ninterests in a fund with its own investment objectives. \\\\n1.4 Member A person who holds Ordinary Shares in the Company \\\\n \\\\n\\', \"  \\\\nOD \\\\n  1.5 \\\\n1.6 \\\\n1.7 \\\\n1.8 \\\\n1.9 \\\\n1.10 \\\\n1.11 \\\\n1.12 \\\\n1.13 Material Agreements \\\\nMonth \\\\nNet Asset \\\\nValue \\\\nNotice \\\\nOffering Memorandum \\\\nPerson \\\\nRedemption Day \\\\nResolution of 1.12.1 \\\\nDirectors \\\\n1.12.2 \\\\n1.12.3 \\\\nResolution of 1.13.1 \\\\nMembers Any and all contracts entered into by the Company with its \\\\nservice providers, including, but not limited to, its Administrator, \\\\nRegistrar and Trarisfer Agent, Investment Advisor, Investment \\\\nManager, Custodian, Banker or Broker. \\\\ncalendar month. \\\\nthe Net Asset Value of each Share shall be determined on each \\\\nValuation Day by dividing the value of the Company\\'s total assets \\\\nless its total liabilities by the number of Shares outstanding at \\\\nsuch time. \\\\nWritten notice unless otherwise specifically stated. \\\\nAll constituent parts of the Company’s Offering Memorandum \\\\nincluding all Appendices. \\\\nAn individual, a corporation, a trust, the estate of a deceased \\\\nindividual, a partnership or an unincorporated association of \\\\npersons. \\\\nThe last Business Day of each calendar quarter or such other day \\\\nas defined in the Offering Memorandum or as determined by a \\\\nResolution of Directors. \\\\nA resolution approved at a duly constituted meeting of Directors \\\\nor of a committee of Directors of the Company, by affirmative \\\\nvote of a simple majority of the Directors present at the meeting \\\\nwho voted and did not abstain; or \\\\nA resolution consented to in writing by an absolute majority of all \\\\nthe Directors or of all the members of the committee, as the case \\\\nmay be; \\\\nwhere a Director is given more than one vote in any \\\\ncircumstances, he shall in the circumstances be counted for the \\\\npurposes of establishing majorities by the number of votes he \\\\ncasts. \\\\nA resolution approved at a duly constituted meeting of the \\\\nMembers of the Company by the affirmative vote of \\\\n1.13.1.1 An absolute majority of the votes of the Shares \\\\nentitled to vote thereon and were voted and did \\\\nnot abstain, or \\\\n1.13.1.2 An absolute majority of the votes of each Class or \\\\nSeries of Shares entitled to vote thereon as a Class \\\\nor Series and were voted and not abstained and of \\\\nan absolute majority of the votes of the remaining \\\\n \\\\n\", \\'  \\\\n  1.14 Restricted \\\\npersons 1.13.2 \\\\n1.14.1 \\\\n1.14.2 \\\\n1.14.3 Shares entitled to vote thereon and were voted and \\\\nnot abstained; or \\\\nA resolution consented to in writing by \\\\n1.13.2.1 \\\\n1.13.2.2 An absolute majority of the votes of each Class or \\\\nSeries of Shares entitled to vote thereon, or \\\\nAn absolute majority of the votes of each Class or \\\\nSeries entitled to vote thereon as a Class or Series \\\\nand of an absolute majority, of the votes of the \\\\nremaining Shares entitled to vote thereon. \\\\nA United States Person, Bahamian Person, and any Other Person \\\\nwho may be restricted from purchasing the Shares of a given \\\\nClass or Series of Shares. \\\\nUnited \\\\nStates \\\\nperson \\\\nBahamian \\\\nPerson \\\\nProfessiona \\\\n| Investor “A United States Person\" as defined in Regulation \\\\nS promulgated under the United States Securities \\\\nAct of 1933, as amended, or in the United States \\\\nInternal Revenue Code of 1986, as amended, \\\\nexcluding a United States Person who is both a \\\\n\"Qualified Purchaser” as this term is defined in \\\\nSection 2(a)(51) of the United States Investment \\\\nCompany Act, 1940 and an “Accredited Investor” \\\\nas this term is defined in Regulation D of the \\\\nSecurities Act. \\\\nA national, citizen or resident of a person normally \\\\nresident in the Commonwealth ‘of The Bahamas or \\\\nany corporation, partnership, trust, estate or other \\\\nentity formed or organised under the laws of, or \\\\nexisting in the Commonwealth of The Bahamas \\\\nand deemed resident therein within the meaning of \\\\nthe Exchange Control Regulations made under the \\\\nExchange Control Regulations Act. \\\\nAny person or entity wishing to make an \\\\ninvestment in the Company, and in addition to \\\\npoints 1.14.1 and 1.14.2 above must fall within and \\\\nbe able to prove one of the following: \\\\n(a) any bank or trust company licensed under \\\\nthe Bank and Trust Companies. Regulation \\\\nAct or licensed in a prescribed jurisdiction, \\\\nwhether acting in its individual or fiduciary \\\\ncapacity; \\\\n(b) any registered broker-dealer or firm \\\\n \\\\n\\', \\'  \\\\nregistered as a securities investment advisor \\\\nunder the Securities Industry Act which \\\\nmaintains a minimum of one hundred and \\\\ntwenty thousand dollars ($120,000) of \\\\nregulatory capital or is a Broker-dealer or \\\\nfirm of Securities Investment Advisors \\\\nregistered in a prescribed jurisdiction; \\\\n(c) any insurance company licensed under the \\\\nInsurance Act or licensed in a prescribed \\\\njurisdiction; \\\\n(d) any investment fund licensed or registered \\\\nunder this Act or regulated in a prescribed \\\\nJurisdiction; \\\\n\\\\\\\\ (e) any natural person whose individual net \\\\nworth, or joint net worth with the person’s \\\\nspouse exceeds one million dollars \\\\n($1,000,000); \\\\n(f) any natural person who had an individual \\\\nincome in excess of two hundred thousand \\\\ndollars ($200,000) in each of the two most \\\\nrecent years or joint income with.that \\\\nperson’s spouse in excess of three hundred \\\\nthousand dollars ($300,000) in each of those \\\\nyears and has a reasonable expectation of \\\\nreaching the same income level in the \\\\ncurrent year; \\\\n(g) any trust with total assets in excess of five \\\\nmillion dollars ($5,000,000); \\\\n(h) any entity in which all the equity owners \\\\nsatisfies one of the requirements in \\\\nparagraph (a) to (g). \\\\n14.4 Other Any person, corporation or entity whose ownership \\\\nPerson of Shares is unlawful or detrimental to the interests \\\\nof the Company as determined by the Board of \\\\ndirectors or who is otherwise defined as a \\\\nRestricted Person by a resolution of the Directors. \\\\n1.15 Series each separate tranche of any Class of Preferred Shares issued \\\\nwithin a designated period. \\\\n  1.16 Shareholders the holders of Preferred Shares in the Company.\\\\n\\', \\'  \\\\n  1.17 \\\\n1.18 \\\\n1.19 \\\\n1.20 \\\\n1.21 \\\\n1.22 \\\\n1.23 \\\\n1.24 \\\\n1.25 \\\\n1.26 \\\\n1.27 \\\\n2.1 \\\\n2.2 the The Memorandum of Association of the Company as originally \\\\nMemorandum framed or as from time to time amended. \\\\nthe Act The International Business Companies Act 2000 of the \\\\nCommonwealth of The Bahamas as amended \\\\nthe Seal The Common Seal of the Company. \\\\nthese Articles These Articles of Association as originally framed or as from \\\\ntime to time amended. \\\\nTreasury Shares in the Company that were previously issued but were \\\\nShares repurchased, redeemed or otherwise acquired by the Company \\\\nand not cancelled. \\\\nUnited States The United States of America, its territories, possessions or any \\\\nor U.S. area subject to its jurisdiction. \\\\nValuation The last business Day of each calendar month or such other days \\\\nDay as defined in the Offering Memorandum or as determined by a \\\\nResolution of Directors. \\\\nYear Calendar year \\\\n\"Written\" or any term of like import includes words typewritten, printed, painted, engraved, \\\\nlithographed, photographed or represented or reproduced by any mode of representing or \\\\nreproducing words in a visible form, including telex, telegram, cable or other forms of \\\\nwriting produced by electronic communication. \\\\nSave as aforesaid, any words or expressions defined in the Act shall bear the same meaning \\\\nin these Articles. \\\\nA reference to money in these Articles is a reference to the currency of the United States of \\\\nAmerica unless otherwise stated. \\\\nAUTHORISED CAPITAL \\\\nThe Authorised Capital is U.S.$ 30,050,000.00 consisting of 30,000,000 authorized Preferred \\\\nShares (the “Preferred Shares”), with U.S.$1.00 par value per Preferred Share, and 50,000 \\\\nvoting Ordinary Shares (the “Ordinary Shares”), with U.S.$1.00 par value per Ordinary \\\\nShare. \\\\nThe Company is hereby authorized to issue fractions of a Preferred Share as the Directors \\\\nmay by resolution determine and each such fractional Preferred Share shall have the \\\\ncorresponding fractional designations, powers, references, rights, qualifications and \\\\nlimitations of a Preferred Share of the same Class or Series of Preferred Shares. \\\\n  \\\\n \\\\n\\', \\'  \\\\n  O \\\\n2.3 The Company is hereby authorized to issue such Classes of Preferred Shares, and may \\\\nsubdivide any Class of Preferred Shares into such series of Preferred Shares, in each case as \\\\nthe Directors may by resolution determine, which resolutions shall (i) fix the designations, \\\\npowers, preferences, rights, qualifications and limitations of such additional Classes or Series \\\\nof Preferred Shares, and (ii) set forth the investment objectives of such Classes or Series of \\\\nPreferred Shares. \\\\nSHARE RIGHTS AND LIMITATIONS \\\\nVOTING \\\\nThe Members shall be entitled to attend and vote at all General Meetings of Members and to \\\\ntake any action by written Resolution of Members. \\\\nSubject to the provisions of these Articles and the Memorandum of Association of the \\\\nCompany, the Shareholders:shall not be entitled to attend and vote at any General Meetings \\\\nof Members or to take any action by written resolution of Shareholders. Meetings and \\\\nconsents of Shareholders required pursuant to these Articles and the Memorandum of \\\\nAssociation of the Company shall be governed mutatis mutandis by the provisions of Article \\\\n17 hereof. \\\\nMINIMUM PURCHASE \\\\nThe minimum initial investment is United States Dollars Fifty Thousand (US$50,000) or \\\\nsuch other amounts as may be stipulated in the Offering Memorandum or as may be \\\\ndetermined by a Resolution of Directors in relation to any particular Class of Preferred \\\\nShares. The initial offer price of each Preferred Share shall be United States Dollars one \\\\nhundred (US$100) plus any applicable sales commissions. Thereafter, the purchase price of \\\\neach Preferred Share shall be the prevailing Net Asset Value per Preferred Share, plus any \\\\napplicable sales commissions, subject to such other amounts as may be stipulated in the \\\\nOffering Memorandum or as may be determined by a Resolution of Directors in relation to \\\\nany particular Class of Preferred Shares. Subsequent purchases or transfers shall be \\\\ndetermined by a Resolution of Directors. \\\\nREDEMPTION OF PREFERRED SHARES \\\\nSubject to the provisions of Articles 11 and 13 hereof and the Offering Memorandum, \\\\nShareholders shall have the right to request redemption of their Preferred Shares, by notice \\\\nin the form acceptable to the Company provided such notice is received by the Administrator \\\\nno later than 5 p.m. New York time at least sixty (60) calendar days prior to the relevant \\\\nValuation Day, or any other such time limit as may be stipulated in the Offering \\\\nMemorandum or determined by a Resolution of the Directors. \\\\nDIVIDEND POLICY AND DISTRIBUTION \\\\nThe Directors from time to time and at their discretion may declare and pay dividends to the \\\\nholders of any Class or Series of Preferred Shares in amounts that the Directors in their \\\\ndiscretion deem appropriate but shall be under no obligation to do so. \\\\n  \\\\n \\\\n\\', \\'  \\\\n  \\\\nO \\\\n6.2 \\\\n6.3 \\\\n6.4 \\\\n6.5 \\\\n6.6 \\\\n6.7 \\\\n6.8 \\\\n6.9 \\\\n6.10 \\\\n7.1 No dividend shall be declared and paid except out of surplus and unless the Directors \\\\ndetermine that immediately after the payment of the dividend: \\\\n(a) the Company will be able to satisfy its liabilities as they become due in the \\\\nordinary course of its business; and \\\\n(b) the realizable value of the assets of the Company will not be less than the \\\\nsum of its total liabilities, as shown in the books of the Company, and its \\\\nissued and outstanding Share-capital. \\\\nDividends shall be paid on Preferred Shares of any Class or Series only out of lawfully \\\\navailable assets belonging to that Class or Series. \\\\nDividends if and when declared may be paid to one Class or Series of Shareholders to the \\\\nexclusion of the holders of other Classes or Series, or in unequal amounts to holders of the \\\\nvarious Classes or Series of Preferred Shares. \\\\nDividends may be declared and paid in money, Preferred Shares or property. \\\\nIf several persons are registered as joint holders of any Preferred Share, any of them may give \\\\neffectual receipt for any dividend or other monies payable on or in respect of the Preferred \\\\nShare. \\\\nNotice of any dividend that may have been declared shall be given to each Shareholder in the \\\\nmanner hereinafter provided for notices and all dividends unclaimed for three (3) years after \\\\nhaving been declared may be forfeited by the Directors for the benefit of the Company. \\\\nNo dividend shall bear interest against the Company. \\\\nIf the Company shall be wound up (whether the liquidation is voluntary, under supervision, \\\\nor by the Court), the Shareholders shall be entitled to participate in the assets of the \\\\nCompany such Shareholders in proportion to the number of Preferred Shares held by them \\\\nand recorded in the share register of the Company. \\\\nThe Members shall be entitled to repayment of the amount of capital paid up on their \\\\nPreferred Shares, subject to repayment in full to the Shareholders, but shall not be entitled to \\\\nparticipate further in the distribution of assets in a winding up of the Company. \\\\nVARIATION OF CLASS OF RIGHTS \\\\nIf at any time the authorized capital is divided into different classes or Series of Preferred \\\\nShares, the rights attached to any Class or Series (unless otherwise provided by the terms of \\\\nissue of the Preferred Shares of that Class or Series) may, whether or not the Company is \\\\nbeing wound up, be varied with the consent in writing of the holders of not less than three- \\\\nfourths (3/4) of the issued Preferred Shares of that Class or Series and of the holders of not \\\\nless than three-fourths (3/4) of the issued Preferred Shares of any other Class or Series of \\\\nPreferred Shares which may be affected by such variation, \\\\n \\\\n\\', \"  \\\\n  O \\\\n7.2 \\\\n8.1 \\\\n8.2 \\\\n8.3 \\\\n8.4 \\\\n9.1 \\\\n9.2 \\\\n9.3 The rights conferred upon the holders of Preferred Shares of any Class issued with preferred \\\\nor other rights shall not, unless otherwise expressly provided by the terms of issue of the \\\\nPreferred Shares of that Class, be deemed to be varied by the creation or issue of further \\\\nPreferred Shares ranking pari passu therewith. \\\\nREGISTERED SHARES AND RESTRICTION ON SALE OR TRANSFER \\\\nShares shall be issued as registered Shares only. \\\\nShares may not be issued or transferred to any Restricted Person. Shares are transferable only \\\\nwith the prior written consent of the Company, such consent may be withheld at the sole \\\\ndiscretion of the Company. The Company or its agent has the right to determine \\\\nconclusively whether any person or entity is a Restricted Person. The Company or its agent \\\\nmay determine that a person or entity is a Restricted Person after such person or entity has \\\\npurchased Shares. \\\\nIf it shall come to the attention of the Company or its agent at any time that any Shares are \\\\nbeneficially owned by a Restricted Person, either alone or in connection with any other \\\\nperson, the Company may compulsorily redeem such Shares at their Net Asset Value per \\\\nShare and the Restricted Person(s) will cease to be the owner(s) of those Shares. \\\\nEach subscriber for Preferred Shares of any series shall be required to represent and warrant \\\\nin the Subscription Agreement that he is not a Restricted Person and that the Preferred Shares \\\\nare being acquired for his own account in the ordinary course of his business and not with a \\\\nview to any public resale or distribution thereof, nor with any present intention of \\\\ndistribution or selling the same. \\\\nREGISTERS AND TITLE TO SHARES \\\\nThe Company shall have authority to issue Preferred Shares only in registered form. Share \\\\nCertificates will not be issued by the Company, except in respect of Ordinary Shares. \\\\nThe Company shal! appoint a Registrar and Transfer Agent upon such terms and conditions \\\\nas shall be agreed to by the Directors and representatives of said Agent and the Company \\\\nshall pay such remuneration and expenses of said Agent as the Directors shall approve. \\\\nA share register shall be kept by or for the Company separately in respect of each Class or \\\\nSeries of Shares and there shall be entered on such Registers: \\\\n(1) the name, address, nationality, place or legal residence of and the number of \\\\nShares held by each Shareholder or Member; \\\\n(2) the Valuation Day as of which each Shareholder\\'s or Member\\'s Shares were \\\\nissued; \\\\n(3) the Valuation Day as of which any transfer of Shares is effected; and \\\\n \\\\n\", \"  \\\\n  9.4 \\\\n9.5 \\\\n9.6 \\\\n9.7 \\\\n9.8 \\\\n9.9 \\\\n10. \\\\n10.1 \\\\n10.2 (4) the Valuation Day as of which each Shareholder\\'s Shares, or a portion \\\\nthereof, are redeemed. \\\\nThe share register may be kept in written form or by such other means (including electronic \\\\nrecording) as the Directors may from time to time approve. \\\\nThe share register shall be conclusive evidence as to the ownership of Shares entered therein, \\\\nand no notice of any trust, express, implied or constructive, shall be entered upon the share \\\\nregister in respect of any such Shares. \\\\nUpon any change of name or address on the part of a Shareholder being notified to the \\\\nCompany, the Company shall forthwith comply with all formalities as shall be required to \\\\ncause the Register to be altered accordingly. \\\\nA body corporate or any other legal entity may be registered as a Shareholder or as one ofa \\\\nnumber of joint Shareholders. \\\\nIf several persons are registered as joint holders of any Shares, any one-of such persons may \\\\ngive a receipt for any dividend payable in respect of such Shares. \\\\nEvidence of ownership of Shares shall be the share register maintained by or for the \\\\nCompany, which may be by computer display or printout. A Shareholder who does not elect \\\\nto receive a certificate in respect of his Shares may ask for such computer printout or other \\\\nextract from the share register to be certified as correct by the Director, President or a Vice \\\\nPresident or by the Secretary of the Company, or by the Registrar and Transfer Agent. \\\\nISSUE AND SALE OF PREFERRED SHARES \\\\nThe applicable Offering Price for Preferred Shares after trading has begun will be based on \\\\nthe Net Asset Value per Preferred Share on the Valuation Day after the order is received by \\\\nthe Company. In the event the Company has suspended or postponed the Net Asset Value \\\\ndetermination, the valuations on the first Valuation Day occurring after receipt of the order \\\\nwill be utilized. The Company reserves the right to suspend the sale of Preferred Shares in \\\\nresponse to conditions in the securities markets or upon a declared suspension in accordance \\\\nwith Article 13 hereof. A sales commission fee of up to four (4%) per cent of the total \\\\nsubscription amount or such other amounts as may be stipulated in the Offering \\\\nMemorandum or as may be determined by a Resolution of Directors in relation to any \\\\nparticular Class of Preferred Shares offer may be charged for all Classes of Preferred \\\\nShares, but this fee may be waived in whole or part at the discretion of the Directors of the \\\\nCompany. \\\\nShareholders will be required to provide assurances satisfactory to the Company indicating \\\\nthat the Shareholder is a Professional Investor and is not a United States Person or Bahamian \\\\nPerson as such terms are defined in these Articles. \\\\n \\\\n\", \\'  \\\\n  () 10.3 \\\\n10.4 \\\\nil. \\\\n11.2 \\\\n11.3 \\\\n11.4 Shareholders will only receive a confirmation of their share holding. Share confirmations \\\\nwill be sent to investors within one month after Preferred Shares are issued. Applicants who \\\\nsubmit an application form should indicate on such form the full name and address of each of \\\\nthe persons in whose name the Preferred Shares are to be registered and in the case of a joint \\\\napplication which is to be the first named Shareholder. \\\\nThe Company may reject any application for Preferred Shares in whole or in part for any \\\\nreason which the Company in their sole discretion deem sufficient and need not assign any \\\\nreason therefor. \\\\nREDEMPTION OF PREFERRED SHARES \\\\nSubject to Article 13 hereof and the provisions of the Offering Memorandum or a Resolution \\\\nof the Directors, Shareholders have the right to request redemption of all or any part of their \\\\nShares on any Redemption Day: If, as a result of a redemption request less than United \\\\nStates Dollars Fifty Thousand (US$50,000.00) in value for each Class of Shares, or such \\\\nother amounts as may be stipulated in the Offering Memorandum or as may be determined \\\\nby a Resolution of Directors, are held by a Shareholder on redemption, then all remaining \\\\nShares shall be compulsorily redeemed. Payment of ninety percent (90%) (or more, in the \\\\ndiscretion of the Board of Directors) of the aggregate estimated redemption price for redeemed \\\\nShares normally will be made within thirty (30) days following the applicable Redemption Day, \\\\nand the balance will be paid (subject to audit adjustments) within thirty (30) days after \\\\ncompletion of the audit of the Company’s books for the fiscal year in which the redemption is \\\\neffected. \\\\nPayments will ordinarily be made in United States Dollars, but payments may also be made \\\\nin such other currencies which are freely purchasable with United States Dollars as the \\\\nShareholder effecting the redemption may request, provided that any foreign exchange costs \\\\nshall be deducted from the amount payable to the Shareholder. \\\\nAll redemption requests must be in the form provided to the Shareholder by the Company \\\\nand must be accompanied by a signature certification provided by a recognized Bank or Trust \\\\ncompany or witnessed by the recognized sales representative. \\\\nThe price to be paid for Preferred Shares tendered for redemption will be the next computed \\\\nNet Asset Value determined in accordance with the Article 12 immediately after a written \\\\nand irrevocable redemption request in proper form is received with signature certification \\\\nexcept in case of suspension of Net Asset Value determination or postponement of \\\\nredemption in which case Preferred Shares presented for redemption as from the date of such \\\\nsuspension or postponement will be redeemed upon the Company resuming redemption at \\\\nthe next Redemption Day. The proceeds of a redemption of Preferred Shares may be reduced \\\\nby charges imposed by the Administrator in respect of payments by wire or cheque \\\\nrespectively. \\\\nWithout prejudice to the power of the Directors to borrow, the Company may borrow an \\\\namount up to twenty- five (25%) percent of the Net Asset Value or such other amounts as \\\\nmay be stipulated in the Offering Memorandum or as may be determined by a Resolution of \\\\nDirectors, to finance the redemption of Preferred Shares. \\\\n \\\\n\\', \"  \\\\n  () 11.5 \\\\n11.6 \\\\n11.8 \\\\n12.1 The Company may, from time to time, require the redemption of any of its outstanding \\\\nPreferred Shares that are or become owned, directly or indirectly, by a U.S. Person, \\\\nBahamian Person, any person who is not a Professional Investor or by any person whose \\\\nownership of Preferred Shares is detrimental to the interests of the Company as determined \\\\nby the Directors. \\\\nAny compulsory redemption shall be made on the Valuation Day next following the issuance \\\\nof a written or telegraphic notice of ten (10) days of redemption to the Shareholder (or any \\\\nother such time limit as may be stipulated in the Offering Memorandum or determined by a \\\\nResolution of the Directors) and upon such other terms and conditions as the Directors shall \\\\ndeem advisable. The value of shares compulsorily redeemed will be calculated at the Net \\\\nAsset Value of those Shares determined on the last Valuation Day before compulsory \\\\nredemption takes place. \\\\nThe Directors may limit the amount of any Class of Preferred Shares which may be \\\\nredeemed on any given Redemption Day to 10% of the Net Asset Value of that particular \\\\nClass of Company’s issued and outstanding Preferred Shares, if they reasonably believe that \\\\nredemption of such Class of Preferred Shares is likely to have a material adverse effect on \\\\nthe Company, may (i) reduce the number of such Class of Preferred Shares to be redeemed \\\\non such Redemption Day, so that the aggregate Net Asset Value of all such Class of \\\\nPreferred Shares redeemed on such Redemption Day will not exceed 10% (or such greater \\\\npercentage as the Board of Directors may determine) of the Net Asset Value of‘such Class of \\\\nPreferred Shares, or (ii) effect a mandatory redemption of all Preferred Shares in such Class . \\\\nExcept as provided below, any such “gating” limitation shall be on a pro rata basis among all \\\\nShareholders requesting redemption as of such Redemption Day. Any part of a redemption \\\\nrequest to which effect is not given by reason of the gating limitation (“Deferred Shares”) \\\\nwill be treated as if a request has been made in respect of the next Redemption Day and all \\\\nfollowing Redemption Days (in relation to which the Directors have the.same power to limit \\\\nredemptions) until the original request has been satisfied in full. Except as provided in the \\\\nfollowing sentence, no priority will be given to the redemption of Deferred Shares over other \\\\nPreferred Shares requested to be redeemed if it is necessary to apply the gating limitation on \\\\nany subsequent Redemption Day. \\\\nNET ASSET VALUE \\\\nThe Net Asset Value of Preferred Shares shall be determined as of the Administrator\\'s close \\\\nof business on each Valuation Day. The Net Asset Value shall be determined by or at the \\\\ndirection of the Directors or by the Administrator and made available monthly at its \\\\nRegistered Office or such other office or such other time as the Directors may determine. \\\\n \\\\n\", \\'  \\\\n  12.2 \\\\n12.3 \\\\n12.4 \\\\n13. \\\\n13.1 For the purpose of calculating Net Asset Value, portfolio securities which are traded on \\\\nstock exchanges or traded on other organized markets are valued at the last known price on \\\\nthe principal market on which such securities are traded as of the close of business of such \\\\nmarket immediately proceeding the determination of the Net Asset Value. Fixed income \\\\nsecurities not traded in such markets are valued at the last available price or yield \\\\nequivalents obtained from one or more dealers or pricing services. If events materially \\\\naffecting the value of such securities occur between the time of obtaining their value and the \\\\ncalculation of the Net Asset Value of Preferred Shares, then these securities will be valued \\\\nat their fair value as determined in good faith by or under the direction of the Directors. \\\\nIn all cases the Net Asset Value of Preferred Shares is determined by dividing the value of \\\\nthe total assets properly allocated to such Preferred Shares less the liabilities properly \\\\nallocated to such Preferred Shares by the total number of Preferred Shares outstanding on \\\\nthe Valuation Day. \\\\nThe Net Asset Value of Preferred Shares shall be certified by a Director or an authorized \\\\nofficer or representative of the Company and any such certification shall be conclusive \\\\nexcept in the case of manifest error. \\\\nSUSPENSION OF ISSUE AND REDEMPTION OF PREFERRED SHARES \\\\nAND CALCULATION OF NET ASSET VALUE \\\\nNotwithstanding the provisions of Articles 10, 11 & 12 hereof the issue or redemption of \\\\nPreferred Shares or calculation of the Net Asset Value shall not be made during any period \\\\nwhen a suspension has been declared by the Directors in any of the following events or in \\\\nany other events (each a “Valuation Constraint”) stipulated in the Offering Memorandum: \\\\n12 \\\\n \\\\n\\', \\'  \\\\n  C) \\\\n14. \\\\n14.1 (a) one or more banks, stock exchanges, or other markets which provide a basis \\\\nfor valuing any of the assets of the Company are closed other than for or \\\\nduring ordinary holidays or if dealings therein are restricted or suspended, \\\\n(b) as a result of political, economic, military or monetary events or any \\\\ncircumstances outside the control, responsibility and power of the Company, \\\\ndisposal of the assets of the Company is not reasonably practicable without \\\\nbeing seriously detrimental! to Shareholders’ interests or if, in the opinion of \\\\nthe Board of Directors, a fair price cannot be calculated of the assets of the \\\\nCompany; \\\\n(c) a breakdown of the means of communication normally used for the valuing \\\\nof any investment of the Company or if for any reason the value of any asset \\\\nof the Company may not be determined as rapidly and accurately as \\\\nrequired; \\\\n(d) redemption of Preferred Shares is restricted by a law of general application; \\\\n(e) a notice has been published convening a meeting of the holders of Ordinary \\\\nShares or of the Board of Directors for the purpose of resolving a winding up \\\\nof the Company; or \\\\n(f) redemption of Preferred Shares would seriously impair the Company’s \\\\nability to operate or would seriously jeopardize its tax status \\\\n(g) during any period after the Members or Directors have resolved to liquidate \\\\nthe Company in accordance with the provisions of Article 31 hereof ;or \\\\n(h) upon the order of a supervisory authority of the Commonwealth of The \\\\nBahamas. \\\\nThe Directors shall take all reasonable steps to bring any suspension to an end as soon as \\\\npossible. Notice of any suspension or the cessation of any suspension will be given to \\\\nShareholders and the Securities Commission of The Bahamas. \\\\nTRANSFER OF SHARES \\\\nSubject to any limitations in the Memorandum and in these Articles, Shares in the Company \\\\nmay be transferred by a written instrument of transfer signed by the transferor and \\\\ncontaining the name and address of the transferee, but in the absence of such written \\\\ninstrument of transfer the Directors may accept such evidence of a transfer as they consider \\\\nappropriate. \\\\n13 \\\\n  \\\\n \\\\n\\', \"  \\\\n  14.2 \\\\n14.3 \\\\n14.4 \\\\n14.5 \\\\n15. \\\\n15.1 \\\\n15.2 The Company shall not be required to treat a transferee of a Share in the Company as a \\\\nShareholder or Member until the transferee\\'s name has been entered in the share register. \\\\nShares are transferable only with the prior written consent of the Directors of the Company. \\\\nThe Directors may in their absolute discretion and without assigning any reason therefore \\\\ndecline to register-any transfer of ‘Shares and shall within thirty (30) days (or any other such \\\\ntime limit as may be stipulated in the Offering Memorandum or determined by a Resolution \\\\nof the Directors) after the date on which the transfer was lodged with the Company or to a \\\\nduly authorized agent send to the transferee notice of the refusal. \\\\nSubject to any limitations in the Memorandum and in these Articles, the Company must, on \\\\nthe application of the transferor or transferee of a Share in the Company, enter in the share \\\\nregister the name of the transferee of the Share save that the registration of transfers may be \\\\nsuspended and the share register closed at such times and for such periods as the Company \\\\nmay from time to time by Resolution of Directors determine provided always that such \\\\nregistration shall not be suspended and the share register closed for more than sixty (60) \\\\ndays in any period of twelve (12) months. \\\\nTransfers may be refused unless an instrument of transfer in the form provided by the \\\\nCompany has been completed and delivered to the Administrator. The Shares may not be \\\\ntransferred to U.S. Persons, Bahamian Persons or any person who is not a Professional \\\\nInvestor without the prior written approval by or on behalf of the Company. The Shares \\\\nmay not be pledged, assigned or otherwise encumbered without the prior written consent by \\\\nor on behalf the Company. \\\\nThe Company, in its sole discretion, may decline to transfer any Shares, or decline to \\\\nregister any Shareholder, including where such transfers or registration might place the \\\\nCompany in breach of any applicable law, regulation or requirement of any jurisdiction, \\\\notherwise adversely affect or prejudice the tax status, residence or good standing of the \\\\nCompany, or otherwise cause the Company to suffer financial or legal disadvantage. \\\\nTRANSMISSION OF SHARES \\\\nThe executor or administrator of a deceased Shareholder or Member, the guardian of an \\\\nincompetent Shareholder or Member or the trustee of a bankrupt Shareholder or Member \\\\nshall be the only person recognized by the Company as having any title to his Shares but \\\\nthey shall not be entitled to exercise any rights as a Shareholder or Member of the Company \\\\nuntil they have proceeded as set forth in the next following two (2) articles. \\\\nAny person becoming entitled by operation of law or otherwise to a Share or Shares in \\\\nconsequence of the death, incompetence or bankruptcy of any Shareholder or Member may \\\\nbe registered as a Shareholder or Member upon such evidence being produced as may \\\\nreasonably be required by the Directors. An application by any such person to be registered \\\\nas a Shareholder or Member shall be deemed to be a transfer of Shares of the deceased, \\\\nincompetent or bankrupt Shareholder or Member and the Directors shall treat it as such. \\\\n \\\\n\", \\'  \\\\n  oo 15.3 \\\\n15.4 \\\\n16. \\\\n16.1 \\\\n16.2 \\\\n16.3 \\\\n16.4 \\\\n16.5 \\\\n17. Any person who has become entitled to a Share or Shares in consequence of the death, \\\\nincompetence or bankruptcy of any Shareholder or Member may, instead of being registered \\\\nhimself, request in writing that some person to be named by him be registered as the \\\\ntransferee of such Share or Shares and such request shall likewise be treated as if it were a \\\\ntransfer. \\\\nWhat amounts to incompetence on the part of a person is a matter to be determined by the \\\\ncourt having regard to all the-relevant evidence and the circumstances of the case. \\\\nREDUCTION OR INCREASE IN AUTHORISED CAPITAL OR CAPITAL \\\\nThe Company may by a Resolution of Members or a Resolution of Directors amend the \\\\nMemorandum to increase or reduce its authorized capital and in connection therewith the \\\\nCompany may in respect of any unissued Shares increase or reduce the number of Shares, \\\\nincrease or reduce the par value of any Shares or effect any combination of the foregoing. \\\\nThe capital of the Company may by a Resolution of Directors be increased by transferring \\\\nan amount of the surplus of the Company to capital, and the capital of the Company may be \\\\nreduced by transferring an amount of the capital of the Company to surplus. \\\\nNo reduction of capital shall be effected that reduces the capital of the Company to an \\\\namount that immediately after the reduction is less than the aggregate par value of all \\\\noutstanding Shares with par value and all Shares with par value, held by the Company as \\\\nTreasury Shares and the aggregate of the amounts designated as capital of all outstanding \\\\nShares without par value and all Shares without par value held by the Company as Treasury \\\\nShares that are entitled to a preference, if any, in the assets of the Company upon liquidation \\\\nof the Company. \\\\nThe Directors may not reduce the capital of the Company if they determine that such \\\\nreduction would prevent the Company from being able to satisfy its liabilities as they \\\\nbecome due in the ordinary course of its business and that the realizable assets of the \\\\nCompany will be less than the sum of its total liabilities, as shown in the books of the \\\\nCompany, and its remaining issued and outstanding Share capital. In the absence of fraud, \\\\nthe decision of the Directors as to the realizable value of the assets of the Company is \\\\nconclusive, unless a question of law is involved. \\\\nWhere the Company reduces its capital the Company may \\\\n16.5.1 return to the Shareholders or Members any amount received by the Company \\\\nupon the issue of any of its Shares; \\\\n16.5.2 purchase, redeem or otherwise acquire its Shares out of capital; or \\\\n16.5.3 cancel any capital that is lost or not represented by assets having.a realizable \\\\nvalue. \\\\nMEETINGS AND CONSENTS OF MEMBERS \\\\n15 \\\\n  \\\\n \\\\n\\', \\'  \\\\n  O 17.1 \\\\n17.2 \\\\n17.3 \\\\n17.4 \\\\n17.5 \\\\n17.6 \\\\n17.7 \\\\n17.8 \\\\n17.9 The Directors of the Company may, in their discretion convene an annual general meeting \\\\nof the Members of the Company at such times and in such manner as the Directors consider \\\\nnecessary or desirable. \\\\nThe Directors of the Company may convene other meetings of the Members of the \\\\nCompany at such times and in such manner and places within or outside The Bahamas as \\\\nthe Directors consider necessary or desirable. \\\\nUpon the written request of Members holding more than fifty (50%) percent of the \\\\noutstanding Ordinary Shares in the Company the Directors shall convene a meeting of \\\\nMembers. \\\\nThe Directors shall give not less than seven (7) days notice of meetings of Members to those \\\\npersons whose names on the date the notice is given appear as Members in the share register \\\\nof the Company and are entitled to vote at the meeting. All meetings of the Members shall \\\\ntake place outside the United States. \\\\nA meeting of Members held in contravention of the requirement to give notice is valid if \\\\nMembers holding more than fifty (50%) percent of the total number of Ordinary Shares \\\\nentitled to vote on all matters to be considered at the meeting, or fifty (50%) percent of the \\\\nvotes of each Class or Series of Ordinary Shares where Members are entitled to vote thereon \\\\nas a Class or Series together with more than a fifty (50%) percent majority of the remaining \\\\nvotes, have agreed to shorter notice of the meeting, or if all Members holding Ordinary \\\\nShares entitled to vote on all or any matters to be considered at the meeting have waived \\\\nnotice of the meeting and for this purpose presence at the meeting shall be deemed to \\\\nconstitute waiver. \\\\nThe inadvertent failure of the Directors to give notice of a meeting to a Member, or the fact \\\\nthat a Member has not received notice, does not invalidate the meeting. \\\\nA Member may be represented at a meeting of Members by a proxy who may speak and \\\\nvote on behalf of the Member. \\\\nThe instrument appointing a proxy shall be produced at the place appointed for the meeting \\\\nbefore the time for holding the meeting at which the person named in such instrument \\\\nproposes to vote. \\\\nAn instrument appointing a proxy shall be in substantially the following form or such other \\\\nform as the Chairman of the meeting shall accept as properly evidencing the wishes of the \\\\nMember appointing the proxy. \\\\n \\\\n\\', \\'  \\\\n  CO) \\\\n17.9 \\\\n17.10 \\\\n17.11   (Naine of Company) \\\\nV/We \\\\nBeing a Member of the above Company with Ordinary Shares HEREBY \\\\nAPPOINT of \\\\nto be my/our proxy to vote for me/us at the meeting of Members to be held on the___day \\\\nof. 20__ and at any adjournment or adjournments thereof. \\\\n(Any restrictions on voting are to be inserted here) \\\\nSigned this day of. »20_. \\\\nMember \\\\nThe following shall apply in respect of joint ownership of Ordinary Shares:- \\\\n17.9.1 if two (2) or more persons hold Shares jointly each of them may be present in \\\\nperson or by proxy at a meeting of Members and may speak as a Member; \\\\n17.9.2 if only one (1) of the joint owners is present in person or by proxy he may \\\\nvote on behalf of al! joint owners; and \\\\n17.9.3 if two (2) or more of the joint owners are present in person or by proxy they \\\\nmust vote as one. \\\\nA Member shall be deemed to be present at a meeting of Members if he participates by \\\\ntelephone or other electronic means and all Members participating in the meeting are.able to \\\\nhear each other. \\\\nA meeting of Members is duly constituted if, at the commencement of the meeting, there are \\\\npresent in person or by proxy more than fifty percent (50%) of the votes of the Shares or \\\\nClass or Series of Shares entitled to vote on resolutions of Members to be considered at the \\\\nmeeting. If a quorum be present, notwithstanding the fact that such quorum may be \\\\nrepresented by only one person, then such person may resolve any matter and a certificate \\\\nsigned by such person accompanied where such person be a proxy by a copy of the proxy \\\\nform shall constitute a valid Resolution of Members. \\\\n \\\\n\\', \\'  \\\\n  17.12 \\\\n17.13 \\\\n17.14 \\\\n17.15 \\\\n17.16 \\\\n17.17 If within Thirty (30) minutes from the time appointed for the meeting a quorum is not \\\\npresent, the meeting, if convened upon the requisition of Members, shall be dissolved; in \\\\nany other case it shall stand adjourned to the next Business Day at the same time and place \\\\nor to such other time and place as the Directors may determine, and if at the adjourned \\\\nmeeting there are present within thirty minutes from the time appointed for the meeting in \\\\nperson or by proxy not less than one third of the votes of the Ordinary Shares or each class \\\\nor series of Ordinary Shares entitled to vote on the resolutions to be considered by the \\\\nmeeting, those present shall constitute a quorum but otherwise the meeting shall be \\\\ndissolved. \\\\nAt every meeting of Members, the Chairman of the Board of Directors shall preside as \\\\nChairman of the meeting. If there is no Chairman of the Board of Directors or if the \\\\nChairman of the Board is not present at the meeting, the Members present shall choose \\\\nsomeone of their number to be the Chairman. If the Members are unable to choose a \\\\nChairman for any reason, then the person representing the greatest number of Ordinary \\\\nShares present in person or by prescribed form of proxy at the meeting shall preside as \\\\nChairman failing which the oldest individual Member or representative of a Member present \\\\nshall take the Chair. \\\\nThe Chairman may, with the consent of the Members, adjourn any meeting from time to \\\\ntime, and from place to place, but no business shall be transacted at any adjourned meeting \\\\nother than the business left unfinished at the meeting from which the adjournment took \\\\nplace. \\\\nAt any meeting of the Members the Chairman shall be responsible for deciding in such \\\\nmanner as he shall consider appropriate whether any resolution has been carried or not and \\\\nthe result of his decision shall be announced to the meeting and recorded in the minutes \\\\nthereof. If the Chairman shall have any doubt as to the outcome of any resolution put to the \\\\nvote, he shall cause a poll to be taken of all votes cast upon such resolution, but if the \\\\nChairman shall fail to take a poll then any Member present in person or by proxy who \\\\ndisputes the announcement by the Chairman of the result of any vote may immediately \\\\nfollowing such announcement demand that a poll be taken and the Chairman shall \\\\nthereupon cause a poll to be taken. Ifa poll is taken at any meeting, the result thereof shall \\\\nbe duly recorded in the minutes of that meeting by the Chairman. \\\\nAny person other than an individual shall be regarded as one Member and subject to Article \\\\n17.7 hereof the right of any individual to speak for or represent such Member shall be \\\\ndetermined by the law of the jurisdiction where, and by the documents by which, the person \\\\nis constituted or derives its existence. In case of doubt, the Directors may in good faith seek \\\\nlegal advice from any qualified person and unless and until a court of competent jurisdiction \\\\nshall otherwise rule the Directors may rely and act upon such advice without incurring any \\\\nliability to any Member. \\\\nAny person other than an individual which is a Member of the Company may by resolution \\\\nof its Directors or other governing body authorize such person as it thinks fit to act as its \\\\nrepresentative at any meeting of the Company or of any Class of Members of the Company, \\\\nand the person so authorized shall be entitled to exercise the same powers on behalf of the \\\\nperson which he represents as that person could exercise if it were an individual Member of \\\\nthe Company. \\\\n18 \\\\n  \\\\n \\\\n\\', \"  \\\\n  17.18 \\\\n17.19 \\\\n18. \\\\n18.1 \\\\n18.2 \\\\n18.3 \\\\n18.4 \\\\n18.5 \\\\n18.6 \\\\n18.7 \\\\n18.8 \\\\n18.9 The Chairman of any meeting. at which a vote is cast by proxy or on behalf of any person \\\\nother than an individual may call for a notarized certified copy of such proxy or authority \\\\nwhich shall be produced within seven (7) days of being so requested or the votes cast by \\\\nsuch proxy or on behalf of such person shall be disregarded. \\\\nDirectors of the Company may attend and speak at any meeting of Members of the \\\\nCompany and at any separate meeting of the holders of any class or series of Ordinary \\\\nShares in the Company. \\\\nDIRECTORS \\\\nThe first Directors of the Company shall be elected by the Subscribers to the Memorandum, \\\\nand thereafter, the Directors shall be elected:- \\\\n18.1.1 by the Members for such terms as the Members determine or \\\\nUntil Directors are appointed, the Subscribers to the Memorandum of Association shall \\\\nhave the power to act as Directors. \\\\nThe minimum number of Directors shall be two (2). \\\\nEach Director shall hold office for the term, if any, fixed by a resolution of Members or \\\\nDirectors as the case may be. In the case of a director who is an individual the term of \\\\noffice of a director shall terminate on the Director\\'s death, resignation or removal. The \\\\nbankruptcy of a corporate Director shall terminate the term of office of such Director. \\\\nA Director may be removed from office, with or without cause, by a Resolution of Members \\\\nor Directors, as the case may be. \\\\nA Director may resign his office by giving written notice of his resignation to the Company \\\\nand the resignation shal! have effect from the date the notice is received by the Company or \\\\nfrom such later date as may be specified in the notice. \\\\nA vacancy in the Board of Directors may be filled by a Resolution of Members. \\\\nThe Company will reimburse each Director for the travel and other reasonable out-of-pocket \\\\nexpenses incurred in connection with his services. The emoluments, (if any) of the \\\\nDirectors shall be fixed by a Resolution of Directors. \\\\nA Director shall not require a share qualification, and may be an individual or a-company \\\\n \\\\n\", \\'  \\\\n  () 19. \\\\n19.1 \\\\n19.2 \\\\n19.3 \\\\n19.4 \\\\n19.5 \\\\n19.6 \\\\n19.7 \\\\n19.8 POWER OF DIRECTORS \\\\nThe business and affairs of the Company shall be managed by the Directors who may pay all \\\\nexpenses incurred preliminary to and in connection with the formation and registration of \\\\nthe Company and may exercise all such powers of the Company as are not by the Act or by \\\\nthe Memorandum or these Articles required to be exercised by the Members of the \\\\nCompany, subject to any delegation of such powers as may be authorised by these Articles \\\\nand to such requirement as may be prescribed by a Resolution of Members; but no \\\\nrequirement made by a Resolution of Members shall prevail if it be inconsistent with these \\\\nArticles nor shall such requirement invalidate any prior act of the Directors which would \\\\nhave been valid if such requirement had not been made. The Company shall be represented \\\\nin all matters either; \\\\n(i) by any two Directors acting jointly; or \\\\n(ii) by any of the duly appointed and empowered attorneys-in-fact acting in the manner \\\\ndetermined from time to time by the Company. \\\\nThe Directors may, by a Resolution of Directors, appoint any person, including a person \\\\nwho is a director, to be an officer or agent of the Company, and without limiting the \\\\ngenerality of the foregoing, appoint Investment Advisor(s), Manager(s), Administrator(s), \\\\nDistributor(s), Custodian(s), Registrar(s) and Transfer Agent(s) upon such terms and \\\\nconditions as the Directors, in their absolute discretion, shall determine. \\\\nEvery officer or agent of the Company has such powers and authority of the Directors, \\\\nincluding the power and authority to affix the Seal, as are set forth in these Articles or in the \\\\nresolution of Directors appointing the officer or agent, except that no officer or agent has \\\\nany power or authority with respect to fixing the emoluments of Directors. \\\\nAny Director which is a body corporate may appoint any person its duly authorized \\\\nrepresentative for the purpose of representing it at meetings of the Board of Directors or \\\\nwith respect to unanimous written consents. \\\\nThe continuing Directors may act notwithstanding any vacancy in their body, save that if \\\\ntheir number is reduced below the number fixed by or pursuant to these Articles as the \\\\nnecessary quorum for a meeting of Directors, the continuing directors or director may \\\\nappoint directors to fill any vacancy that has arisen or to summons a meeting of Members. \\\\nNotwithstanding Article 11.4 hereof the Directors may exercise all the powers of the \\\\nCompany to borrow money, and to mortgage or charge its undertaking, property and \\\\nuncalled capital, and to issue debentures and other securities, whether outright or as \\\\ncollateral security for any debt, liability or obligation of the Company or of any third party. \\\\nAll cheques, promissory notes, drafts, bills of exchange and other negotiable instruments \\\\nand all receipts for monies paid to the Company, shall be signed, drawn, accepted, endorsed \\\\nor otherwise executed, as the case may be, in such manner as shall from time to time be \\\\ndetermined by Resolution of Directors. \\\\nThe Directors may at any time exercise the powers conferred upon them by Section 10 (i) of \\\\nthe Act. \\\\n20 \\\\n \\\\n\\', \\'  \\\\n  C) 19.9 \\\\n19.10 Subject to the provisions of the Memorandum of Association of the Company and these \\\\nArticles, the Directors may at any time amend its Constitutive Documents (which include \\\\nthe Memorandum and Articles of Association and all Material Agreements of the Company) \\\\nby a Resolution of Directors without the approval of the Members if the Directors certify in \\\\nwriting that they are satisfied that the modification or addition either \\\\n(a) does not materially prejudice the interests of the Shareholders, or \\\\n(b) does not operate to release to any material extent the Company or the \\\\nDirectors or any other person from any responsibility to the Shareholders, or \\\\n(c) is necessary for compliance with any fiscal,,statutory or official \\\\nrequirements, or \\\\n(d) is made to correct a manifest error. \\\\n19.9.1 Any other modification or supplement requires, in addition, the approval of \\\\nthe Members by a special or extraordinary resolution. No modification or \\\\naddition may impose on any Shareholder or Member any obligation to make \\\\na further payment or to accept any liability in respect of his Preferred Shares \\\\nor Ordinary Shares as the case may be. \\\\n19.9.2 The Directors of the Company may amend the Offering Memorandum of \\\\nthe Company at any time by a Resolution of Directors; provided that any \\\\namendment to the investment objectives or restrictions of the Company as \\\\nset out in the Offering Memorandum and in Article 29 below, requires the \\\\nptior approval of a Resolution of Shareholders by a two-thirds (2/3) \\\\nmajority of the issued Preferred Shares of the Company \\\\nAny determination made in good faith by or pursuant to the direction of the Board of \\\\nDirectors, as to the amount of the assets, debts, obligations or liabilities of the Company, as \\\\nto the amount of any reserves or charges set up and the propriety thereof, as to the time of or \\\\npurpose for creating such reserves or charges, as to the use, alteration or cancellation of any \\\\nreserves or charges (whether or not any debt, obligation or liability) for which such reserves \\\\nor charges shall have been created shall have been paid or discharged or shall be then or \\\\nthereafter required to be paid or discharged, as to the value of or the method of valuing any \\\\ninvestment or other asset owned or held by the Company, as to the allocation of any asset or \\\\nliability of the Company to a particular class or series of Shares, as to the number of Shares \\\\nof any class or series of Shares outstanding, as to the estimated expense to the Company in \\\\nconnection with purchases of its Shares, as to the ability to liquidate investments in orderly \\\\nfashion, or as to any other matters relating to the issue, sale, purchase or other acquisition or \\\\ndisposition of investments or Shares of the Company, shall be final and conclusive and shall \\\\nbe binding upon the Company and all holders of its Shares, past, present and future, and \\\\nShares of the Company are issued and sold on the condition and understanding that any and \\\\nall such determinations shall be binding as aforesaid. \\\\n21 \\\\n \\\\n\\', \\'  \\\\n  20. \\\\n20.1 \\\\n20.2 \\\\n20.3 \\\\n20.4 \\\\n20.5 \\\\n20.6 \\\\n20.7 \\\\n20.8 \\\\n20.9 PROCEEDINGS OF DIRECTORS \\\\nThe Directors of the Company or any committee thereof may meet at such times and in such \\\\nmanner and places within or outside of The Bahamas as the Directors may determine to be \\\\nnecessary or desirable. \\\\nA Director shall be deemed to be present at a meeting of Directors if he participates by \\\\ntelephone or other electronic means and all Directors participating in the meeting are able to \\\\nhear each other. \\\\nA Director shall be given not less than three (3) days’ notice of meetings. of Directors, but a \\\\nmeeting of Directors held without three (3) days’ notice having been given to all Directors \\\\nshall be valid if all the Directors entitled to vote at the meeting waive notice of the meeting; \\\\nand for this purpose, the presence of a Director at the meeting shall be deemed to constitute \\\\nwaiver on his part. The inadvertent failure to give notice of a meeting to a Director, or the \\\\nfact that a Director has not received the notice, does not invalidate the meeting. \\\\nA Director may by a written instrument appoint an alternate who need not be a Director and \\\\nan alternate is entitled to attend meetings in the absence of the Director who appointed him \\\\nand to vote or consent in place of the Director. \\\\nA meeting of Directors is duly constituted for all purposes if at the commencement of the \\\\nmeeting there are present in person or by alternate not less than one-half (1/2) of the total \\\\nnumber of Directors, unless there are-only two (2) Directors in which case the quorum shall \\\\nbe two (2). \\\\nAt every meeting of the Directors, the Chairman of the Board of Ditectors shall preside as \\\\nChairman of the Meeting. If there is no Chairman of the Board of Directors or if the \\\\nChairman of the Board of Directors is not present at the meeting, the Vice Chairman of the \\\\nBoard of Directors shall preside. If there is no Vice Chairman of the Board of Directors or if \\\\nthe Vice Chairman of the Board of Directors is not present at the meeting, the Directors \\\\npresent shall choose someone of their number to be Chairman of the meeting. \\\\nThe Company shall keep minutes of all meetings of directors, members, committees of \\\\ndirectors, committees of officers and committees of members, and copies of all resolutions \\\\nconsented to by directors, members, committees of directors, committees of officers and \\\\ncommittees of members. \\\\nThe Company shall duly comply with the provisions of the Act in regard to keeping a \\\\nRegister of Directors and Officers at its Registered Office and in regard to the filing with the \\\\nRegistrar General of The Bahamas a copy of such Register of the Directors and Officers and \\\\nany amendments thereto in accordance with the said provisions. \\\\nThe books, records and minutes required by Article 20.7 and 20.8 shall be kept at the \\\\nregistered office of the Company. \\\\n22 \\\\n \\\\n\\', \\'  \\\\n) | : 20.10 \\\\n20.11 \\\\n20.12 \\\\n21. \\\\n21.1 \\\\n21.2 \\\\n  The Directors may, by a resolution of Directors, designate one or more committees, each \\\\nconsisting of one (1) or more Directors. \\\\nEach committee of Directors has such powers and authorities of the Directors, including the \\\\npower and authority to affix the Seal, as are set forth in the resolution of the Directors \\\\nestablishing the committee, except that no committee has any power or authority either to \\\\namend the Memorandum or these Articles or with respect to the matters requiring a \\\\nresolution of Directors under Articles 18.7, 18.8 and 19.2 hereof. \\\\nThe meetings and proceedings of each committee of Directors consisting of two (2) or more \\\\nDirectors shall be governed mutatis mutandis by the provisions of these Articles regulating \\\\nthe proceedings of Directors so far as the same are not superseded by any provisions in the \\\\nresolution establishing the committee. \\\\nOFFICERS \\\\nThe Company may by resolution of Directors appoint officers of the Company at such times \\\\nas shall be considered necessary or expedient. Such officers may consist of a Chairman of \\\\nthe Board of Directors, a Vice Chairman of the Board of Directors, a President and one or \\\\nmore Vice Presidents, Secretaries and Treasurers and such other officers as may from time to \\\\ntime be deemed desirable. Any number of offices may be held by the same person. \\\\nThe officers shall perform such duties as shall be prescribed at the time of their appointment \\\\nsubject to any modification in such duties as may be prescribed thereafter by resolution of \\\\nDirectors or resolution of Members, but in the absence of any specific allocation of duties, it \\\\nshall be the responsibility of the Chairman of the Board of Directors to: preside at meetings of \\\\nDirectors and Members; the Vice Chairman to act in the absence of the Chairman; the \\\\nPresident to manage the day-to-day affairs of the Company; the Vice Presidents to act in \\\\norder of seniority in the absence of the President but otherwise to perform such duties as may \\\\nbe delegated to them by the President; the Secretaries to maintain the share register, minute \\\\nbooks and records (other than financial records) of the Company and to ensure compliance \\\\nwith all procedural requirements imposed on the Company by applicable law, and the \\\\nTreasurer to be responsible for the financial affairs of the Company. \\\\nThe-emoluments of all officers, if any, shall be fixed by Resolution of Directors. \\\\nThe officers of the Company shall hold office until their successors are duly elected and \\\\nqualified, but any officer elected or appointed by the Directors may be removed at any time, \\\\nwith or without cause, by Resolution of Directors. Any vacancy occurring in any office of \\\\nthe Company may be filled by Resolution of Directors. \\\\n23 \\\\n \\\\n\\', \\'  \\\\n  22. \\\\n22.1 \\\\n22.2 \\\\n23.2 \\\\n23.3 \\\\n24. CONFLICT OF INTERESTS \\\\nNo agreement or transaction between the Company and one or more of its Directors or any \\\\nperson in which any Director has a financial interest or to whom any Director is related, \\\\nincluding as a Director of that other person, is void or voidable for this reason only or by \\\\nreason only that the Director is present at the meeting of Directors or at the meeting of the \\\\ncommittee of Directors that approves the agreement or transaction or that the vote or consent \\\\nof the Director is counted for that purpose (i) if the material facts of the interest of each \\\\nDirector in the agreement or transaction and his interest in or relationship to any other party \\\\nto the agreement or transaction are disclosed in good faith or are known by the other \\\\nDirectors and (ii) the agreement or transaction is approved or ratified by a resolution of \\\\nMembers. \\\\nA Director who has an interest in any particular business to be considered at a meeting of the \\\\nDirectors or Members may be counted for purposes of determining whether the meeting is \\\\nduly constituted. \\\\nINDEMNIFICATION AND INSURANCE \\\\nSubject to Article 23.2 hereof the Company shall indemnify against all expenses, including \\\\nlegal fees, and against all judgements, fines and amounts paid in settlement and reasonably \\\\nincurred in connection with legal, administrative or investigative proceedings any person \\\\nwho \\\\n23.1.1 is or was a party or is threatened to be made a party to any threatened, \\\\npending or completed proceedings, whether civil, criminal, administrative or \\\\ninvestigative, by reason of the fact that the person is or was a Director, an \\\\nofficer or a liquidator of the Company; or \\\\n23.1.2 is or was, at the request of the Company, serving as a Director, officer or \\\\nliquidator of, or in any other capacity is or was acting for, another company * \\\\nor a partnership, joint venture, trust or other enterprise. \\\\nArticle 23.1 hereof only applies to a person referred to in that Article if the person acted \\\\nhonestly and in good faith with a view to the best interests of the Company and, in the case \\\\nof criminal proceedings, the person had no reasonable cause to believe that his conduct was \\\\nunlawful. \\\\nThe Company may purchase and maintain insurance in relation to any person who is or was a \\\\nDirector, an officer or a liquidator of the Company, or who at the request of the Company is \\\\nor was serving as a Director, an officer or a liquidator of, or in any other capacity is or was \\\\nacting for, another company or a partnership, joint venture, trust or other enterprise, against \\\\nany liability asserted against the person and incurred by the person in that capacity, whether \\\\nor not the Company has or would have had the power to indemnify the person against the \\\\nliability under Article 23.1 hereof. \\\\nLIMITATION OF LIABILITY \\\\n24 \\\\n \\\\n\\', \\'  \\\\n  ) ( \\\\n24.1 \\\\n24.2 \\\\n24.3 \\\\n24.4 \\\\n25. \\\\n25.1 \\\\n26. \\\\n26.1 \\\\n27. \\\\n27.1 \\\\n27.2 \\\\n27.3 No Director, Officer, Liquidator or Agent of the Company shall be liable to the Company or \\\\nits Shareholders or Members for any mistake in judgment or any other event whatsoever \\\\nprovided that no such Director, Officer, Liquidator or Agent shall be protected against any \\\\nliability to the Company or its Shareholders or Members to which they would otherwise be \\\\nsubject by reason of willful default or bad faith on their part in the performance of their \\\\nduties or from-reckless disregard by them of their duties and obligations. \\\\nNo Director, Officer, Liquidator or Agent of the Company shall be liable to the Company or \\\\nits Shareholders or Members for any losses.due to the mistakes, negligence, misconduct or \\\\nbad faith of any broker or other agent of the Company selected by any of them with \\\\nreasonable care. \\\\nAny Director, Officer, Liquidator or Agent of the Company may consult with legal counsel \\\\nor accountants, selected by any of them, and any act or omission by any of them on behalf of \\\\nthe Company or in furtherance of the business of the Company in good faith on and in \\\\naccordance with the advice of such counsel or accountants will be full justification for the act \\\\nor omission and such Director, Officer, Liquidator or Agent of the Company will be fully \\\\nprotected in so acting or omitting to act if the counsel or accountants were selected with \\\\nreasonable care. \\\\nNothing in the material contracts, these Articles or the Memorandum of Association may \\\\nprovide that the Directors, custodian, administrator, investment manager or investment \\\\nadvisor shall be exempted from any liability to the Shareholders imposed under the laws of \\\\nThe Bahamas in the case of wilful default. \\\\nSEAL \\\\nThe Directors shall provide for the safe custody of the Seal. An imprint of the Seal shall be \\\\nkept at the registered office of the Company. The Seal when affixed to any written \\\\ninstrument shall be witnessed by a Director or any other person so authorized from time to \\\\ntime by resolution of Directors. The Directors may provide for a facsimile of the Seal and of \\\\nthe signature of any Director or authorized person which may be reproduced by printing or \\\\nother means on any instrument and it shall have the same force and validity as if the Seal had \\\\nbeen affixed to such instrument and the:same had been signed as hereinbefore described. \\\\nACCOUNTS \\\\nThe Company shall keep such accounts and records as the Directors consider necessary or \\\\ndesirable in order to reflect the financial position of the: Company. \\\\nAUDIT \\\\nThe Company may by resolution of the Directors or the Members call for the accounts to be \\\\nexamined by external auditors. \\\\nThe auditors shall be appointed by a Resolution of Directors. \\\\nThe auditors may be Members of the Company but no Director or other officer shall be \\\\neligible to be an auditor of the Company during his continuance in office. \\\\n25 wo \\\\n  \\\\n \\\\n\\', \"  \\\\n  C) \\\\n27.4 \\\\n27.5 \\\\n27.6 \\\\n27.7 \\\\n27.8 \\\\n28. The remuneration of the auditors of the Company \\\\n27.4.1 in the case of auditors appointed by the Directors, may be fixed by a \\\\nResolution of Directors \\\\n27.4.2 subject to the foregoing, shall be fixed by Resolution of Members or in such \\\\nmanner as the Company may by Resolution of Members determine. \\\\nThe auditors shall examine each profit and loss account and balance sheet required to be \\\\nserved on every Member of the Company or laid before a meeting of the Members of the \\\\nCompany and shall state in a written report whether or not \\\\n27.5.1 in their opinion the profit and loss account and balance sheet have been \\\\nprepared in accordance with international accounting standards and in a \\\\nmanner which is consistent with the prior year\\'s presentation; and \\\\n27.5.2 all the information and explanations required by the auditors have been \\\\nobtained. \\\\nThe report of the auditors shal) be annexed to the accounts and shall be read at the meeting of \\\\nMembers at which the accounts are laid before the Company or shall be served on the \\\\nMembers. \\\\nEvery auditor of the Company shall have a right of access at all times to the books of account \\\\nand vouchers of the Company, and shall be entitled to require from the Directors and officers \\\\nof the Company such information and explanations as he thinks necessary for the \\\\nperformance of the duties of the auditors. \\\\nThe auditors of the Company shall be entitled to receive notice of, and to attend any meetings \\\\nof Members of the Company at which the Company\\'s profit and loss account and balance \\\\nsheet are to be presented. \\\\nPRELIMINARY EXPENSES \\\\n26 \\\\n \\\\n\", \\'  \\\\n  28.1 \\\\n29 \\\\n30. \\\\n30.1 \\\\n30.2 \\\\n30.3 Subject to the provisions of the Offering Memorandum, the preliminary expenses incurred in \\\\nconnection with forming the Company and with the initial issue of its Preferred Shares shall \\\\nbe paid by the Company and shall be amortized to expense in equal monthly amounts over a \\\\nperiod not to exceed two (2) years, beginning with the commencement of trading by the \\\\nCompany. \\\\nINVESTMENT RESTRICTIONS \\\\nThe Company may not: \\\\n29.1 Lend money to other persons except through the purchase of securities and the \\\\nuse of repurchase agreements, in each case in a manner consistent with the Company’s \\\\ninvestment objective, policies and restrictions; \\\\n29.2. Enter into any transactions with the investment managers, except for those \\\\ntransactions which are contemplated by the investment management agreements, or with \\\\neither investment managers; \\\\n29.3. Issue guarantees in favor of third parties (other than as part of a trading \\\\ntransaction); \\\\n29. 4. Lend its securities without receiving adequate compensation therefore;-or \\\\n29.5. Purchase real estate. \\\\nNOTICES \\\\nAny notice, information or written statement to be given by the Company to Shareholders or \\\\nMembers must be served by mail addressed to each Shareholder or Member at the address \\\\nshown in the share register. \\\\nAny summons, notice, order, document, process, information or written statement to be \\\\nserved on the Company may be served by leaving it with, or by sending it by registered mail \\\\nto, the registered agent of the Company. \\\\nService of any summons, notice, order, document, process, information or written statement \\\\nto be served on the Company may be proved by showing that the summons, notice, order, \\\\ndocument, process, information or written statement was mailed in such time as to admit to \\\\nits being delivered in the normal course of delivery within the period prescribed for service \\\\nand was correctly addressed and the postage was prepaid. \\\\n27 \\\\n \\\\n\\', \"  \\\\n  30.5 \\\\n31.2 \\\\n32. \\\\n32.1 \\\\n32.2 The audited annual report of the Company will be mailed to Shareholders at their registered \\\\naddress not later than four (4) months after the end of each fiscal year. Subject to the terms \\\\nof the Offering Memorandum or a Resolution of the Directors the Company\\'s fiscal year will \\\\nbe the twelve (12) month period ending on 315 December of each year. \\\\nThe Administrator shall make available copies of the following documents which shall also \\\\nbe made available for inspection free of charge or purchased at a reasonable fee between the \\\\nhours of 9:30 am. and 4:00 p.m. on any Business Day at the registered office of the \\\\nCompany, 1 Floor, King’s Court, Bay Street, P.O. Box N-3944, Nassau, Bahamas. \\\\n(a) The Memorandum of Association, \\\\n(b) The Articles of Association, \\\\n(c) The Offering Memorandum and any supplemental addenda, \\\\n(d) The material contracts referred to in the Offering Memorandum. \\\\nARBITRATION \\\\nWhenever any difference arises between the Company on the one hand and any of the \\\\nShareholders or Members or their executors, administrators or assigns on the other hand, \\\\ntouching the true intent and construction or the incidence or consequences of these Articles \\\\nor of the act, touching anything done or executed, omitted or suffered in pursuance of the Act \\\\nor touching any breach or alleged breach or otherwise relating to the premises or to these \\\\nArticles, or to any Act or Statute affecting the Company or to any of the affairs of the \\\\nCompany such difference shall, unless the parties agree to refer the same to a single \\\\narbitrator, be referred to two (2) arbitrators one to be chosen by each of the parties to the \\\\ndifference and the-arbitrators shall before entering-on the reference appoint an umpire. \\\\nIf either party to the reference makes default in appointing an arbitrator either originally or by \\\\nway of substitution (in the event that an appointed arbitrator shall die; be incapable of acting \\\\nor refuse to act) for ten (10) days after the other party has given him notice to appoint the \\\\nsame, such other party may appoint an arbitrator to act in the place of the arbitrator of the \\\\ndefaulting party. \\\\nWINDING UP AND DISSOLUTION \\\\nThe Company, whether or not it has issued Shares, may voluntarily commence to wind up \\\\nand dissolve by a Resolution of Members or Directors. \\\\nIf the Company shall be wound-up (whether the liquidation is voluntary, under supervision, \\\\n28 \\\\n  \\\\n \\\\n\", \\'  \\\\n  CO) \\\\n32.3 \\\\n34. \\\\n34.1 or by the Court) the holders of a Class or Series of the Company’s Shares shall be-entitled to \\\\nreceive, as a class, out of the assets of the Company available for distribution to \\\\nShareholders, the assets belonging to that Class or Series less the liabilities allocated to that \\\\nClass or Series. The assets so distributable to the Shareholders of a Class or Series shall be \\\\ndistributed among such Shareholders in proportion to the number of Shares of that Class or \\\\nSeries held by them and recorded in the share register of the Company. In the event that \\\\nthere are any assets available for distribution that are not attributable to any particular Class \\\\nor Series of Shares, such assets shall be allocated to al] Classes and Series in proportion to \\\\nthe net asset value of the respective Classes and Series and distributed to the holders of \\\\nShares of each Class or Series in proportion to the net asset value of the Shares of that Class \\\\nor Series held by the respective holders. The liquidation of a particular Class or Series may \\\\nbe accomplished, in whole or in part, by the transfer of assets of such Class or Series to \\\\nanother Class or Series and/or by the exchange of Shares of such Class or Series for the \\\\nShares of another Class or Series. \\\\nIf the Shareholders or Directors shall resolve to wind up the Company, the Shareholders may, \\\\nby resolution approved at a meeting called for that purpose by at least seventy-five percent \\\\n(75%) of the votes of all outstanding Shares entitled to vote thereon in the aggregate or as a \\\\nClass or Series, or, alternatively, the Directors may by Resolution, approve the transfer of all \\\\nor any part of the assets of the Company or such Class or Series, as the case may be, to \\\\nanother company, whether or not organized under the laws of The Bahamas, and may also \\\\nresolve to accept the exchange of assets of the Company, or such Class or Series, as the case \\\\nmay be, for Shares or similar interests in the transferee company. \\\\nGOVERNING LAW \\\\nThese Articles and the Memorandum of Association of the Company shall be governed in \\\\naccordance with the laws of the Commonwealth of The Bahamas. \\\\nCONTINUATION \\\\nThe Company may by Resolution of Members or by Resolution of Directors continue as a \\\\ncompany incorporated under the laws of a jurisdiction outside The Bahamas in the manner \\\\nprovided under those laws or may convert the Company to a closed-end Company in The \\\\nBahamas or any other jurisdiction. \\\\n29 \\\\n  \\\\n \\\\n\\', \\'  \\\\n  The undersigned for the purpose of incorporating an International Business Company under the laws of the \\\\nCommonwealth of the Bahamas hereby\\\\\\'subscribe our names to the Articles of Association this 28\" day March 2007*: \\\\n*As amended & restated this 16\" day of August 2007. \\\\nSUBSCRIBERS \\\\nTRIDENT CORPORATBSERVICES (BAHAMAS LIMITED FREGO)-CORPORATION \\\\nThe above subscribers \\\\nSigned in the.presence of: \\\\n     \\\\n(Witness) “ \\\\n31 \\\\n \\\\n\\']',\n",
       " '[\\'  \\\\n  \\\\n-f \\\\nCommonwealth of The Bahamas \\\\nTHE INTERNATIONAL BUSINESS COMPANIES ACT, 2000 \\\\nCOMPANY LIMITED BY SHARES \\\\nNo. 69,725 B \\\\nAmended and Restated Memorandum and Articles \\\\nof \\\\nAssociation \\\\nof \\\\nAEGIS FUND, LTD. SAC \\\\n  \\\\nIncorporated the 17\" day of December, A.D., 1997 \\\\n \\\\n\\', \\'  \\\\n  \\\\n“WO COMMONWEALTH OF THE BAHAMAS \\\\nTHE INTERNATIONAL BUSINESS COMPANIES ACT 2000 \\\\n(No. 45 of 2000) \\\\n(As Amended) \\\\nAMENDED AND RESTATED \\\\nMEMORANDUM OF ASSOCIATION \\\\nOF \\\\nAEGIS FUND, LTD. SAC \\\\n  \\\\nNAME \\\\nThe name of the Company is Aegis Fund, Ltd. SAC \\\\nREGISTERED OFFICE \\\\nThe Registered Office of the Company will be situated at Suite 6, BayParl Building, \\\\nBay & Parliament Streets, Nassau, Bahamas. \\\\nREGISTERED AGENT \\\\nThe Registered Agent of the Company will be M. Rudolph Smith, Jr. & Co. \\\\nOBJECTS AND POWERS \\\\n4.1 The objects for which the Company is established are to engage in any act or \\\\nactivity that is not prohibited under any law for the time being in force in The \\\\nBahamas. Without limitation to the generality of the foregoing, the objects of \\\\nthe Company include the establishment of one or more investment funds \\\\nthrough the creation of sub-funds each representing a Segregated Account as \\\\nsuch term is defined in the Segregated Accounts Companies Act, 2004. \\\\n42 THE COMPANY.SHALL NOT: \\\\n(a) carry on business with persons resident in The Bahamas, except as \\\\npermitted by the International Business Companies Act 2000, as \\\\namended (“the Act”); \\\\n(b) carry on banking or trust business, \\\\n(c) carry on business as an insurance or a reinsurance company; \\\\n \\\\n\\', \\'  \\\\n  2 \\\\n(d) carry on the business-of providing the registered office for companies; or \\\\n(e) carry on the business of “dealing” or “trading” in securities as an agent \\\\nor providing “securities investment advice”. \\\\n4.3. The Company shall have all such powers as are permitted by law for the time \\\\nbeing in force in The Bahamas, irrespective of corporate benefit, to perform all \\\\nacts and engage in all activities necessary or conducive to the conduct, \\\\npromotion or attainment of the, objects of the Company. \\\\n44 The Directors may by resolution exercise all the powers of the Company to \\\\nborrow money and to mortgage or charge its undertaking and property-or any \\\\npart thereof, to issue debentures, debenture stock and other securities whenever \\\\nmoney is borrowed or as security for any debt, liability or obligation of the \\\\ncompany or of any third party. \\\\n4.5 Any mortgage or charge of the undertaking and property of the Company shall \\\\nfor the purposes of Section 80 of the Act be, regarded as in the usual or regular \\\\ncourse of the business carried on by the Company. \\\\nAUTHORISED CAPITAL \\\\nThe authorized share capital of the Fund is Five hundred thousand two hundred ninety \\\\none and eighty-eight hundredths Euros €511,291.88 consisting of 250,000 non-voting \\\\nparticipating redeemable shares denominated as \"Common Investor Shares C\" issued in \\\\nrespect of Sub-Fund C and having a par value of One-Hundredth (1/100) of a Euro \\\\n(€0:01) each; 500,000 non-voting participating redeemable shares denominated as \\\\n\"Common Investor Shares CL\" issued in respect of Sub-Fund CL and having a par \\\\nvalue of One-Hundredth. (1/100) of a Euro (€0.01) each; 500,000 non-voting \\\\nparticipating redeemable shares denominated as \"Common Investor Shares I\" issued in \\\\nrespect of Sub-Fund J and having a par value of One-Hundredth (1/100) of a Euro \\\\n(€0.01) each; 500,000 non-voting participating redeemable shares denominated as \\\\n“Common Investor Shares X\" issued in respect of Sub-Fund X and having a par value \\\\nof One-Hundredth (1/100) of a Euro (€0.01) each; 49,379,188 non-voting participating \\\\nredeemable shares un-denominated and having a par value of One-Hundredth (1/100) of \\\\na Euro (€0.01) each and One Hundred (100) non-participating non-redeemable \\\\nCommon Shares of par value €1.00 each (the \"Management Shares\") with voting rights. \\\\n5.1 The Company is hereby authorised to issue fractions of a Share as the \\\\nDirectors may by resolution determine and each such fractional Share shall \\\\nhave the corresponding fractional designations, powers, preferences, rights, \\\\nqualifications and limitations of a Share of the same Class or Series of Shares. \\\\n5.2 The Company is hereby authorised to issue such Classes or series of Shares, \\\\nand may subdivide any Class of Shares, into such Series of Shares, in each \\\\ncase as the Directors may by resolution determine, which resolutions shall (i) \\\\nfix the designations, powers, preferences, rights, qualifications and limitations \\\\n \\\\n\\', \\'  \\\\n  OS \\\\n6.1 \\\\n6.2 3 \\\\nof such additional Classes or Series of Shares and (ii) set forth the investment \\\\nobjective of any investment funds represented by such Classes or Series of \\\\nShares. \\\\n5.2.1 The provisions of this Memorandum, including those in this Clause 5, \\\\nand in the Articles of Association of the Company shal! apply to each \\\\nClass or Series of Shares of any Segregated Account unless otherwise \\\\nprovided by the Directors prior to issuance of any Shares of that Class \\\\nor Series: \\\\n5.2.1 (a) As more fully set forth hereafter, the assets of each Class \\\\nor Series of Shares or Segregated Accounts shall be \\\\ndetermined separately and, accordingly, the net asset \\\\nvalue, the dividends payable to holders, the amounts \\\\ndistributable in the event of dissolution of the Company \\\\nto holders and the voting powers of Shares may vary \\\\nfrom Class to Class and Series to Series. \\\\n5.2.1 (b) All consideration received by the Company for the issue \\\\nor sale of Shares of a Class or Series of Shares or Shares \\\\nin any Segregated Account, together with all funds \\\\nderived from any investment and reinvestment thereof, \\\\nshall irrevocably belong to that Class or Series or \\\\nSegregated Account for all purposes, subject only to the \\\\nrights of creditors a particular Segregated Account, and \\\\nshall be so recorded upon the books of account of the \\\\nrelevant Segregated Account. Such consideration and \\\\nany funds derived from any investment and reinvestment \\\\nare herein referred to as “assets belonging to” that Class \\\\nor Series or Segregated Account. \\\\n5.2.1 (c) The assets belonging to a Segregated Account shall be \\\\ncharged only with the liabilities of that Segregated \\\\nAccount. \\\\nPROVIDED THAT the Directors may at anytime exercise the powers \\\\nconferred upon them by Section 11 (i) of the Act to give further effect to this \\\\nClause 5. \\\\nSHARE RIGHTS AND LIMITATIONS \\\\nVOTING \\\\nThe holders of the Management Shares (the \"members\") shall be entitled to \\\\nattend and vote at all General Meetings and to take any action by written \\\\nresolution of members. \\\\nSubject to the provisions of Clause 10.1 hereof, the holders of Common Shares \\\\n \\\\n\\', \\'  \\\\n  \\\\n‘© 4 \\\\n(“the Shareholders\") shall not be entitled to attend and vote at any General \\\\nMeetings or, subject to the provisions of the Articles of Association, to take \\\\nany action by written resolution of Shareholders. \\\\nMINIMUM PURCHASE \\\\nThe minimum initial subscription for Common Shares shall be as described in a \\\\nPrivate Placement Memorandum dated December 19, 2006 or as amended from time \\\\nto time (“the Private Placement Memorandum”). Thereafter, the purchase price of \\\\neach Share shall be the prevailing Net Asset Value per Share of the relevant Sub-Fund, \\\\nsubject to such other amounts as may be stipulated in the Private Placement \\\\nMemorandum or as may be determined by a Resolution of Directors. Subsequent \\\\npurchases or transfers shall be determined by a Resolution of Directors. Shareholders \\\\nwishing to exchange into a different Sub-Fund will be required to redeem their Shares, \\\\nconvert the proceeds and re-subscribe for Shares of a different Sub-Fund at the \\\\nprevailing Net Asset Value per Share. \\\\nDIVIDENDS AND DISTRIBUTION \\\\n8.1 The Directors from time to time and at their discretion may declare and pay \\\\ndividends to the holders of any class or series of Shares in amounts that the Directors in \\\\ntheir discretion deem appropriate but shall be under no obligation to do so. Dividends \\\\nshall be paid on Shares only out of lawfully available assets belonging to that class or \\\\nseries. If the Company shall have Segregated Accounts, dividends if and when declared \\\\nshall only be payable to holders of Shares in that Segregated Account to the exclusion \\\\nof the holders of Shares in any other Segregated Account. \\\\n8.2 If the Company shall be wound up (whether the liquidation is voluntary, under \\\\nsupervision, or by the Court), the Shareholders shall only be entitled to participate in \\\\nthe assets of the Segregated Account in which they hold Shares. The assets so \\\\ndistributable shall be distributed among such Shareholders in proportion to the number \\\\nof Shares held by them and recorded in the share register of that Segregated Account. \\\\nREDEMPTION OF SHARES \\\\nSubject to the provisions of Articles 11 and 13 of the Articles of Association of the \\\\nCompany and the Offering Memorandum (as may be validly amended from time to \\\\ntime), the Shareholders shall have the right to request redemption of all or part of their \\\\nShares, by notice in a form acceptable to the Company, provided such notice is \\\\nreceived by the Administrator thirty (30) Business Days prior to the relevant \\\\nRedemption Date or any other such time limit as may be stipulated in the Offering \\\\nMemorandum or determined by a resolution of the Directors. Shares may be \\\\ncompulsorily redeemed at the direction of the Directors in cases where continuation of \\\\ninvestment would have an adverse impact on the tax or legal status of the Company or \\\\n \\\\n\\', \\'  \\\\n  @ \\\\n10. \\\\n11. \\\\n12. 5 \\\\ninvestment would have an adverse impact on the tax or legal status of the Company or Investment Manager or where the Company or Investment Manager would be subject \\\\nto further or additional legal reporting or operational compliance or violation of law. \\\\nVARIATION OF CLASS RIGHTS \\\\n10.1 If at any time the authorised capital is divided into different Classes or Series of Shares, the right attached to any Class or Series (unless otherwise provided \\\\nby the terms of issue of the Shares of that Class or Series) may, whether or not the Company is being wound up, be varied with the consent in writing of the holders of not less than three-fourths (3/4) of the issued Shares of that Class or Series and the holders of not less than three-fourths (3/4) of the issued Shares of any other Class or Series of Shares which may be affected by such variation. \\\\n10.2. The rights conferred upon the Shareholders of any Class issued with preferred \\\\nor other rights shall not, unless otherwise expressly provided by the terms of issue of the Shares of that Class be deemed to be varied by the creation or issue of further Shares ranking pari passu therewith. \\\\nREGISTERED SHARES AND RESTRICTION ON SALE OR TRANSFER \\\\n11.1 Shares shall be issued as registered shares.only. \\\\n11.2 Shares may not be issued or transferred to any Restricted Person. Shares are \\\\ntransferable only with the prior written consent of the Directors and at the \\\\ndiscretion of the Directors of the Company. The Company or its agent has the \\\\nright to determine conclusively whether any person or entity is a Restricted \\\\nPerson. The Company or its agent may determine that a person or entity is a Restricted Person after such person or entity has purchased Shares. \\\\n11.3 If it shall come to the attention of the Company or its agent at any time that any \\\\nShares are beneficially owned by a Restricted Person, either alone or in conjunction with any other person, the Company may compulsorily redeem such Shares at their Net Asset Value per Share and the Restricted Person(s) will cease to be the owner(s) of those Shares. \\\\nAMENDMENT OF MEMORANDUM AND ARTICLES OF ASSOCIATION \\\\nAND OFFERING MEMORANDUM \\\\n12.1 Subject to the provisions of this Memorandum and the Articles of Association \\\\nof the Company, the Company may at any time amend its Constitutive \\\\nDocuments (defined in the Act to include the Memorandum and Articles of Association and Material Agreements) by a resolution of Directors without the approval of the Members if the Directors certify in writing that they are \\\\nsatisfied that the amendment either: \\\\n  \\\\n \\\\n\\', \"  \\\\n  12.2 \\\\n12.3 \\\\n13.1. \\\\n13.2. (a) \\\\n(b) \\\\n(c) \\\\n(d) \\\\n(f) \\\\n(g) does not materially prejudice the interest of the Shareholders, \\\\ndoes not operate to release to any material extent the Company or the \\\\nDirectors the Custodian, Administrator, Investment Manager or any \\\\nother person or any other persons from any responsibility to the \\\\nShareholders, \\\\nis necessary for compliance with any fiscal, statutory or official \\\\nrequirements, \\\\nis made-to correct a manifest error; or \\\\nis in the discretion of the Directors in the best interests of the Company. \\\\ndoes not materially increase the costs and charges payable from the \\\\ninvestment fund property. \\\\nExcept as permitted under Clause 12.1 hereof, no amendment of the Company\\'s \\\\nConstitutive Documents as defined above may be made except by a Special or \\\\nExtraordinary Resolution of the Members; provided, however, that no amendment \\\\nof the Company\\'s Constitutive Documents may be made without the approval of \\\\nthe Directors. \\\\nThe Directors of the Company may amend the Private Placement Memorandum of \\\\nthe Company at any time by a resolution of Directors. \\\\nDEFINITIONS \\\\nThe meanings of words in this Memorandum of Association are as defined in the \\\\nArticles of Association of the Company. \\\\nFor the avoidance of doubt, in the event of a conflict between the provisions of the \\\\nMemorandum and Articles of Association and the Private Placement Memorandum of \\\\nthe Company, the Private Placement Memorandum shall prevail. \\\\n \\\\n\", \\'  \\\\n  COMMONWEALTH OF THE BAHAMAS \\\\nTHE INTERNATIONAL BUSINESS COMPANIES ACT 2000 \\\\n(No. 45-of 2000) \\\\n(As Amended) \\\\nCD AMENDED AND RESTATED \\\\nARTICLES OF ASSOCIATION \\\\nOF \\\\nAEGIS FUND, LTD. SAC \\\\nPRELIMINARY \\\\nIn these Articles, if not inconsistent with the context, the words and expressions standing in the first column of the following table shall bear the meanings set opposite,them in the second column thereof. 1 \\\\nExpressions Meanings \\\\nLl Capital The sum of the aggregate par value of all outstanding Shares of the Company or \\\\nin the case where any Segregated Accounts have been designated any Segregated \\\\nAccount with par value and Shares of the Company or any Segregated Account ! \\\\nwith par value held-as treasury shares plus \\\\nid The aggregate of the amounts designated as capital of all outstanding Shares of \\\\nthe Company or of any Segregated Account without par value and shares of the \\\\nCompany or of any Segregated Account without par value held as treasury \\\\n\" shares, and \\\\n1.1.2 The amounts as are from time to time transferred from surplus to capital by a \\\\nresolution of Directors. \\\\n1.2 Class A Class of Shares of the Company or, in the case where.any Segregated Accounts \\\\nhave been designated any Segregated Account, each Class Tepresenting interests : \\\\nin a fund differentiated from other interests either\\\\\\'by investment objective, fee, or . \\\\nsome other manner as decided upon by the Directors. \\\\n13 Closing Day The day on which shareholdings are either added to or deleted from the share \\\\nregister. \\\\n1.4 Common Investor Shares Non-voting-participating redeemable shares. \\\\\\' \\\\n1.5 Management Shares or Voting non-participating non-redeemable:shares. \\\\nMember Shares \\\\n1.6 Material Agreements Any and all contracts entered into by the Company or, in the case where any \\\\nSegregated Accounts have been so designated, by any Segregated Account with \\\\nits service providers, including, but not limited to, its Administrator, Registrar | \\\\nand Transfer Agent, Trading Manager, Custodian, Banker or Broker.   1.7 Member A person who holds Management Shares in the Company. | \\\\n1.8 Month Calendar month. \\\\n1.9 Net Asset Value The Net Asset Value of each Share of every designated Segregated Account \\\\n1 \\\\n \\\\n\\', \\'  \\\\n  1.10 \\\\n[1 \\\\nL.12 \\\\n1.13 \\\\n1.14 \\\\n1.15 \\\\n1.16 \\\\n1.17 \\\\n1.18 \\\\n1.19 Notice \\\\nPrivate Placement \\\\nMemorandum \\\\nPerson \\\\nRedemption Date \\\\nResolution of \\\\nDirectors \\\\nResolution of \\\\nMembers \\\\nRestricted \\\\nPersons \\\\nSegregated \\\\nAccount \\\\nSegregated \\\\nAccounts \\\\nCompany (SAC) \\\\nSeries 1.14.1 \\\\n1.14.2 \\\\n1.14.3 \\\\n1.15.1 \\\\n1.15.2 \\\\n1.16.1 \\\\n1.16.2 calculated in accordance with the provisions of Article 12, or of each Share of \\\\nthe Company, in the case where there shall be no Segregated Accounts in \\\\nexistence. \\\\nWritten notice unless otherwise specifically stated. \\\\nAll constituent parts of the Company’s Offering Memorandum dated December \\\\n19, 2006 including all exhibits and appendices thereto, as the same may be \\\\namended or supplemented from time to time by the Directors. \\\\nAn individual, a corporation, a trust, the estate of a deceased individual, a \\\\npartnership or an unincorporated association of persons. \\\\nThe last business day of each month. \\\\nA resolution approved at a duly constituted meeting of Directors or of a \\\\ncommittee of Directors of the Company, by affirmative vote of a simple-majority \\\\nof the Directors present at the meeting who voted and did not abstain; or \\\\nA resolution consented to in writing by an absolute majority of all the Directors \\\\nor of all the-members of the committee, as the case may be; \\\\nwhere a Director is given more than one vote in any circumstances, he shall in the \\\\ncircumstances be counted for the, purposes of establishing majorities by the \\\\nnumber of votes he casts. \\\\nA resolution approved at a duly constituted meeting of the holders of Member \\\\nShares of the Company by the affirmative vote of: \\\\n(i) an absolute majority of the votes of the Member Shares entitled to vote \\\\nthereon and were voted:and did not abstain, or \\\\n(ii) A resolution consented to in writing by an absolute majority of the votes of \\\\nthe Member Shares’entitled to vote thereon, or \\\\nA United States Person, Bahamian Person, and any Other Person who may be \\\\nrestricted from purchasing the Shares of a given Class or Series:of Shares. \\\\nUnited States © A United States person as defined in the Rules promulgated \\\\nperson under the United States Securities Act of 1933 \\\\nBahamian A national, citizen or resident of a person normally resident in \\\\nperson the Commonwealth of The Bahamas or any corporation, \\\\npartnership, trust, estate or other entity formed or organised \\\\nunder the laws of, or existing in the Commonwealth of The \\\\nBahamas and deemed resident therein within the meaning of the \\\\nExchange Contro! Regulations made under the Exchange \\\\nContral Regulations Act, and which do not have the written \\\\npermission of the Central Bank of The Bahamas. \\\\nAs defined in Section 2(1) of the SAC Act. \\\\nA company which is registered under Section 6 of the SAC Act \\\\nEach separate tranche of any Class‘of Shares (if any) issued within a designated period. \\\\n \\\\n\\', \\'  \\\\n  \\\\n\"y 1.20 \\\\n1.2] \\\\n1,22 \\\\n1.23 \\\\n1.24 \\\\n1.25 \\\\n1.26 \\\\n1.27 \\\\n1.28 \\\\n1.29 \\\\n1.30 \\\\n1.31 \\\\n1.32 \\\\n1.33 \\\\n1,34 \\\\n1.35 Shares \\\\nShareholders \\\\nThe \\\\nMemorandum \\\\nThe {BC Act \\\\nThe SAC Act \\\\nThe Seal \\\\nSub- Fund C \\\\nSub-Fund CL \\\\nSub-Fund | \\\\nSub-Fund X \\\\nSurplus \\\\nThese Articles \\\\nTreasury Shares \\\\nUnited States or \\\\nUS. \\\\nValuation Day \\\\nYear If any Segregated Account shall havebeen so designated, the term “Share” or “Shares” when \\\\nused in these Articles shall mean the non-voting participating redeemable shares (“Common \\\\nInvestor Shares”) of the particular Segregated Account. If no Segregated Accounts shall have \\\\nbeen so designated or if all Segregated Accounts shall have been deregistered in accordance \\\\nwith these Articles, the term “Shares” when used in these Articles shall mean the holder of \\\\nCommon Investor Shares in the Company. \\\\nThe holders of Shares in any Segregated Account\\\\\\'or, if none shall have been designated, in \\\\nthe Company. \\\\nThe Memorandum of Association of the Company as originally framed or as \\\\nfrom time to time-amended. \\\\nThe Internationa! Business Companies Act, 2000 of the Commonwealth of The \\\\nBahamas as amended. \\\\nThe Segregated Accounts Companies Act, 2004 of the Commonwealth of The \\\\nBahamas. \\\\nThe Common Seal of the Company. \\\\nA Segregated Account to be designated as such pursuant to the Private \\\\nPlacement Memorandum, the capital of which shall be exclusively designated in \\\\nthe:currency of the Member States of the European Union (€ -Euro). \\\\nA Segregated Account to be designated as such pursuant to the Private \\\\nPlacement Memorandum, the capital of which shall be exclusively designated in \\\\nthe currency‘of the Member States of the European Union (€ -Euro). \\\\nA Segregated Account to be designated as such pursuant to the Private \\\\nPlacement Memorandum, the capital of which shall be-exclusively designated in \\\\nthe currency of the Member States of the European Union (€ -Euro). \\\\nA Segregated Account to be designated as such pursuant to the Private \\\\nPlacement Memorandum, the capital of which shall be exclusively designated in \\\\nthe currency of the Member States of the European Union (€ -Euro). \\\\nThe excess, if any, at the time of the determination of the total assets of each \\\\nSegregated Account (or of the Company as a whole if there shall be no \\\\nSegregated Accounts) over the total liabilities of such Segregated Account (or \\\\nthe liabilities of the Company as a whole if there shall be no Segregated \\\\nAccounts), as shown in its books of account of each Segregated Account (or the \\\\nCompany), including the Company’s issued and outstanding share capital. \\\\nThese Articles of Association as originally framed or as from time to time \\\\namended. \\\\nShares in any Segregated Account that were previously issued but were \\\\nrepurchased, redeemed or otherwise acquired by the Company and not cancelled. \\\\nThe United States of America, its territories, possessions or any area subject to \\\\nits jurisdiction. \\\\nThe last Business Day of each month or such other day as the Directors may \\\\ndesignate. \\\\nCalendar year \\\\n \\\\n\\', \\'  \\\\n  1.36 \\\\n1.37 \\\\n1.38 \\\\n2.1 \\\\n2.2 \\\\n2.3 \\\\n3.1 \\\\n3.2 \"Written\" or any term of like import includes words typewritten, printed, painted, engraved, lithographed, \\\\nphotographed or represented or reproduced by any mode of representing or reproducing words in a visible form, \\\\nincluding telex, telegram, cable or other forms of writing produced by electronic communication. \\\\nSave as aforesaid, any words or expressions defined in the IBC Act and / or the SAC Act shall bear the same \\\\nmeaning in these Articles. \\\\nA reference to money in these Articles is a reference, in the case of Sub-Fund C, Sub-Fund CL, Sub-Fund I, and \\\\nSub-Fund X, to the currency of the Member States of the European Union (the Euro) or in the case of additional \\\\nSub-Funds as may be authorized by the Directors to the currency in which such Sub-Funds are issued, unless \\\\notherwise stated. \\\\nAUTHORISED CAPITAL \\\\nThe authorized share capital of the Fund is Five hundred thousand two hundred ninety one and eighty-eight \\\\nhundredths Euros €511,291.88 consisting of 250,000 non-voting participating redeemable shares denominated as \\\\n“Common Investor Shares C\" issued in respect of Sub-Fund C and having a par value of One-Hundredth (1/100) \\\\nof a Euro (€0.01) each; 500,000 non-voting participating redeemable shares denominated as “Common Investor \\\\nShares CL” issued in respect of Sub-Fund CL and having:a par value of One-Hundredth (1/100) of a Euro (€0.01) \\\\neach; 500,000 non-voting participating redeemable shares denominated as \"Common Investor Shares 1” issued in \\\\nrespect of Sub-Fund I and having a par value of One-Hundredth (1/100) of a Euro (€0:01) each; 500,000 non- \\\\nvoting participating redeemable shares denominated as \"Common Investor Shares X\" issued in respect of Sub- \\\\nFund X and having a par value of One-Hundredth (1/100) of a Euro (€0:01) each; 49,379,188 non-voting \\\\nparticipating redeemable shares un-denominated and having a par value of One-Hundredth (1/100) of a Euro \\\\n(€0.01) each and One Hundred (100) non-participating non-redeemable Common Shares of par value €1.00 each \\\\n(the “Management Shares\") with voting rights. \\\\nThe Company is hereby authorised to issue fractions of a Share as the Directors may by resolution of Directors \\\\ndetermine and each such fractional Share shall have the corresponding fractional designations, powers, \\\\nreferences, rights, qualifications and limitations of a Share of the same Class or Series of Shares. \\\\nThe Company is hereby authorised to issue such Classes or Series of Shares, and may subdivide any Class or \\\\nSeries of Shares in the capital for the time being into such Classes or Series of Shares, in each case as the \\\\nDirectors may by resolution of Directors determine, which resolutions shall (i) fix the designations, powers, \\\\npreferences, rights, qualifications and limitations of such.additional Classes or Series of Shares, and (ii) set forth \\\\nthe investment objectives of such Classes or Series of Shares. \\\\nThe Company is also authorized to increase and/or reduce the capital in the manner provided in Article 16 hereof. \\\\nSHARE RIGHTS AND LIMITATIONS \\\\nVOTING \\\\nThe holders of the Management Shares (the \"Members\") shall be entitled to attend and vote at all General \\\\nMeetings of Members and to take any action by written resolution of Members. \\\\nSubject to the provisions of these Articles and the Memorandum of Association of the Company, the holders of \\\\nthe Shares (the \"Shareholders\") shall not be entitled’to attend and vote at any General Meetings of Members or to \\\\ntake any action by written resolution of Shareholders. Meetings and consents of Shareholders required pursuant \\\\nto these Articles and the Memorandum of Association of the Company shall be governed mutatis mutandis by the \\\\nprovisions of Articles 7 and 7 hereof. \\\\nMINIMUM PURCHASE \\\\nThe minimum initial subscription shall be as specified in the Private Placement Memorandum. \\\\n \\\\n\\', \"  \\\\ned \\\\n6.1 \\\\n6.2 \\\\n6.3 \\\\n6.4 \\\\n6.5 \\\\n6.6 \\\\n6.7 \\\\n6.8 \\\\n6.9 \\\\n7.1 \\\\n7.2 REDEMPTION OF SHARES \\\\nSubject to the provisions of Articles 11 and 13 hereof and the Private Placement Memorandum, Shareholders \\\\nshall have the right to request redemption of their Shares in the relevant Segregated Account, by notice in the \\\\nform acceptable to the Company provided such notice is received by the Administrator no later thirty (30) days \\\\nprior to the relevant Redemption Date, or any other such time limit as may be stipulated in the Private Placement \\\\nMemorandum or determined by a Resolution of the Directors. \\\\nDIVIDEND POLICY AND DISTRIBUTION \\\\nThe Directors from time to time and at their discretion may declare and pay dividends to the holders of any \\\\nShares of any Segregated Account in amounts that the Directors in their discretion deem.appropriate but shall be \\\\nunder no obligation to do so. \\\\nNo.dividend shall be declared and paid except out of surplus.and unless the Directors:determine that immediately \\\\nafter the payment of the dividend: \\\\n(a) the Segregated Account in question (or the Company as a whole if there shall be no Segregated \\\\nAccounts) will be able to satisfy its liabilities as they become due in the ordinary course of its \\\\nbusiness; and \\\\n(b) the realizable value of the assets of the Segregated Account (or of the Company as a whole if \\\\nthere shall be no Segregated Accounts) will not be less than the sum of its total liabilities, as \\\\nshown in the books of the Segregated Account (or the Company as the case may be), and its \\\\nissued and outstanding Share capital. \\\\nDividends shall be paid on Shares of any Class or Series or Segregated Account only out of lawfully available \\\\nassets belonging to that Class or Series or Segregated Account. \\\\nIf the Company shall have Segregated Accounts, dividends if and when declared shall only be payable to holders \\\\nof Shares in that Segregated Account to the exclusion of the holders of Shares-in any other\\'Segregated Account. \\\\nDividends may be declared and paid in money, Shares:or property. \\\\nIf several persons are registered as joint holders of any Share, any of them may give effectual receipt for any \\\\ndividend or other monies: payable-on or in respect of the Share. \\\\nNotice of any dividend that may have been declared shall be given to each Shareholder of the particular \\\\nSegregated Account declaring the same in the manner hereinafter provided for notices and all dividends \\\\nunclaimed for three (3) years after having been declared may be forfeited by the Directors for the benefit of the \\\\nSegregated Account or if none shall then exist for the benefit of the Company. \\\\nNo dividend shall bear interest against the Company. \\\\nIf any Segregated Account shall be wound up (whether the liquidation is voluntary, under supervision, or by the \\\\nCourt), the Shareholders of that Segregated Account shall only be entitled to participate in the net assets of that \\\\nSegregated Account in proportion to the number of Shares held by them and recorded in the share register of that \\\\nSegregated Account. \\\\nVARIATION OF CLASS OF RIGHTS \\\\nIf at any time the authorised capital is divided into different classes or Series of Shares, the rights attached to any \\\\nClass or Series (unless otherwise provided by the terms of issue of the Shares of that Class or Series) may, \\\\nwhether or not the Company is being wound up, be varied with the consent in: writing of the holders of not less \\\\nthan three-fourths (3/4) of the issued Shares of that Class or Series and of the holders of not less than three-fourths \\\\n(3/4) of the issued Shares of any other Class or Series of Shares which may be affected by such variation. \\\\nThe rights conferred upon the holders of Shares of any Class issued with preferred or other riglits shall not, unless \\\\notherwise expressly provided by the terms of issue of the Shares of that Class, be deemed ito be varied by the \\\\ncreation or issue of further Shares ranking pari passu therewith. \\\\n  \\\\n \\\\n\", \\'  \\\\n  8.1 \\\\n8.2 \\\\n8.3 \\\\n8.4 \\\\n9.1 \\\\n9.2 \\\\n9.3 \\\\n9.4 \\\\n9.5 REGISTERED SHARES AND RESTRICTION ON SALE OR TRANSFER \\\\nShares shall be issued as registered shares only. \\\\nShares may not be issued or transferred to any Restricted Person. Shares are transferable only with the prior \\\\nwritten consent of the Directors of the Company; such consent may be withheld at the sole discretion of the \\\\nDirectors of the Company. The Company or its agent has the right to determine conclusively whether any person \\\\nor entity is a Restricted Person. The Company or its agent may determine that a person or entity is a Restricted \\\\nPerson after such person or entity has purchased Shares. \\\\nIf it shall come to the attention of the Company or its agent at any time that any Shares are beneficially owned by \\\\na Restricted Person, either alone or in connection with any other person, the Company may compulsorily redeem \\\\nsuch Shares at their Net Asset Value per Share and the Restricted Person(s) will cease to be the owner(s) of those \\\\nshares. \\\\nEach subscriber for Shares of any series shall be required to represent and warrant in the Subscription Agreement \\\\nthat he is not a Restricted Person and that the Shares are being acquired for his own account in the ordinary’course \\\\nof his business and not with a view to any public resale or distribution thereof, nor with any present intention of \\\\ndistribution or selling the same. \\\\nREGISTERS AND TITLE TO SHARES \\\\nThe Company shall have authority to issue Shares only in registered form. \\\\nShareholders shall not be entitled to Share Certificates for their Shares in any Segregated Account (or in the \\\\nCompany if no Segregated Accounts shall exist). All Shares shall be noted in the Share Register of each \\\\nSegregated Account kept by or on behalf of the Company in writing or by electronic means and each Shareholder \\\\nshall receive from the Company a contract note showing the details of such Shareholder’s share purchase and the \\\\ntype of share that the Shareholder has purchased in any Segregated Account. The Company shall not be bound to \\\\nregister on the Register- more than Two (2) persons as the joint owners of any-shares in the.Company (except in \\\\nthe case of executors or trustees of a deceased member) and in the case of a share in the Company owned jointly \\\\nby several persons, the Company shall only be bound to note the-names of the first Two (2) of such joint owners. \\\\n(i) The Company shal] maintain in such form as the Directors think fit a Register of Shareholders for each \\\\nSegregated Account. Subject to the provisions hereof the Directors may from time to time make such \\\\nregulations and provisions as they think fit respecting the keeping of all such Registers. \\\\n(2) The Directors shall cause to be entered in the Register the following particulars: \\\\n(a) The name and address of each Shareholder and the number and type of Shares held by each Shareholder \\\\nina particular Segregated Account from time to time following each Valuation Date. \\\\n(b) The date upon which the name of any person was entered in the Register asa registered Shareholder. \\\\n(c) The date upon which any person ceased to be-a registered Shareholder \\\\n(3) The Register shall be kept in such manner as to show at all times the numbers of Shares in each Segregated \\\\nAccount in issue. \\\\n(4) Every Register shall be kept in such manner as to show at all times the registered Shareholders of each \\\\nSegregated Account for the time being. The register of account owners shall not be open to \\\\ninspection, provided that an account owner shall be entitled to receive a copy of the \\\\ninformation in the register pertaining to his interest in any Segregated Account. \\\\nThe Company shall appoint a Registrar and Transfer Agent upon such terms and conditions as shall be agreed to \\\\nby the Directors and representatives of said Agent and the Company shall pay such remuneration and-expenses of \\\\nsaid Agent, as the Directors shall approve. \\\\nThe share register may be kept in written form or by such other means (including electronic recording) as the \\\\nDirectors may from time to time approve. \\\\n  \\\\n \\\\n\\', \\'  \\\\n  9.6 \\\\n9.7 \\\\n98 \\\\n99 \\\\n9.10 \\\\n10. \\\\n10.1 \\\\n10.2 \\\\n10.3 \\\\n10.4 \\\\nLL. The share register shall be conclusive evidence-as to the ownership of shares entered therein, and no notice of any \\\\ntrust, express, implied or constructive, shall be entered upon the share register in respect of any such shares. \\\\nUpon any change-of name or address on the part of a Shareholder being notified to the Company, the Company \\\\nshall forthwith comply with all formalities as shall be required to cause the relevant Register to be altered \\\\naccordingly. \\\\nA body corporate or any other legal entity may be registered as a Shareholder or as one of a number of joint \\\\nShareholders. \\\\n{f several persons are registered as joint holders of any Shares, any one of such persons may give a receipt for any \\\\ndividend payable in respect of such Shares. \\\\nEvidence of ownership of Shares of any Segregated Account shall be the share register maintained by or for the \\\\nCompany, which may be by computer display or printout. A Shareholder may ask for such computer printout or \\\\nother extract from the share register to be certified as correct by the Director, President or a Vice President or by \\\\nthe Secretary of the Company, or by the Registrar and Transfer Agent. \\\\nISSUE AND SALE OF SHARES \\\\nThe applicable Offering Price for Shares after trading has begun will be based on the Net Asset Value’ per Share \\\\nof the relevant Segregated Account on the Valuation Day after the order is received by the Company, plus a Sales \\\\nCommission of up to 5%. In the event the Company has suspended or postponed the Net Asset Value \\\\ndetermination, the valuations on the first Valuation: Day occurring after receipt of the order will’ be utilized. The \\\\nCompany reserves the right to suspend the sale of Shares in response to conditions in the securities markets or \\\\nupon a declared suspension in accordance with Article 13 hereof. \\\\nShareholders will be required to:provide assurances satisfactory to the Company indicating that the Shareholder is \\\\nnot a Restricted Person as‘such terms are defined in these Articles. \\\\nShareholders will only receive a confirmation of their share-holding. Share confirmations will be sent to investors \\\\nwithin one month after Shares are issued. Applicants: who submit an application form should indicate.on such \\\\nform the full name and address of each of the persons in whose name the Shares.are to be registered and in the \\\\ncase of a joint application which is to be the first named Shareholder. \\\\nThe Directors may reject any application for Shares.in whole or in part for any reason, which the Directors in their \\\\nsole discretion deem sufficient and need not assign any reason therefor. \\\\nREDEMPTION OF SHARES \\\\nSubject to this Article, Article 13 hereof and the provisions of the Private Placement Memorandum or a resolution \\\\nof the Directors, Shareholders have the right to request redemption of all or any part of their Shares on any \\\\nRedemption Date. \\\\n  \\\\n \\\\n\\', \\'  \\\\n  11.2 \\\\n11.3 \\\\n11.4 \\\\n11.5 \\\\n11.6 \\\\n11.7 \\\\n11.8 \\\\n12. \\\\n12.1 Each Share of any Segregated Account is redeemable by Shareholders on a weekly basis or at such other intervals \\\\nas shall be defined in the Company’s Private Placement Memorandum or any addendum thereto relating to the \\\\nparticular Sub-Fund at its Net Asset Value in the denominated currency of the Share determined as on the \\\\nValuation Day provided the Company has received the appropriate notice as specified in the Private Placement \\\\nMemorandum for the relevant Sub-Fund (each a \"Redemption Notice Date\") subject to a Redemption Penalty \\\\nequal to three percent (3%) if the redemption occurs during the first twenty-four months of ownership of Sub- \\\\nFund C Shares, during the first 12 months of ownership of Sub-Fund CL Shares, during the first three months of \\\\nownership of Sub-Fund | Shares, during the first three months of ownership of Sub-Fund X Shares, and at such \\\\nminimum ownership periods as specified in the Private Placement Memorandum for such other additional Sub- \\\\nFunds as may be in issue. Partial redemptions may not be permitted at-the discretion of the Trading Manager. \\\\nSubject to the foregoing, all requests in proper form (a \"Redemption Notice\") received by the Company not later \\\\nthan 30 days prior to the relevant Redemption Date or at such other times as the Directors may specify will be \\\\naccepted and processed as of that date; provided, however, that no Shares.may be redeemed during a suspension \\\\nof the determination of the Net Asset Value. Redemption requests that are not received by the Fund in a timely \\\\nmanner will be accepted and processed as of the following Redemption Date. Redemption proceeds based upon \\\\nthe un-audited Net Asset Value per Share of any Segregated Account prevailing on the Redemption Date will be \\\\nremitted within 30 days of such Redemption Date, provided that such amounts may be subject to change’ based on \\\\nthe annual audit. \\\\nAll redemption requests must be in the form provided to the Shareholder by the Company. \\\\nShares will be redeemed at the Net Asset Value per Share determined in accordance with Article 12 as relates to \\\\nthe relevant Segregated Account, subject to any redemption penalties expressed in these Articles and in the \\\\nPrivate Placement Memorandum, as of the close of business on a Redemption Date. The Directors, after \\\\nconsultation with any appointed trading manager, have the discretion to waive the redemption notice requirement \\\\nor any other restrictions on redemptions. The proceeds of a redemption of Shares may be reduced by charges \\\\nimposed by the Administrator in respect of payments by wire or cheque’ respectively. \\\\nNinety percent (90%) of redemption proceeds will be paid no later than 30 Business Days after the applicable \\\\nRedemption Date, and the balance will be paid upon finalization of the Net Asset Value per Share as relates to the \\\\nrelevant Segregated Account adjusted for the Redemption Penalty, if any, to which the redemption relates. No \\\\ninterest will be paid on the redemption proceeds between the Redemption Date and the date of actual payment to \\\\nthe Shareholders. \\\\nThe Directors have the right, acting with the consent of the Trading Manager, to distribute Securities in lieu of \\\\ncash (a \"Distribution in Kind\") in full satisfaction of the Company\\\\\\'s obligation to a redeeming Shareholder. The \\\\nvalue of the Securities so distributed will be based upon their market or fair value, as conclusively determined by \\\\nthe Directors and the Trading Manager, as of the prevailing Redemption Date. \\\\nThe Company may, from time to time, require the redemption of any of its outstanding:Shares that are or become \\\\nowned, directly or indirectly, by a U.S. Person or Bahamian Person or by any person whose ownership of Shares \\\\nis detrimental to the interests of the Company as determined by the Directors. \\\\nShares may be compulsorily redeemed at the direction of the. Directors in cases where’continuation of investment \\\\nwould have an adverse impact on the tax or legal status of the Company or Trading Manager or where the \\\\nCompany or Trading Manager would be subject to further or additional legal reporting or operational compliance \\\\nor violation of law. \\\\nNET ASSET VALUE \\\\nThe Net Asset Value of Shares of each Segregated Account shall be-determined as of the Administrator\\\\\\'s close of \\\\nbusiness on each Valuation Day. The Net Asset Value of each Segregated Account shal! be determined by the \\\\nAdministrator with the consultation of the Trading Manager and made available weekly at its Registered Office or \\\\nat such other office or such other time as the Directors may determine. \\\\n \\\\n\\', \"  \\\\n  Cr 12.2 \\\\n12.3 \\\\n12.4 \\\\n13. \\\\n13.1 \\\\n13.2 For the purpose of calculating Net Asset Value, portfolio securities which are traded on stock exchanges or traded \\\\non other organized markets are valued at the last known price on the principal market on which such securities are \\\\ntraded as of the close of business of such market immediately preceding the determination of the Net Asset Value. \\\\nFixed income securities not traded in such markets are valued at the last available price or yield equivalents \\\\nobtained from one or more dealers or pricing services. If events materially affecting the value of such securities \\\\noccur between the time of obtaining their value and the calculation of the Net Asset Value of Shares, then these \\\\nsecurities will be valued at their fair value as determined in good faith by or under the direction of the Directors. \\\\nThe Net Asset Value of the Shares of each Segregated Account at any date shall be the assets of the Segregated \\\\nAccount less the liabilities of the Segregated Account divided by the number of Shares then outstanding of the \\\\nSegregated Account. The Net Asset Value of the Segregated Accounts shall be determined in conformity with \\\\nAccounting Standards generally accepted in the United States of America and in accordance with any specific \\\\nconventions noted in the Private Placement Memorandum. The Administrative Fee, the, Management Fee and the \\\\nPerformance Fee as more particularly described in the Private Placement Memorandum shall be separately \\\\nallocated to each Segregated Account. \\\\nThe Net Asset Value of Shares shall be certified by a Director or an authorized officer or representative of the \\\\nCompany and any such-certification shall be conclusive except in the:case of manifest error. \\\\nSUSPENSION OF ISSUE AND REDEMPTION OF SHARES \\\\nAND CALCULATION OF NET ASSET VALUE \\\\nThe Directors may, at any time at the direction of the Directors or any appointed Trading Manager, suspend the \\\\ndetermination of the Net Asset Value of Shares of any Segregated Account for the whole or any part of any \\\\nperiod: \\\\n(a) during which any market in which a significant portion of the Company\\'s investments are \\\\ncurrently quoted or traded is closed, other than for customary holidays and weekends, or \\\\nduring which dealings’ therein are restricted or suspended; \\\\n(b) during the existence of any state of affairs which, in the opinion of the Directors, constitutes an \\\\nemergency as a result of which disposition by the Company of investments owned by it is not \\\\nreasonably practicable or would be seriously prejudicial to the Shareholders; \\\\nduring any breakdown in the means of communication normally employed in determining the \\\\n(c) price or value of any of the Company\\'s-investments, or of current prices in any market as \\\\ndescribed above or when for any other reason the prices or values of any investments owned \\\\nby the Company cannot reasonably be promptly and accurately ascertained; \\\\nduring which the transfer of funds involved in the realisation-or acquisition of any securities or \\\\n({d) other investments cannot, in the opinion of the directors, be effected at normal rates of \\\\nexchange; \\\\nduring which the Sub-Fund has insufficient assets, in the view of the Directors, to discharge its \\\\n{e) liabilities on the relevant date; or \\\\n(f at such other times as the Directors, in-their sole:discretion, may determine. \\\\nThe Directors shall take all reasonable steps to bring any suspension to an end as soon as possible. Notice of any \\\\nsuspension or the cessation of any suspension will be given to Shareholders, and the Securities Commission of \\\\nThe Bahamas. \\\\n  \\\\n \\\\n\", \\'  \\\\n  >? 14.1 \\\\n14.2 \\\\n14.3 \\\\n15, \\\\n15.1 \\\\n15.2 \\\\n15.3 \\\\n15.4 \\\\n16. \\\\n16.1 \\\\n16.2 TRANSFER OF SHARES \\\\nSubject to any limitations in the Private Placement Memorandum and in these Articles, Shares of any Segregated \\\\nAccount may be transferred by a written instrument of transfer signed by the transferor and containing the name \\\\nand address of the transferee, but in the absence of such written instrument of transfer the Directors may accept \\\\nsuch evidence of a transfer as they consider appropriate. \\\\nThe Directors shall not be required to treat a transferee of.a Share in the Company as a Shareholder or Member \\\\nuntil the transferee’s name has been entered in the-relevant share register. Shares are transferable only with the \\\\nprior written consent of the Directors of the Company. The Directors may in their absolute discretion and without \\\\nassigning any reason therefor decline to register any transfer of Shares and shall within twenty (20) days (or any \\\\nother such time limit as may be stipulated in the Private Placement Memorandum or determined by a Resolution \\\\nof the Directors) after the date on which the transfer was lodged with the Company or to a duly authorised agent \\\\nsend to the transferee notice of the refusal. \\\\nSubject to any limitations in the Memorandum and in these Articles, the Company must, on the application of the \\\\ntransferor or transferee of a Share in the Company, enter in the share register the name of the transferee of the \\\\nShare save that the-registration of transfers may be suspended and the share register closed at such times and for \\\\nsuch periods as the Company may from time to time by resolution of Directors determine provided always that \\\\nsuch registration shall not be suspended and the share register closed for more than sixty (60) days in any period \\\\nof twelve (12):months. \\\\nTRANSMISSION OF SHARES \\\\nThe executor or administrator of a deceased Shareholder or Member, the guardian of an incompetent Shareholder \\\\nor Member or the trustee of a bankrupt Shareholder or Member shall be the only person recognised by the \\\\nCompany as having any title to his Shares but they shall not be entitled to exercise any tights as a Shareholder or \\\\nMember of the Company until they have proceeded as set forth in the next following two (2) articles. \\\\nAny person becoming entitled by operation ’of law or otherwise to a Share or Shares in consequence of the death, \\\\nincompetence or bankruptcy of.any Shareholder or Member may be registered as a Shareholder or Member upon \\\\nsuch evidence being produced as may reasonably be required by the Directors. An application by any such person \\\\nto be registered, as a Shareholder or Member shall be deemed to be a transfer of Shares of the deceased, \\\\nincompetent or bankrupt Shareholder or Member and the Directors shall treat it as such. \\\\nAny person who has become entitled to a Share or Shares in consequence of the death, incompetence or \\\\nbankruptcy of any Shareholder or-Member may, instead of being registered himself, request in writing that some \\\\nperson to be named by him be registered as the transferee of such-Share or Shares and such request shall likewise \\\\nbe treated as if it were a transfer. \\\\nWhat amounts to incompetence on the part of a person is a matter to be determined by the court having regard to \\\\nall the relevant evidence and the circumstances of the case. \\\\nREDUCTION OR INCREASE IN AUTHORISED CAPITAL OR CAPITAL \\\\nThe Company may by a resolution of Directors amend the Memorandum to increase or reduce its, authorised \\\\ncapital as it relates to one or more existing Segregated Accounts or for the purposes of creating new Segregated \\\\nAccounts and in connection therewith the Company may in respect of any unissued Shares increase or reduce the \\\\nnumber of Shares, increase or reduce the par value of any Shares or effect any combination of.the foregoing. \\\\nThe capital of any particular Segregated Account may by a resolution of Directors be increased by transferring an \\\\namount of the surplus of the Segregated Account to capital, and the capital of the Segregated Account may be \\\\nreduced by transferring an amount of the capital of the Segregated Account to surplus. \\\\n10 \\\\n  \\\\n \\\\n\\', \"  \\\\n  () 16.3 \\\\n16.4 \\\\n16.5 \\\\n17.1 \\\\n17.2 \\\\n17.3 \\\\n17.4 \\\\n17.5 \\\\n17.6 \\\\n17.7 No reduction of capital of any Segregated Account shall be effected that reduces the capital of the Segregated \\\\nAccount to an amount that immediately after the reduction is less than the aggregate par value of all outstanding \\\\nShares with par value and all Shares with par value held by the Company as Treasury Shares and the aggregate of \\\\nthe amounts designated as capital of all outstanding Shares without par value and all Shares without par value \\\\nheld by the Company as Treasury Shares that are entitled to a preference, if any, in the assets of the Segregated \\\\nAccount upon liquidation of the Segregated Account. \\\\nThe Directors may not reduce.the capital of any Segregated Account if they determine that such reduction would \\\\nprevent the Segregated Account from being able to satisfy its liabilities as they become due in the ordinary course \\\\nof its business and that the realizable assets of the Segregated Account will be less than the sum of its total \\\\nliabilities, as shown in the books of the Segregated Account, and its remaining issued and outstanding Share \\\\ncapital. In the absence of fraud, the decision of the Directors as to the realizable value of the assets of the \\\\nSegregated Account is conclusive, unless a question of law is involved. \\\\nWhere any Segregated Account reduces its capital the Segregated Account may: \\\\n16.5.1 return to the Shareholders or Members of the Segregated Account any amount received by the \\\\nSegregated Account.upon the issue of any of its Shares; \\\\n16.5.2 purchase, redeem or otherwise acquire its Shares out of capital; or \\\\n16.5.3 cancel any capital that is lost:or not represented by assets having a realizable-value. \\\\nMEETINGS AND CONSENTS OF MEMBERS \\\\nThe Directors of the Company may, in their discretion convene an annual general meeting of the Members of the \\\\nCompany at such times and in such manner as the Directors consider necessary or desirable. Business conducted \\\\nat an annual general meeting (if any shall be:convened) shall be considered Ordinary business. \\\\nThe Directors of the Company may convene other meetings of the Members.or Shareholders.of the Company at \\\\nsuch times and in such manner and places as the Directors consider necessary‘or desirable. Business conducted at \\\\nsuch other meetings shall be considered Extraordinary business. \\\\nUpon the written request of Members holding more than twenty-five (25%) percent of the outstanding \\\\nManagement Shares the Directors shall convene a meeting of the Members. The Shareholders shall have no power \\\\nor authority to request the Directors to convene a meeting of the Shareholders. Unless otherwise expressly \\\\nrequired by these Articles, the Memorandum of Association, the Private Placement Memorandum, the IBC Act, \\\\nthe SAC Act or the Investment Funds Act, 2003 and its Regulations, the convening of Meetings of Shareholders \\\\nshall be entirely at the discretion of the Directors. \\\\nThe Directors shall give not less than seven (7) days notice of general meetings of Members to those persons \\\\nwhose names on the date the notice is given appear as Members in the share register and are entitled to vote at the \\\\nmeeting. \\\\nA meeting of Members or Shareholders held in contravention of the requirement to give notice is valid if \\\\nMembers holding more than fifty (50%) percent of the total number of Management Shares entitled to vote on all \\\\nmatters to be considered at the general meeting, or fifty (50%) percent of the votes of each Class or Series of \\\\nShares where Shareholders are entitled to vote thereon as a Class or Series together with more than a fifty (50%) \\\\npercent majority of the remaining votes, have agreed to shorter notice of the meeting, or if all Members or \\\\nShareholders holding Shares entitled\\'to vote on all or any matters to be considered at the meeting have waived \\\\nnotice of the meeting and for this,purpose presence at the-meeting shall be deemed to constitute-waiver. \\\\nThe inadvertent failure of the Directors to give notice of a meeting to a Member or Shareholder, or the fact that a \\\\nMember or Shareholder has not received notice, does not invalidate the meeting. \\\\nA Member or Shareholder may be represented at a meeting of Members.or Shareholders by a proxy who may \\\\nspeak and vote on behalf of the Member or Shareholder. \\\\n  \\\\n \\\\n\", \"  \\\\n17.8 The instrument appointing a proxy shall be produced at the place appointed for the meeting before the time for \\\\nholding the meeting at which the person named in-such instrument proposes to vote. \\\\n17.9 An instrument appointing a proxy shall be in substantially the following form or such other form as the Chairman \\\\nof the meeting shall accept as properly evidencing the wishes of the Member or Shareholder appointing the proxy. 7) \\\\n17.12 (a) \\\\n(b) \\\\n    (Name of Company) \\\\nI/We \\\\nBeing.a Member/Shareholder of the above Company with Member Shares/Shares \\\\nHEREBY \\\\nAPPOINT of \\\\nto be my/our proxy to vote for me/us at the meeting of Members/Shareholders to be held on the___day \\\\nof. 20__ and at any adjournment or adjournments thereof. \\\\n(Any restrictions.on voting are to be inserted here) \\\\nSigned this day of. , 20. \\\\nMember/Shareholder \\\\n(7.10 The following shall apply in respect of joint ownership: \\\\n17.10.1 if two (2) or more persons hold shares jointly each of them may be present in person or by \\\\nproxy at a meeting of\\'Members or Shareholders and may speak as a Member or Shareholder; \\\\n17.10.2 if only one (1) of the joint owners is present in person or by proxy he may vote on behalf of all \\\\njoint owners; and \\\\n17.10.3 if two (2) or more of the joint owners.are present in person or by proxy they must vote as one. \\\\n17.11 A Member or Shareholder shall be deemed to be present at a meeting of Members or Shareholders if he \\\\nparticipates by telephone or other electronic means and all Members participating in the meeting are able to hear \\\\neach other. \\\\nA general meeting of Members is duly constituted if, at the commencement of the meeting, there are \\\\npresent in person or by proxy fifty percent (50%) or more of the votes of the Management Shares \\\\nentitled to vote on resolutions of Members to be considered at the meeting. If a quorum be present, \\\\nnotwithstanding the fact that such quorum may be-represented by only one-person, then such person may \\\\nresolve any matter and a certificate signed by such person accompanied where such person be a proxy by \\\\na copy of the proxy form shall constitute a valid resolution of Members. \\\\nIf a meeting of Shareholders shall be convened at the sole discretion of the Directors for the purposes of \\\\nobtaining any necessary consents it shall be duly constituted if, at the commencement of the meeting, \\\\nthere are present in person or by proxy twenty five percent (25%) or more of the votes of the Shares or \\\\nClass or.Series of Shares entitled to vote at the meeting. \\\\n17.13 If within half an hour fromthe time.appointed for the meeting a quorum is not present, the meeting shall stand \\\\nadjourned for not less than fifteen days and the quorum at an adjourned meeting shall be those persons present in \\\\nperson or by proxy. \\\\n \\\\n\", \"  \\\\n  aa 17.14 \\\\n17.15 \\\\n17.16 \\\\n17.17 \\\\n17.18 \\\\n17.19 \\\\n17.20 \\\\n17.21 \\\\n18. \\\\n18.1 \\\\n18.2 At every meeting of Members or Shareholders, the Chairman of the Board of Directors shall preside as Chairman \\\\nof the meeting. If there is no Chairman of the Board of Directors or if the Chairman of the Board is not present at \\\\nthe meeting, the Members or Shareholders present shall choose someone of their number to be the Chairman. If \\\\nthe Members or Shareholders are unable to choose a Chairman for any reason, then the person representing the \\\\ngreatest number of Shares present in person or by prescribed form of proxy at the meeting shall preside as \\\\nChairman failing which the oldest individual Member or Shareholder or representative of a Member or \\\\nShareholder present shall take the Chair. \\\\nThe Chairman may, adjourn any meeting from time to time, and from place to place, but no business shall be \\\\ntransacted at any adjourned meeting other than the business left unfinished at the meeting from which the \\\\nadjournment took place. \\\\nAt any meeting of the Members or Shareholders the Chairman shall be responsible for deciding in such manner as \\\\nhe shall consider appropriate whether any resolution has been carried or not and the result of his decision shall be \\\\nannounced to the meeting and recorded in the minutes thereof. If the Chairman shall have any doubt as to the \\\\noutcome of any resolution put to the vote, he shall cause a poll to be taken of all votes cast upon such resolution, \\\\nbut if the Chairman shall fail to take a poll then any Member or Shareholder present in person or by proxy who \\\\ndisputes the announcement by the Chairman of the result of any vote may immediately following such \\\\nannouncement demand that a poll be taken and the Chairman shall thereupon cause a poll to be taken. Ifa poll is \\\\ntaken at any meeting, the result thereof shall be duly recorded \\'in the minutes of that meeting by the Chairman. \\\\nAny person other than an individual shall be regarded as one Member or Shareholder and subject to Article 17.7 \\\\nhereof the right of any individual to speak for or represent such Member or Shareholder shall be determined by \\\\nthe law of the jurisdiction where, and by the documents by which, the person is constituted or derives its \\\\nexistence. In case of doubt, the Directors may in good faith seek legal advice from any qualified person and \\\\nunless and until a court of competent jurisdiction shall otherwise:rule the Directors may rely and act upon such \\\\nadvice without incurring.any liability to.any Member or Shareholder. \\\\nAny person other than an individual which is a Member or Shareholder may by resolution of its Directors or other \\\\ngoverning body authorise such person as it thinks fit to act as its representative.at any meeting of the Company or \\\\nof any Class of Members or Shareholders of the Company, and the person so authorised shall be entitled to \\\\nexercise the same powers on behalf of the person which he represents as that person could exercise if it were an \\\\nindividual Member or Shareholder of the Company. \\\\nThe Chairman of any meeting at which a vote is‘cast by proxy or on behalf of any person other than an individual \\\\nmay call for a notarially certified copy of such proxy or authority which shall be produced within seven (7) days \\\\nof being so requested or the votes cast by such proxy or on behalf of such person shall be disregarded. \\\\nDirectors of the Company may attend and speak at any meeting of Members or Shareholders and at any separate \\\\nmeeting of the holders of any class or series of Shares in the Company. \\\\nThe Directors of the Company, the Administrator, the Trading Manager and their connected persons shall be \\\\npermitted to vote their beneficially owned equity interests at, a meeting in which they’have a material interest in \\\\nthe business to be contracted, if— \\\\n(a) full disclosure of the existence of such a relationship is made in the Private Placement \\\\nMemorandum; and \\\\n(b) the possibility exists of a decision being made to the detriment of the investors of the Company, and \\\\nthat the investors had been notified of such conflict prior to the meeting and were afforded the \\\\nopportunity to redeem their shares prior to a meeting being held. \\\\nDIRECTORS \\\\nThe first Directors of the Company shail be elected by the Subscribers to the Memorandum; and thereafter, the \\\\nDirectors:shall be elected by either the Members for.such terms.as the Members may determine or the Directors. \\\\nUntil Directors are appointed, the Subscribers to the Memorandum of Association shall-have the power to act as \\\\nDirectors. \\\\n  \\\\n \\\\n\", \"  \\\\n  \\\\nC) 18.3 \\\\n18.4 \\\\n18.5 \\\\n18.6 \\\\n18.7 \\\\n18.8 \\\\n18.9 \\\\n19. \\\\n19.1 \\\\n19.2 \\\\n19.3 \\\\n19.4 \\\\n19.5 \\\\n19.6 The minimum number of Directors shall be two (2). \\\\nEach Director shall hold office.for the term, if any, fixed by a resolution of Members or Directors as the case may \\\\nbe. In the case of a director who is an individual the term of office of a director shall terminate on the Director\\'s \\\\ndeath, resignation or removal. The bankruptcy of a corporate Director shall terminate the term of office of such \\\\nDirector. \\\\nA Director may be removed from office, with or without cause, by a-resolution of Members or Directors, as the \\\\ncase may be. \\\\nA Director may resign his.office by giving written notice of his resignation to the Company and the resignation \\\\nshall have effect from the date the notice is received by the Company or from such later date as may be specified \\\\nin the notice. \\\\nA vacancy in the Board of Directors may be filled by a resolution of Members or\\'by a resolution of a majority of \\\\nthe remaining. Directors or the remaining sole Director. \\\\nThe Company will reimburse each Director for the travel and other reasonable out-of-pocket expenses incurred in \\\\nconnection with his services. The Directors may not receive any remuneration for general services performed on \\\\nbehalf of the Company. Remuneration for special services shall be fixed by a Resolution of Directors. \\\\nA Director shall not require a share qualification, and may be an individual or a company. \\\\nPOWER OF DIRECTORS \\\\nThe business and affairs of the Company and each Segregated Account shall be managed by the Directors who \\\\nmay pay all expenses incurred preliminary to and in connection with the formation and on-going registration of \\\\nthe Company and may exercise all such powers of the Company as are not by the IBC Act or by the SAC Act or \\\\nby the Memorandum or these Articles required to be exercised by the Members, subject to any delegation of such \\\\npowers as may be authorised by these Articles and to such requirement as may be prescribed by a resolution of \\\\nMembers; but no requirement made by a resolution of Members shall prevail if it be inconsistent with these \\\\nArticles nor shall such requirement invalidate any prior act ofthe Directors which would have been valid if‘such \\\\nrequirement had not been made. \\\\nThe Directors may, by a resolution of Directors, appoint any person, including a person who is a director, to.be an \\\\nofficer or agent of the Company, and without limiting the generality of the foregoing, appoint Trading Advisor(s), \\\\nTrading Manager(s), Administrator(s), Distributor(s), Custodian(s), Registrar(s) and Transfer Agent(s) upon such \\\\nterms and conditions as the Directors, in their absolute discretion, shall determine. \\\\nEvery officer or agent of the Company has such powers and authority of the Directors, including the power and \\\\nauthority to affix the Seal, as are set forth in these Articles or in the resolution of Directors appointing the officer \\\\nor agent, except that no officer or agent has any power or authority with respect to fixing the emoluments of \\\\nDirectors. \\\\nAny Director which is a body corporate may appoint any person its-duly authorised representative for the purpose \\\\nof representing it at meetings of the Board of Directors or with respect to unanimous written consents. \\\\nThe continuing Directors may act notwithstanding any vacancy in their body, save that if their number is reduced \\\\nbelow the number fixed by or pursuant to these Articles as the necessary quorum for a meeting of Directors, the \\\\ncontinuing directors or director may appoint directors to fill any vacancy that has arisen or to summons a meeting \\\\nof Members. \\\\nThe Directors may, without limitation, exercise all the powers of the Company to borrow money, and to mortgage \\\\nor charge its undertaking, property and uncalled capital, and to issue debentures and other securities, whether \\\\noutright or as collateral security for any debt, liability or obligation of the Company or of any third party. The \\\\nDirectors authorized to borrow in order to fund redemption requests or meet any temporary shortage of funds \\\\ncreated by such requests and to enhance its investment leverage. There is-no limit on the leverage employed and \\\\nno restrictions on the Company’s borrowing capacity other than any limitations imposed by lenders and applicable \\\\ncredit regulations.   \\\\n \\\\n\", \\'  \\\\n  19.7 \\\\n19.8 ~ c™ \\\\n‘19.9 \\\\n19.10 \\\\n20. \\\\n20.1 \\\\n20.2 All cheques, promissory notes, drafts, bills of exchange and other negotiable instruments and all receipts for \\\\nmonies paid to the Company, shall be signed, drawn, accepted, endorsed or otherwise executed, as the case may \\\\nbe, in such manner as shal! from time to time be determined by resolution of Directors. \\\\nThe Directors may at any time exercise the powers conferred upon them by Section 10 (i) of the IBC Act. \\\\nSubject to the provisions of the Memorandum of Association of the Company and these Articles, the Directors \\\\nmay at any time amend its Constitutive Documents, the Private Placement Memorandum, and all Material \\\\nAgreements of the Company by a resolution of Directors without the approval of the Members if the Directors \\\\ncertify in writing that they are satisfied that the modification or addition either: \\\\n(a) does not materially prejudice the interests of the Shareholders, or \\\\n(b) does not operate to release to any material extent the Company or the Directors, the Custodian, \\\\nAdministrator, Trading Manager or any other person from any responsibility to the \\\\nShareholders, or \\\\n(c) is necessary for compliance with any fiscal, statutory or official requirements, or \\\\n(d) is made to correct a manifest error, or \\\\n(e) is in the discretion of the Directors in the best interests of the Company. \\\\n(ff) does not materially increase the costs and charges payable, from the investment fund property. \\\\n19.9.1 Any other modification or supplement requires, in addition, .the-approval of the Membersiby an \\\\nextraordinary resolution. No modification or addition may impose on any Shareholder or \\\\nMember any obligation to make a further payment or to accept any liability in respect of his \\\\nShares. \\\\nThe Directors of the Company may amend the Private Placement Memorandum of the \\\\n19.9.2 Company at any time by a resolution of Directors. \\\\nAny determination made in good faith by or pursuant to the direction of the Board of Directors, as.to: the-amount \\\\nof the assets, debts, obligations or liabilities of any Segregated Account, as to the amount of any reserves or \\\\ncharges set up and the propriety thereof, as to the time of or purpose for creating such reserves or charges, as to \\\\nthe use, alteration or cancellation of any reserves or charges (whether or not.any debt, obligation or liability) for \\\\nwhich such reserves or charges shall have been created shall have been paid or discharged or shall be then or \\\\nthereafter required to be paid or discharged, as to the value of or the method of valuing any investment or other \\\\nasset owned or held by any Segregated Account, as to the allocation of any asset or liability of the any Segregated \\\\nAccount to a particular class or series of Shares, as to the number of Shares of any class or series of Shares \\\\noutstanding, as to the estimated expense to the Segregated Account in connection with purchases of its Shares, as \\\\nto the ability to liquidate investments in orderly fashion, or as to any other matters relating to the issue, sale, \\\\npurchase or other acquisition or disposition of investments or Shares of the Company, shall be final and \\\\nconclusive and shall be final and binding upon the Company and all holders of its Shares, past, present and future, \\\\nand Shares of the Company are issued and sold on the condition and understanding that any and all such \\\\ndeterminations shall be final binding as aforesaid. \\\\nPROCEEDINGS OF DIRECTORS \\\\nThe Directors of the Company or any committee thereof may meet.at such times and in such manner:and places \\\\nwithin or outside of The Bahamas as the Directors may determine to be necessary or desirable. \\\\nA Director shall be deemed to be present at a meeting of Directors if he participates by telephone or other \\\\nelectronic means and all Directors participating in the meeting are able to hear each other. \\\\n15 \\\\n  \\\\n   \\\\n\\', \"  \\\\n  20.3 \\\\n20.4 \\\\n20.5 \\\\n20.6 \\\\n20.7 \\\\n20.8 \\\\n20.9 \\\\n20.10 \\\\n20.11 \\\\n20.12 \\\\n21. \\\\n21.1 A Director shal! be given not less than three (3) days’ notice of meetings of Directors, but a meeting of Directors \\\\nheld without three (3) days’ notice having been given to all Directors shall be valid if all the Directors entitled to \\\\nvote at the meeting waive notice of the meeting; and for this purpose, the presence of a Director at the meeting \\\\nshall be deemed to constitute waiver on his part. The inadvertent failure to give notice of a meeting to a Director. \\\\nor the fact that a Director has not received the notice, does not invalidate the meeting. \\\\nA Director may by a written instrument appoint an alternate who need not be a Director and an alternate is entitled \\\\nto attend meetings in the absence of the Director who appointed him and to vote or consent in place of the \\\\nDirector. \\\\nA meeting of Directors is duly constituted for all purposes if at the commencement of the meeting there are \\\\npresent in person or by alternate not less than one-half (1/2)\\'of the total number of Directors, unless there-are only \\\\ntwo (2) Directors in which case the quorum shall be two (2). \\\\nAt every meeting of the Directors, the Chairman of the Board of Directors shall preside as Chairman of the \\\\nMeeting. If there is no Chairman of the Board of Directors or if the Chairman of the Board of Directors is not \\\\npresent at the meeting, the Vice Chairman of the Board of Directors shall preside. If there is no Vice Chairman of \\\\nthe Board of Directors or if the Vice Chairman of the Board of Directors is not present at the meeting, the \\\\nDirectors present shall choose someone of their number to be Chairman of the meeting. \\\\nThe Company shall keep minutes of all meetings.of directors, members, committees of directors, committees of \\\\nofficers and committees of members, and copies of all resolutions consented to by directors, members, \\\\ncommittees of directors, committees of officers and committees. of members. \\\\nThe Company shall duly comply with the provisions of the IBC Act in regard to keeping a Register of Directors \\\\nand Officers at its Registered Office and in regard to the filing with the Registrar General of The Bahamas a copy \\\\nof such Register of the Directors and Officers and any amendments thereto in accordance with the said provisions. \\\\nThe books, records and minutes required by Article 20.7 and 20.8 shall be kept at the registered office of the \\\\nCompany. \\\\nThe Directors may, by a resolution of Directors, designate one or more committees, each consisting of one (1) or \\\\nmore Directors. \\\\nEach committee of Directors has such powers and authorities of the Directors, including the power and authority \\\\nto affix the Seal, as are set forth in the resolution of the Directors establishing the committee, except that no \\\\ncommittee has any power or authority either to amend the Memorandum or these Articles or with respect to the \\\\nmatters requiring a resolution of Directors under Articles 18.7, 18.8 and 19.2 hereof. \\\\nThe meetings and proceedings of each committee of Directors consisting of two (2) or more Directors shall be \\\\ngoverned mutatis mutandis by the provisions of these Articles regulating the proceedings of Directors so far as the \\\\nsame are not superseded by any\\'provisions in the resolution establishing the committee. \\\\nOFFICERS \\\\nThe Company may by resolution of Directors appoint officers of the Company at such times as shall be \\\\nconsidered necessary or expedient. Such officers may consist of a Chairman of the Board of Directors, a Vice \\\\nChairman of the Board of Directors, a President and one or more Vice Presidents, Secretaries and Treasurers and \\\\nsuch other officers as may from time to time be deemed desirable. Any number of offices may be held by the \\\\nsame person. \\\\n \\\\n\", \"  \\\\n  21.2 \\\\n21.3 \\\\n21.4 \\\\n22. \\\\n22.1 \\\\n22.2 \\\\n23. \\\\n23.1 \\\\n23.2 \\\\n23.3 \\\\n24, \\\\n24.1 The officers shall perform such duties as shall be prescribed at the time of their appointment subject to any \\\\nmodification in such duties as may be prescribed thereafter by resolution of Directors or resolution of Members, \\\\nbut in the absence of any specific allocation of duties, it shall be the responsibility of the Chairman of the Board \\\\nof Directors to preside at meetings of Directors and Members, the Vice Chairman to act in the absence of the \\\\nChairman; the President to manage the day-to-day affairs of the Company; the Vice Presidents to act in order of \\\\nseniority in the absence of the President but otherwise to perform such duties as may be delegated .to them by the \\\\nPresident; the Secretaries to maintain the share register, minute books and records (other than financial records) of \\\\nthe Company and to ensure compliance with all procedural requirements imposed on the Company by applicable \\\\nlaw, and the Treasurer to be responsible for the financial affairs of the Company. \\\\nThe emoluments of all officers, if any, shall\\'be fixed by resolution of Directors. \\\\nThe officers of the Company shall hold office unti] their\\'successors are duly elected and qualified, but any officer \\\\nelected or appointed by the Directors may be removed at any time, with or without cause, by resolution of \\\\nDirectors. Any vacancy occurring in any office of the Company may be filled by resolution of Directors. \\\\nCONFLICT OF INTERESTS \\\\nNo agreement or transaction between the Company and one or more of its Directors or any person in which any \\\\nDirector has a financial interest or to whom any Director is\\'related, including as a Director of that other person, is \\\\nvoid-or voidable for this reason only or by reason only that the Director is,present at the meeting.of Directors or at \\\\nthe meeting of the committee of Directors that approves the agreement or transaction or that the vote or consent of \\\\nthe Director is counted for that. purpose (i) if the material facts:of the interest of each Director in the agreement :or \\\\ntransaction and his interest in or relationship to any other party to the agreement or transaction are disclosed in \\\\ngood faith or are known by the other Directors and (ii) the-agreement or transaction is approved or ratified by a \\\\nresolution-of Members. \\\\nA Director who has an interest in any particular business to be considered at a meeting of the Directors or \\\\nMembers may be counted for purposes of determining whether the meeting is duly constituted. \\\\nINDEMNIFICATION AND INSURANCE \\\\nSubject to Article 23.2 hereof the Company shall indemnify against all expenses, including legal fees, and against \\\\nall judgments, fines and amounts paid in settlement and reasonably incurred in connection with legal, \\\\nadministrative or investigative proceedings any person who \\\\n23.1.1 is or was a party or is threatened to be made a party to any threatened, pending or completed \\\\nproceedings, whether civil, criminal, administrative or investigative, by reason of the fact that \\\\nthe person is or was a Director, an officer or a liquidator of the Company; or \\\\n23.1.2 is or was, at the request of the Company, serving as a Director, officer or liquidator of, or in \\\\nany other capacity is or was acting for, another company or a partnership, joint venture, trust or \\\\nother enterprise. \\\\nArticle 23.1 hereof only applies to a person referred to in that Article if the person acted honestly and in good \\\\nfaith with a view to the best interests of the Company and, in the case of criminal proceedings, the person had no \\\\nreasonable.cause to believe that his conduct was unlawful. \\\\nThe Company may purchase and maintain insurance in relation to any person who is or was a Director, an officer \\\\nor a liquidator of the-Company, or who at the request of the Company is or was serving as a Director, an officer or \\\\na liquidator of, or in any other capacity is or was acting for, another company or a partnership, joint venture, trust \\\\nor other enterprise, against any liability asserted against the person and incurred by the person in that capacity, \\\\nwhether or not the Company has or would have had the power to indemnify the person against the liability under \\\\nArticle 23.1 hereof. \\\\nLIMITATION OF LIABILITY \\\\nNo Director, Officer, Liquidator or Agent of the Company shall be liable to the Company or its Shareholders or \\\\n17 \\\\n \\\\n\", \\'  \\\\n  24.3 \\\\n25. \\\\n25.1 \\\\n26. \\\\n26.1 \\\\n27. \\\\n27.1 \\\\n27.2 \\\\n27.3 \\\\n27.4 \\\\n27.5 \\\\n27.6 Members for any mistake in judgment or any other event whatsoever provided that no such Director, Officer, \\\\nLiquidator or Agent shall be protected against any liability to the Company or its Shareholders or Members to \\\\nwhich they would otherwise be subject by reason of willful misfeasance, bad faith or negligence on their part in \\\\nthe performance of their duties or from reckless disregard by them of their duties and obligations. \\\\nNo Director, Officer, Liquidator or Agent of the Company shall be liable to the Company-or its Shareholders or \\\\nMembers for any losses due to the mistakes, negligence, misconduct or bad faith of any broker or other agent of \\\\nthe Company selected by any of them with reasonable care. \\\\nAny Director, Officer, Liquidator or Agent of the Company may consult with legal counsel or accountants, \\\\nselected by any of them, and any act or omission by any of them on behalf of the Company or in furtherance of \\\\nthe business of the Company in good faith on and in accordance with the advice-of such counsel or accountants \\\\nwill be full justification for the act or omission and such Director, Officer, Liquidator or Agent of the Company \\\\nwill be fully protected in so-acting or omitting to act if the counsel or accountants were selected with reasonable \\\\ncare. \\\\nSEAL \\\\nThe Directors shall provide for the safe custody of the Seal. An imprint of the Seal shall be kept at the registered \\\\noffice of the Company. The Seal when affixed to any written instrument shall be witnessed by a Director or any \\\\nother person so authorised from time to time by resolution of Directors. The Directors may provide for a \\\\nfacsimile of the Seal and of the signature of any Director or authorised person which may be reproduced by \\\\nprinting or other means on any instrument and it shall have the same force and validity as if the Seal had been \\\\naffixed to such instrument and the same had been signed as hereinbefore described. \\\\nACCOUNTS \\\\nThe Company shall keep such accounts and records as the Directors.consider necessary or desirable in order to \\\\nreflect the financial position of the Company. \\\\nAUDIT \\\\nThe Company shal! by resolution of the Directors call for the accounts of each Segregated Account to be \\\\nexamined by external auditors. \\\\nThe auditors shall be appointed by resolution of Directors. \\\\nThe auditors may be Shareholders of the Company but no Director, Member or other officer shall’ be eligible to \\\\nbe an auditor of the Company during his continuance in office. \\\\nThe remuneration of the auditors of the Company \\\\n27.4.1 in the case of auditors appointed by the Directors, may be fixed by resolution of Directors \\\\n27,4.2 subject to the foregoing, shall be fixed by resolution of Members or in.such manner as the Company may \\\\nby resolution of Members determine. \\\\nThe auditors shall examine each profit and loss account and balance sheet of each Segregated Account required \\\\nto be served on every Member of the Company or laid before a meeting of the Members of the Company and \\\\nserved on every Shareholder of that Segregated Account and shall state-in a.written report whether or not \\\\n27.5.1 in their opinion the profit and loss account and balance sheet have been prepared in conformity with \\\\naccounting standards generally accepted in the United States of America and in a manner which is \\\\nconsistent with the prior year’s presentation, and \\\\n27.5.2 all the information and explanations required by the auditors have been obtained. \\\\nThe report of the auditors in respect to any Segregated Account shall be annexed to the accounts and shall be \\\\nread at the meeting of Members at which the accounts are laid before the Company or shall be served on the \\\\nMembers:and on the’Shareholders of that Segregated Account. \\\\n18 \\\\n  \\\\n   \\\\n\\', \"  \\\\n27.7 \\\\n= 27.8 \\\\n27.9 \\\\n(a) \\\\n(b) \\\\n(c) \\\\n28. \\\\n28.1 \\\\n29.1 \\\\n29.2 \\\\n29.3 \\\\n29.4 \\\\n29.5   Every auditor of the Company shall have a right of access at all times to the books of account and vouchers of \\\\nany Segregated Account, and shall\\'be entitled to require from the Directors and officers of the-Company such \\\\ninformation and explanations as he thinks necessary for the performance of the duties of the auditors. \\\\nThe.auditors of the Company shall be entitled to receive-notice of,.and to attend any meetings of Members or of \\\\nShareholders of any Segregated Account at\\'which.any Segregated Account profit and loss account and balance \\\\nsheet are to be presented. \\\\nThe auditors of the Company shall ensure that the Directors: \\\\nmaintain the books and records of the Company in accordance with generally accepted accounting \\\\nprinciples used in the preparation of the financial statements of the company so that the records shall, \\\\nto the best of the knowledge, information and belief of the directors and officers of the company, \\\\nclearly show the share capital (if any), proceeds of rights, issues, securities, reserves, assets, \\\\nliabilities, income and expenses, dividends and distributions that are linked to each Segregated \\\\nAccount; \\\\nmaintain a record of each transaction entered into by the company; and \\\\nmaintain a general account with records in accordance with the SAC Act and with all of the assets and \\\\nliabilities of the company which are not linked to a segregated account and which discloses any assets \\\\nintended by the parties to be applied to a risk of any nature, and which therefore exposes such assets to \\\\nliability or loss. \\\\nPRELIMINARY EXPENSES \\\\nSubject to the provisions of the Private Placement Memorandum, the expenses incurred in connection with \\\\nrevising the Company’s Private Placement Memorandum and with any other costs associated with the initial \\\\nissue of new Shares (preliminary expenses) up to a maximum of $50,000 shall be paid by the Company and shall \\\\nbe allocated equally amongst all Segregated Accounts in new issue and amortized in equal monthly amounts in \\\\nthe year of payment. \\\\nNOTICES \\\\nAny notice, information or written statement to be given by the Company to Shareholders or Members of any \\\\nSegregated Account must be served by mail addressed to each Shareholder or Member at the address shown in \\\\nthe share register. \\\\nAny summons, notice, order, document, process, information or written statement to be served on the Company \\\\nor any Segregated Account may be served by leaving it with, or by sending it by registered mail to, the \\\\nregistered agent of the Company or any Segregated Account. \\\\nService of any summons, notice, order, document, process, information or written statement to be served on the \\\\nCompany or any Segregated Account may be proved by showing that the summons, notice, order, document, \\\\nprocess, information or written statement was mailed in such time as to admit to its being delivered in the normal \\\\ncourse of delivery within the period prescribed for service and was correctly addressed and the postage was \\\\nprepaid. \\\\nThe audited annual report of every Segregated Account will be mailed to Shareholdersvat their registered address \\\\nnot later than four (4) months after the end of each fiscal year. Subject to the terms of the Offering \\\\nMemorandum or a Resolution of the Directors the Company\\'s fiscal year will be the period ending on 31“ \\\\nDecember of each year. \\\\nThe Administrator or the Manager shall make available copies of the following documents which shail also be \\\\nmade available for inspection between the hours of 9:30 a.m. and 4:00 p.m. on any Business Day at the \\\\nRegistered Office of the Company, at Suite 6, BayParl Building, P.O. Box CB-11723, Nassau, Bahamas: \\\\n(a) The Memorandum of Association, \\\\n(b) The Articles of Association, \\\\n19 \\\\n   \\\\n\", \"  \\\\n  0z \\\\n“sme 950] JOpUN paplaoid JouueW oy} UT seURYeR oY] spisino uovyotpsiinf 2 Jo sme] oy] Japun \\\\npayesodiooul Aueduios & se snutjuoo sJoj9aN1q JO UOTINfOSaI fq 40 saquiayy Jo uorntosas Aq Aeut Aueduiod ayL \\\\nNOILVNNILNOD \\\\n‘yqunoooe OUT UayRI aq JOU [TEYS JUNODIY payeBo18ag Joyo Aue 0} Payul] SI1]142I] \\\\npur sjasseay3 ‘Couaaposul jo Junoose uo Joyepinbt| @ Aq dn punom Buraq yunoooy paredaidag Aue jo JU9A9 ay} U] \\\\n‘yunodoe [R18U98 Sy) 0} JO JUNO. paledaiFag Joyo Aue 0} PaxUt| \\\\nSOMiiqel] ayy 09 pareao]Te 10 paydde you ze Nosoy payeBaidag svfnoiyed & 0} payul] syasse ol Tey) amsud \\\\n[jeys soyepinbiy ayy ‘A[Burpsosoe “pue OY OVS 94) YIM aoURPIOIB Ut A[uo wmossy paredaidag & 01 pax] \\\\nue YDIYA SanITIqel] pue sjasse oy; WIM [Rep [TeYs Joyepinby] ay) ‘dn punom Bujoq punj-qng Aue jo Wee ou} \\\\nUy “quNosoy paleFe.3ag rey) Jo JaIs!Zoy s1eYg oUT-UL paprooas pue Woy Aq Play UNoDoy payedeudeg Aue yey3 Jo \\\\nsaieys Jo Jaqumu ayy 0} uoruodo.d ur saployareys yons Suoue peing|usip aq |[eYys 1un0soW paleseidog Aue Jo \\\\nSIOP[OYaIeYS Oy} O1 B|QEINGLISIP OS SJassE OY] “MOSSY payeBadag Jey] 0} Payeoor[e Sayi{!qel] Sp sso] 1UNODOW \\\\npaiederdag yey} 0) SuSuojaq siasse oyy ‘slapjoyaseys Oo} UONNGINSIP Jo} a[qe|!eAB yuNOOY poledoisag au) \\\\nJo Sjasse-ay] Jo INO sAla09I 0} Payue aq |[BYs JUNOSTY paledaldag powaye ot JO Sores JO SIOP[OY 1) Gune9 \\\\nap Aq Jo ‘uotstasadns sapun ‘Areyumyoa st uonepinbi] ayy Joyaym) dn-punom aq []eys JUNODIy payedoidag Aue J] \\\\n‘yunooay payedasdas ayy dn \\\\npuim 0} Aiofeu ajduiis e Jo siojooricy a4) Aq Bulssed ay) pure os op 0} JUNODW poyeSaidag Aue Jo SiopjoyasBys\\'ou} \\\\n0] snooSejueApe IsoU! I SyYBU P[NoM s10yDa11G ay) JO UOTUIdO ay UT YOIYM SIUaAas JO IUEAa Aue JO 90001990 3y J, \\\\n‘onqea errr ayi Jo (940) jusd1ed Ay wey) aso Aq aueys Jad ane Jessy JON OU} Ul AUl[D9p Vv \\\\n‘Kuedwio> ou) dn pum‘o1 Ausofew ajduuis e Aq uonnyosad & Jo si0}OaN1q 4) &q Suissed oy) pue yunos0y \\\\npaedardog oy) JO a0ua]sIxe panujuos oy} [NyMeyUN ayBU Pynom YOIYM sjusas JO jWsAS Aue JO 99uaLm090 ay, \\\\n-:BuIMO]OJ BU} JO 990 0} Js4y ayA UO dn punomM aq [feYs JUNODDV poyedoigeg Auy “soreys \\\\njuowadeuep] SuIpurjsino pue panss! ay) Jo %S/ JO AJOA SATTBULTYe ay2 04 Ajuo yoalqns ssO}DaTIC] JO UOLIN|OSTY & &q \\\\nSajossip pue dn pula 0 souaurWod A]jseUNTOA ABU! ‘sereYs panss} SBY YI JOU JO JeyJOyM Gunosoy payedeideg Auy \\\\nSLNNOOOV dILVDAAITS AO NOLLNTOSSIC GNV df ONIGNIM \\\\n‘Aured Sunjnejop ayy Jo Joweyiqse oy) Jo sould oy Ul 398 0} JOJeNIGIe \\\\nue yuiodde Sew Ayred seyjo yons ‘owes ay) qrodde oj aonou Wty sald sey Ayed soto ayy Joye shep (01) \\\\nua} Joy (oe 01 asTyal 10 Bunoe Jo a[qedeoul oq ‘sip [[eys JOwDIGIE poyutodde ue yey) Ju9AS ay} Ul) UOLMISQns \\\\njo Aem Aq 10 Ajjeuidio soya sowniqse ue Sunuiodde uy ynejop sayxeUs souslajo1 Sy) 0} Ayed rye J] \\\\n‘anduin ue juiodde-gouazayai oy] UO BuLIaIU9 e10Joq [[BYS SAOJeNIQIE \\\\nay) pue aduasayJIp ay) 01 sated ay Jo yoko Aq uasoyD aq 0} 9UO siOeNIGUE (Z) OM) 01 Patiajal aq ‘oyeAIque \\\\na[Burs e 07 aures ay) Jaa 0} aaude salued oy) ssayum “[/eYs soUatayIp-yons AueduioD ay) Jo.sTBYe 4 JO Aue 0) 10 \\\\nKuedwoz ayy Sunsaye aymerg 10 Joy Aue 0} Jo ‘sajanTy 3soy} 0} 10 sastusasd oy) 0} BuIze[ad as|muatpO JO YoRoAq \\\\npoBarje Jo yorosq Aue Suyyono} Jo Oy DVS au} 0 TY DEI ay) Jo souensind ul parayns 1o panjwo ‘poinooxa \\\\n40 auop SuipAue durysnoy Joy DVS 94) JO DY OG] BW Jo Jo sajsfuy aseyy Jo Saouenbasuod JO aouaploul \\\\ndy) Jo UOTaNaISUOS pue UsTUT any 94) SuIYoNO) ‘puBY saYIO JY) UO SUBISsE JO SLOJeISIUTUUPE “S1OINIOXA AO) JO \\\\nsiequiay] JO siaployareys ay} Jo Aue pue puey suo oy) uo AuedwioD ou Usemieq Sasle sdUsIEJIP Aue JoAousy A, \\\\nNOILVULISUVY \\\\n“UINPUBIOWIOYY JWOUISOe]| q SALLY AY} Ul O1 paLiayas SJUSUBAIY [elo] YL (p) \\\\n‘epuappe |equawaddns Aue pue winpuriouiay juowese|d areAlld SY, 0) Vee \\\\nCE \\\\nele \\\\ncle \\\\na \\\\nay \\\\nTOE \\\\nvoe-~ \\\\n‘OE \\\\n   \\\\n\", \\'  \\\\n  33. \\\\n33.1 \\\\n33.2 \\\\n33.3 \\\\n33.4 \\\\n33.5 \\\\n33.6 \\\\n33.7 \\\\n33.8 \\\\n33.9 \\\\n33.10 \\\\n34.0 \\\\n34.1 ADDITIONAL PROVISIONS RELATING TO SEGREGATED ACCOUNTS \\\\nIf 75% of the account owners in number and value of a Segregated Accounts Company and 75% of the \\\\ncounterparties in number and in value who are creditors submit a written request to the Registrar General of The \\\\nBahamas for deregistration, then the Registrar shall remove the Company from the Register and the provisions \\\\nof the.SAC Act shall cease to apply to the Company. \\\\nA Segregated Accounts Company must notify all creditors and all account owners of the Segregated Accounts \\\\nCompany of the removal of the Company from the register. \\\\nAn account owner of a Segregated Account or any creditor who is aggrieved by a request made pursuant to \\\\nsubsection (1) may, within 21 days of receipt‘of notice of the request, apply to the Registrar to refuse to remove \\\\nthe Segregated Accounts Company from the register or, if the removal has already occurred, to reinstate the \\\\ncompany on the register. \\\\nWhere an application has been-made under 33.3 above and the Registrar has made a decision on the application, \\\\nany person who is aggrieved by that decision may, within 21 days of the decision, appeal to the court and the \\\\ncourt shall hear the matter and make such order as it thinks fit. \\\\nThe making of a request pursuant to subsection 33.1 shall not of itself effect the removal of a Segregated \\\\nAccounts Company from the register and the Registrar in his absolute discretion shall determine whether to give \\\\neffect to the removal of the company fromthe register and, in.this regard may require such information from the \\\\ncompany as he considers necessary to render such a decision. If the Registrar shall give effect to such removal, \\\\nthese Articles of Association shall be amended accordingly. \\\\nA Segregated Accounts‘Company shall — \\\\n(a) _ inform any person with-whom it enters into a transaction that it is a‘segregated accounts company, \\\\n(b) where the transaction relates to a‘Segregated Account, for the purposes of that transaction identify or \\\\nspecify that Segregated Account; and \\\\n(c) _ include a reference to the fact that the company is a company registered under this Act:on its \\\\nletterhead and contracts. \\\\nIt shall be the.duty of the Directors of a Segregated Accounts Company to keep the assets and liabilities ofeach \\\\nSegregated Account and the general account separate and’ separately identifiable from assets and liabilities of \\\\neach other Segregated Account and the genera! account. \\\\nThe proceeds of the issue of shares or other securities of the Company, other than securities linked to a \\\\n‘Segregated Account, shall be included in the general assets of the Company only and except as provided for in \\\\nthe SAC Act the general shareholders (if any) shall have no rights to the assets of any Segregated Account by \\\\nreason only of being a general stiareholder. \\\\nA Segregated Accounts Company may purchase or otherwise acquire the shares or securities linked to a \\\\nSegregated Account using the assets linked to the relevant Segregated Account provided that on the date of \\\\npurchase or other acquisition, after taking into account the purchase or other acquisition, there are reasonable \\\\ngrounds for believing that — \\\\n(a) the relevant Segregated Account is solvent; and \\\\n(b) __ the realisable value of the assets of the Segregated Account would be more than-the aggregate of \\\\nits liabilities and its issued share capital of all classes. \\\\nThe Directors shall have the power to create and designate additional Segregated Accounts:as they shall from \\\\ntime to time by a Resolution of Directors determine and designate Shares in respect thereof. \\\\nINVESMENT AND BORROWING RESTRICTIONS \\\\nThe Company is not permitted to invest in issuers for the purpose of exercising control. The Company has no \\\\nother investment restrictions and no borrowing restrictions. \\\\n21 \\\\n   \\\\n\\', \\'  \\\\n  35.0 \\\\n35.1 \\\\n36.0 \\\\n36.1 \\\\n36.2 \\\\n36.3 ROLE OF CUSTODIAN \\\\nThe Company shall deposit its property with its Custodian and the Custodian shall take the.property into its \\\\ncustody or under its control and hold it in trust for the‘ Company in accordance with these Articles. \\\\nINVESTMENT FUND REGULATORY REQUIREMENTS \\\\nParticipating Parties: \\\\nAdministrator: XP FUNDSERVICES BAHAMAS LTD. \\\\nTrading Manager: THE AEGIS GROUP, LTD. \\\\nGoverning Law \\\\nThe Governing Law of the Company is the, Law of the: Commonwealth of The Bahamas. \\\\nRole of Administrator \\\\n(i) The Administrator shall: \\\\n(a) Take all reasonable efforts to ensure that the operations of the Company are carried \\\\nout in accordance with the Private Placement Memorandum, the provisions of these \\\\nArticles and the Regulations published under the Investment Funds Act, 2003 (“the \\\\nAct and Regulations”) (as may be amended from time to time), to the exclusive \\\\ninterest of the investors. \\\\n(b) Take all reasonable steps to ensure that the Company maintains proper books and \\\\nrecords. \\\\n(c) Take all reasonable steps to ensure that audited financial statements for the financial \\\\nyear are available for each investor within four months of the end of the Company’s \\\\nfinancial year or within such extension of that period as approved by the Securities \\\\nCommission of The Bahamas (the Commission). \\\\n(d) Make the Memorandum of Association and these Articles of the Company available \\\\nfor inspection in The Bahamas by members free of charge at all times during normal \\\\noffice hours at its place of business, and to make copies of such documents available \\\\nupon the payment of a reasonable fee. \\\\n(e) Take all reasonable steps to ensure that the Company is not carrying on its business \\\\nin 2 manner which is or is likely to be prejudicial to investors or creditors of the \\\\nCompany. \\\\n(f) Make such reports to the Commission as the Commission may require. \\\\n(g) Take all reasonable steps to ensure that the directors are meeting their obligations \\\\nand are complying with the Act and Regulations. \\\\n(ii) The Administrator shall be subject to removal by notice-in writing by the Board if: \\\\n(a) The Administrator goes into liquidation, becomes bankrupt or has a receiver \\\\nappointed over its assets; \\\\n22 \\\\n \\\\n\\', \"  \\\\n(iii) \\\\n(iv) (b) For any good and sufficient reason the Board is so inclined; or \\\\n(c) Members whose voting rights represent at least fifty percent (50%) of the \\\\nManagement Shares outstanding deliver to the Board a written request to dismiss the \\\\nAdministrator setting out good and sufficient reason for the requested dismissal. \\\\nThe Administrator shail retire: \\\\n(a) in any other case provided for in these presents, or \\\\n(b) if the Commission withdraws its approval of the Administrator. \\\\nUpon the retirement or dismissal of the Administrator, the Board shall appoint a successor \\\\nAdministrator as soon as possible thereafter and shall notify immediately upon the retirement \\\\nor dismissal of the Administrator, and within fourteen (14) days after a successor has been \\\\nappointed. If no successor has\\' been appointed, then the Board may apply to the Commission \\\\nfor authorization:to continue under extraordinary circumstances. \\\\n36.4 Fees and Charges \\\\n(a) \\\\n(b) \\\\n@ \\\\n) \\\\n  The Company may charge a sales charge upon subscription to be determined by the Board \\\\nfrom time to time. \\\\nThe Company may charge a redemption fee in accordance with Article 11.2. \\\\nThe Administrator shall bill each Sub-Fund at its normal customary rate. The Administrator \\\\nshall also be entitled to receive additional fees for work done in excess of the normal \\\\ncustomary administrative services at normal commercial rates based upon the amount of time \\\\nspent at the standard hourly rates.of the Administrator’s various personnel. \\\\nFor providing investment management services, The Aegis Group, Ltd, a corporation \\\\norganised under and in accordance with the laws of the Commonwealth of The Bahamas, (the \\\\nTrading Manager) will receive a monthly Management Fee of: 0.5% or six percent annually of \\\\nthe Net Asset Value of Sub-Fund C; 0.325%-or 3.9% annually of the Net Asset Value of Sub- \\\\nFund CL; 0.2333% or 2.8%.annually of the Net Asset Value of Sub-Fund I; 0.2917% or 3.5% \\\\nannually of the Net Asset Value of Sub-Fund X. \\\\nThe Trading Manager shall also receive a monthly Performance Fee equal to twenty percent \\\\n(20%) of Net Profits over Net Losses of each Sub-Fund with the exception of Sub Fund I, in \\\\nwhich case the monthly Performance Fee shall be equal to 15%. \\\\nThe Company is responsible for the costs and expenses associated with the offering and sale of \\\\nits Shares, for necessary market data and related expenses; its own direct costs including \\\\nbrokerage, legal, outside auditing, printing and mailing costs; for Directors’ fees, the expenses \\\\nof Board and member meetings; for any taxes, insurance premiums, transfer agency, registrar, \\\\nclearing and custodial charges; for any expenses and liabilities incurred by the Trading \\\\nManager or Administrator or their affiliates related to any proxy fight, tender offer or similar \\\\ninvestment strategy with respect to any portfolio investment or related to any actual or \\\\nthreatened legal action or proceeding in connection with purchasing, selling or holding any \\\\nportfolio asset; for any extraordinary expenses as shall be determined by the Administrator. \\\\nAll other expenses of the Company\\'s operations not specifically allocated herein to other \\\\nparties shall be borne by the Company. \\\\n23 \\\\n   \\\\n\", \"  \\\\n  \\\\nv) 36.5 \\\\n36.5.1 \\\\n36.5.2 \\\\n36.6 \\\\n36.7 \\\\n36.8 \\\\n36.9 \\\\n36.10 Transactions with Connected Persons   \\\\nCash forming part of the property of the Company may be temporarily placed as deposits with the Administrator \\\\nor its connected persons (being a Financial and Corporate Services Provider), if any, at interest rates no lower \\\\nthan is in accordance with normal banking practice, the commercial rate of deposits of that kind, size and term \\\\nand negotiated at arms length. \\\\nAny transactions between the Company and the Administrator, the Trading Manager, the Directors, or any of \\\\ntheir connected persons\\'as principal may be made only with the prior consent of the Administrator. \\\\nAnnual Accounting Period \\\\nThe annual accounting period for the Company ends December 31. \\\\nBase Currency \\\\nThe base currency of the Company is the United States dollar. \\\\nModification of the Memorandum and Articles of Association \\\\n  See the provisions of paragraph 12 of the Memorandum of Association. \\\\nTermination of the Company \\\\nThe-Company may go into voluntary dissolution upon:a resolution of the Directors of the Company\\'subject only \\\\nto confirmation by the affirmative vote of the holders of not less than three-quarters (3/4) of the then \\\\noutstanding Management Shares, in the event the Investment Management Agreement is terminated or if in the \\\\nevent no Segregated Account shall have been so designated or if all Segregated Accounts shall have been \\\\nderegistered in accordance with these Articles, the Company’s Shares fall to a level that is one-half of its initial \\\\noffering price, or for such other reason as set forth by the Directors. \\\\nExemption Standard \\\\nNothing in these Articles nor in the Memorandum of Association may provide that the Directors, Custodian, \\\\nAdministrator, or Trading Manager of the Company shall be exempt from any liability to investors imposed \\\\nunder Bahamian law in the case of willful default. \\\\n24  \\\\n\"]',\n",
       " '[\\'  First Choice Services Ltd. 2” Terrace West \\\\nCentreville \\\\nP.O. Box N-10567 \\\\nNassau, Bahamas. \\\\nMay 1/4, 2012 \\\\nTel. +1 (242) 326 2150 \\\\nFax +1 (242) 326 2151 So \\\\nNa \\\\nThe Registrar General \\\\nCompanies Section \\\\nShirley House \\\\nNassau, Bahamas \\\\nRe: Enhanced Frontier Limited # 4 SAC Re: Enhanced Vronuern errr \\\\nDear Sirs, \\\\nPlease find enclosed $250.00 for the capital decrease and the restated Memo & Arts for \\\\nEnhanced Frontier Limited (SAC). \\\\nThank you. We remain with kind regards. \\\\n‘16 MAY 2092 \\\\n  — | falr \" \\\\\\\\ tle \\\\nEncl.: fess 2 eeynner®, Smee \\\\n  Cheque \\\\nAbele pleacs. chech \\\\nue ee evel Cees ove \\\\n. 00001105 \\\\nigh \\\\n° aM pate[ bp.5 2D, 2 DBOMM ¥ ¥ ¥ Y ae —-—_ WARNING: THIS DOCUMENT CONTAINS A TAUE WATER! e \\\\nFIRST CHOICE SERVICES LTD. nas \\\\nOakbridge House, 6 West Hill Street \\\\nP. 0. Box N-10567 . \\\\nNassau, Bahamas Py \\\\nTRE ORDER OF We aesicac Cenc BS 25D ERD Tr \\\\n    JAMIAN DOLLARS \\\\n  Security \\\\nFestures \\\\nincluded \\\\n[ED \\\\nCetsils \\\\non \\\\nBac! \\\\nas yet oat + aa \\\\nwe Aaa bate * Pe Bp oe \\\\ncareanisconsrendinoesin: a wh i Bre et Se at \\\\n{BAHAMAS) LIMITED ‘ é ES \\\\nCommercial Banking Cenire \\\\nSite See Lot s ate woo sy “ae \\\\n  . Br Enh a ere pe heae (Rble Pos, \\\\nat USAC. \\\\n        \\\\n    AUG 0 2 2012\\\\n\\']',\n",
       " '[\"      \\\\n    COMMONWEALTH OF THE BAHAMAS \\\\nInternational Business Companies Act, 2000 \\\\n    MEMORANDUM AND ARTICLES \\\\nOF ASSOCIATION \\\\nOF \\\\nBRIC INFRASTRUCTURE FUND LTD., SAC \\\\n    DOMINION MANAGEMENT SERVICES LIMITED \\\\nDOMINION HOUSE \\\\n60 MONTROSE AVENUE \\\\nP.O. BOX\\'N-9932 \\\\nNASSAU, BAHAMAS \\\\n    \\\\n   \\\\n\", \\'  \\\\n  i) \\\\n“BRIC INFRASTRUCTURE FUND © \\\\nLTD., SAC \\\\nMEMORANDUM OF ASSOCIATION\\\\n\\', \"  \\\\nBRIC INFRASTRUCTURE FUND LTD., SAC 2|5 \\\\nMemorandum of Association \\\\nCOMMONWEALTH OF THE BAHAMAS \\\\nCOMPANY LIMITED BY SHARES \\\\nMEMORANDUM OF ASSOCIATION \\\\nOF \\\\nBRIC INFRASTRUCTURE FUND LTD., SAC ~. \\\\n  \\\\nB) \\\\nC) \\\\nD) \\\\n  The name of the Company is ‘BRIC Infrastructure Fund Ltd., SAC \\\\nThe registered office of the Company will be situate at Dominion House, 60 Montrose Avenue, P.O. \\\\nBox N-9932, Nassau, New Providence, The Bahamas. \\\\nThe registered agent of the Company wil] be Dominion Management Services Limited, Box N-9932, \\\\nDominion House, 60 Montrose Avenue, Nassau, New Providence, The Bahamas. \\\\nThe objects for which:the Company is established are: - \\\\n1. To carry on the business:of an investment company and for that purpose to acquire and hold \\\\neither in the name of the Company-or in that of any nominee shares, stocks, debentures, \\\\ndebenture stock, bonds, notes, obligations and securities issued or guaranteed by any \\\\ncompany wherever incorporated or carrying on business and debentures, debenture stock, \\\\nbonds, notes, obligations and securities-issued or guaranteed by‘any government, sovereign \\\\nruler, commissioners, public body or authority, supreme, dependent, municipal, local or \\\\notherwise in any part of the world. \\\\nTo borrow or raise money by the issue of debentures, debenture stock (perpetual or \\\\nterminable), bonds, mortgages, or any other securities founded or based upon all or any of the \\\\nassets or property of the Company or without any such security and upon such terms as to \\\\npriority or otherwise as the Company shall think fit. \\\\nTo guarantee loans and\\'to lend money with or without guarantee or security to any persons, \\\\nfirms or corporations. \\\\n2. To engage in any business or businesses whatsoever, or in any act or activity, which is not \\\\nprohibited under any law for the time being.in force in The Bahamas,       \\\\n   \\\\n\", \"  \\\\n  BRIC INFRASTRUCTURE FUND LTD., SAC 3|5 \\\\nMemorandum of Association   \\\\nNeo \\\\nE) \\\\nG) 3. To do all such.other things-as are incidental to or the Company may think conducive to the \\\\nattainment of.all or any of the above objects. \\\\nAnd it is hereby declared that the intention is that each of the objects specified in each paragraph-of \\\\nthis clause shall, except where otherwise expressed in such paragraph, be an independent main \\\\nobject and be in no way limited or restricted by reference to or inference from the terms of any \\\\nother paragraph or the name ofthe Company. \\\\nTHE COMPANY HAS NO\\'POWER TO: \\\\n1. carry on banking or trust business; \\\\n2. carry on business as\\'an insurance or a reinsurance company; \\\\n3. carry on business of providing the registered\\'office for companies; \\\\nThe shares in the Company shall be issued in the currency of the US Dollars (United States of \\\\nAmerica Dollars). \\\\nThe authorised share capital is USD 50\\'000 (fifty thousand US Dollars) which is divided into \\\\n1,000 voting non-participating non-redeemable shares of no par value “the Management Shares” \\\\nand 5,000,000 non-voting participating redeemable shares with a.par value of USD 0.01 per share \\\\n“the Investors’ Shares” as follows: \\\\n1. 1\\'000 (one thousand) registered voting Shares with no par value each, not.paid\\'up and held \\\\nby the Investment Manager and not offered hereunder (class M Shares); \\\\n2. Management Shares represent the only Class of Shares with the right to notice of and to vote \\\\nat general meetings of the Company’s Shareholders. However, they do not confer any right to \\\\ndividends and, on a winding up of the Company, carry only the right to the return of the \\\\nshare capital paid up thereon, i.e. USD\\'0.00. \\\\n \\\\n\", \\'  \\\\n  BRIC INFRASTRUCTURE FUND LTD., SAC 4[5 \\\\nMemorandutn of Association   \\\\nH) \\\\nI) \"INV ’ ” OR” \". \\\\n3. 100\\\\\\'000 (one hundred thousand) registered non-voting Shares of USD 0.01 (one USD cent) \\\\npar value each, issued.and offered during their Initial Subscription Reriod hereunder above \\\\npar for USD 10°000 (ten thousand USD) each of the Company and may also be subscribed \\\\nagainst contribution:in kind of assets at their respective Net Asset Value in Class A Shares; \\\\n4. 4\\\\\\'900\\\\\\'000 (four million nine hundred thousand) registered non-voting Shares of USD 0.01 \\\\n(one USD cent) par value each, issued but undesignated; \\\\nInvestors’ Shares offered hereunder will not carry the right to notice of and to vote at general \\\\nmeetings of the Company’s Shareholders except in respect of resolutions to vary the class rights \\\\nof the holders of Investors‘ Shares, see below “Rights of Shareholders”). However, they carry the \\\\nright to the return of the Net Asset Value attributable to the Sub-Fund of that Class of Investors’ \\\\nShares upon redemption or upon a winding up of the Company. \\\\nThe shares shall be divided into such number of classes and series as the Directors shall by \\\\nresolution from time to time determine and until so.divided\\\\\\'shall comprise one class and\\\\\\'series. \\\\nThe Directors shall by resolution have the power to issue any class or series of shares that the \\\\nCompany is authorized to issue in its capital, original or increased, with or subject to any \\\\ndesignations, powers, preferences, rights, qualifications, limitations and restrictions. \\\\nThe Company shall by resolution of the Directors have the power to.amend or modify any of the \\\\nconditions contained in this Memorandum of Association and to-increase or reduce the authorized \\\\ncapital of the Company in any way which may be.permitted by law. \\\\nWe the undersigned subscribers are desirous of incorporating an International Business Company under \\\\nthe laws of The Bahamas in pursuance of this Memorandum of Association. \\\\n   \\\\n\\', \\'  \\\\nBRIC INFRASTRUCTURE FUND LTD., SAC \\\\nMemorandum of Association \\\\nSubscriber 1. \\\\nSubscriber 2. NAMES AND ADDRESSES OF SUBSCRIBERS \\\\nMelanie A. Poitier \\\\nP.O. Box N-9932 \\\\nDominion House \\\\n60 Montrose Avenue, \\\\nNassau, Bahamas \\\\nMarva Mackey \\\\nP.O. Box N-9932 \\\\nDominion House \\\\n60 Montrose Avenue, \\\\nNassau, Bahamas \\\\nMasker 545 \\\\n  \\\\n  DATED as of October 30, 2013 \\\\n   \\\\n\\']']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:25.816211Z",
     "start_time": "2024-11-10T22:24:58.673013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if model_path:\n",
    "#     model = TopicModel(None)\n",
    "#     model.load_model(path=model_path)\n",
    "#     print(model)\n",
    "# else:\n",
    "model = TopicModel(documents=sentences)\n",
    "model.save_model(path=model_path)\n"
   ],
   "id": "4fdd94afd8b1efcd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 23:24:58,676 - top2vec - INFO - Pre-processing documents for training\n",
      "/Users/klara/Developer/Uni/WiSe2425/text_topic/topic_venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2024-11-10 23:25:04,883 - top2vec - INFO - Downloading distiluse-base-multilingual-cased model\n",
      "2024-11-10 23:25:07,038 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-11-10 23:29:17,452 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2024-11-10 23:29:25,503 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-11-10 23:29:25,536 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:25.826729Z",
     "start_time": "2024-11-10T22:29:25.822181Z"
    }
   },
   "cell_type": "code",
   "source": "#print('closest topics:', model.get_closest_topics(word='benutzer', num_topics=1)[0])\n",
   "id": "c289bc027065b508",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:25.834971Z",
     "start_time": "2024-11-10T22:29:25.827956Z"
    }
   },
   "cell_type": "code",
   "source": "#model.get_wordcloud_of_similar_topics(num_topics=2, word=\"benutzer\")",
   "id": "8c6476bcac67bab8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:25.842307Z",
     "start_time": "2024-11-10T22:29:25.836521Z"
    }
   },
   "cell_type": "code",
   "source": "#print(type(sentences))",
   "id": "1c63faec9f0d7ece",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:25.849622Z",
     "start_time": "2024-11-10T22:29:25.844819Z"
    }
   },
   "cell_type": "code",
   "source": "#print(sentences[0])",
   "id": "fb98c1975cbd5c46",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:25.858179Z",
     "start_time": "2024-11-10T22:29:25.850876Z"
    }
   },
   "cell_type": "code",
   "source": "len(sentences)",
   "id": "cfe6ade19011f765",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:29:26.129062Z",
     "start_time": "2024-11-10T22:29:25.859365Z"
    }
   },
   "cell_type": "code",
   "source": "#split_sentences = sentences.split('NEWFILE')#(r\"\\n'b'b'\")",
   "id": "6e94199cdb177bb2",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m split_sentences \u001B[38;5;241m=\u001B[39m \u001B[43msentences\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNEWFILE\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;66;03m#(r\"\\n'b'b'\")\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T07:14:12.998165Z",
     "start_time": "2024-11-11T07:14:12.979362Z"
    }
   },
   "cell_type": "code",
   "source": "split_sentences = sentences",
   "id": "e9894d1fb090c975",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T08:21:21.609334Z",
     "start_time": "2024-11-11T08:21:21.606081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_document_topic_incidence(model, document_ids: list):\n",
    "        \"\"\"\n",
    "        This function returns the incidence of topics in documents.\n",
    "        :param documents: List of document IDs\n",
    "        :return: Incidence of topics in documents\n",
    "        \"\"\"\n",
    "        topic_nums, topic_score, topics_words, word_scores = model.model.get_documents_topics(doc_ids=doc_ids, num_topics=10)\n",
    "        #topic_nums, topic_score, topics_words, word_scores = model.get_doc_topics(document_ids=document_ids)\n",
    "        # print(\"num topics: \", model.get_num_topics())\n",
    "        # print(np.where(topic_nums[0] == 11)[0][0])\n",
    "        # print(\"topics nums: \", topic_nums)\n",
    "        # print(\"topic score of id 0: \", topic_score[0])\n",
    "        # print(\"topics scores: \", topic_score)\n",
    "        # print(\"topics words: \", topics_words)\n",
    "        # print(\"word scores: \", word_scores)\n",
    "        \n",
    "        # create dataframe with document-topic incidence\n",
    "        doc_topic_columns = {topic_num: [0 if topic_num not in topic_nums[doc_id] else \\\n",
    "                                             topic_score[doc_id][np.where(topic_nums[doc_id] == topic_num)[0][0]] \\\n",
    "                                         for doc_id in range(len(topic_score))] for topic_num in range(model.get_num_topics())}\n",
    "        # real values are topic scores in [0, 1]\n",
    "        document_topic_incidence = pd.DataFrame(doc_topic_columns)  # automatic index == document id in TopicModel\n",
    "\n",
    "        return document_topic_incidence # TODO: test"
   ],
   "id": "64bbc20a5263f551",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T08:21:22.009632Z",
     "start_time": "2024-11-11T08:21:21.976258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = 5\n",
    "duration = 15\n",
    "doc_ids = list(range(start, start + len(sentences[start:start+duration])))\n",
    "#print(doc_ids)\n",
    "\n",
    "doc_topic_incidence = get_document_topic_incidence(model, document_ids=doc_ids)\n",
    "print(doc_topic_incidence.head())"
   ],
   "id": "aff075e78b09da94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0   1         2    3         4         5   6        7         8   \\\n",
      "0  0.668513   0  0.524073  0.0  0.000000  0.494209   0  0.00000  0.579818   \n",
      "1  0.000000   0  0.000000  0.0  0.000000  0.557770   0  0.00000  0.699701   \n",
      "2  0.573686   0  0.517201  0.0  0.000000  0.599249   0  0.00000  0.701835   \n",
      "3  0.529080   0  0.637995  0.0  0.485293  0.000000   0  0.46362  0.000000   \n",
      "4  0.516861   0  0.000000  0.0  0.000000  0.619646   0  0.00000  0.795205   \n",
      "\n",
      "         9   ...  37  38   39        40   41  42  43   44  45  46  \n",
      "0  0.508977  ...   0   0  0.0  0.000000  0.0   0   0  0.0   0   0  \n",
      "1  0.356830  ...   0   0  0.0  0.000000  0.0   0   0  0.0   0   0  \n",
      "2  0.555147  ...   0   0  0.0  0.000000  0.0   0   0  0.0   0   0  \n",
      "3  0.000000  ...   0   0  0.0  0.543157  0.0   0   0  0.0   0   0  \n",
      "4  0.515852  ...   0   0  0.0  0.000000  0.0   0   0  0.0   0   0  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "doc_topic_incidence.head()",
   "id": "4a665b1eff04526a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1fa50e983304ab67",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
